{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5336b4b9",
   "metadata": {},
   "source": [
    "# 阅读笔记\n",
    "## 基础\n",
    "### 标准强化学和深度强化学习\n",
    "- 标准强化学习：比如 TD-Gammon 玩 Backgammon 游戏的过程，其实就是设计特征，然后训练价值函数的过程。标准强化学习先设计很多特征，这些特征可以描述现在整个状态。 得到这些特征后，我们就可以通过训练一个分类网络或者分别训练一个价值估计函数来采取动作。\n",
    "\n",
    "- 深度强化学习：自从我们有了深度学习，有了神经网络，就可以把智能体玩游戏的过程改进成一个端到端训练（end-to-end training）的过程，我们不需要设计特征，直接输入状态就可以输出动作。我们可以用一个神经网络来拟合价值函数或策略网络，省去特征工程（feature engineering）的过程\n",
    "\n",
    "![pic1](./RL_basic/standard_and_deep_rl.png)\n",
    "\n",
    "### Reward\n",
    "智能体在环境里面存在的目 的就是最大化它的期望的累积奖励（expected cumulative reward）\n",
    "\n",
    "### Trajectory\n",
    "每一个观测可看成一个轨迹（trajectory）。 轨迹就是当前帧以及它采取的策略，即状态和动作的序列： $$ \\tau=\\left(s_{0}, a_{0}, s_{1}, a_{1}, \\ldots\\right) $$\n",
    "\n",
    "### State and Observation\n",
    "状态是对世界的完整描述，不会隐藏世界的信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用实值的向量、矩阵或者更高阶的张量来表示状态和观测.\n",
    "\n",
    "环境有自己的函数$s_{t}^{e}=f^{e}\\left(H_{t}\\right)$ 来更新状态，在智能体的内部也有一个函数$s_{t}^{a}=f^{a}\\left(H_{t}\\right)$来更新状 态。当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的（fully observed）。在这种情况下面，强化学习通常被建模成一个马尔可夫决策过程 （Markov decision process，MDP）的问题。在马尔可夫决策过程中，$o_{t}=s_{t}^{e}=s_{t}^{a}$。\n",
    "\n",
    "强化学习通常被建模成**部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）**的问题。部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。 部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道 部分观测值。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。部分可观测马尔可夫决策过程可以用一个七元组描述：$(S,A,T,R,\\Omega,O,\\gamma)$。其中 \n",
    "S表示状态空间，为隐变量，$A$ 为动作空间，$T(s'|s,a)$ 为状态转移概率，$R$ 为奖励函数，$\\Omega(o|s,a)$ 为观测概率，$O$ 为观测空间，$\\gamma$ 为折扣系数。\n",
    "\n",
    "### Action space\n",
    "不同的环境允许不同种类的动作。在给定的环境中，有效动作的集合经常被称为动作空间（action space）。有两种action space：（discrete action space），在这个动作空间里，智能体的动作数量是有限的。在其他环境，比如在物理世界中控制一个智能体，在这个环境中就有连续动作空间（continuous action space）。在连续动作空间中，动作是实值的向量。\n",
    "\n",
    "### 强化学习组成\n",
    "- Policy\n",
    "  - 随机性策略（stochastic policy）**就是 π 函数，即$\\pi(a | s)=p\\left(a_{t}=a | s_{t}=s\\right)$。输入一个状态s，输出一个概率。 这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。比如可能是有 0.7 的概率往左，0.3 的概率往右，那么通过采样就可以得到智能体将采取的动作。\n",
    "\n",
    "  - 确定性策略（deterministic policy）**就是智能体直接采取最有可能的动作，即 \n",
    "$$\n",
    "\\pi^* = \\arg\\max_a \\pi(a \\mid s)\n",
    "$$\n",
    "\n",
    "- Value function\n",
    "价值函数的值是对未来奖励的预测，我们用它来评估状态的好坏。 价值函数里面有一个折扣因子（discount factor），我们希望在尽可能短的时间里面得到尽可能多的奖励. 价值函数的定义为\n",
    "\n",
    "$$ V_{\\pi}(s) \\doteq \\mathbb{E}{\\pi}\\left[G{t} \\mid s_{t}=s\\right]=\\mathbb{E}{\\pi}\\left[\\sum{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} \\mid s_{t}=s\\right], \\text{对于所有的} s \\in S $$\n",
    "期望 $E_\\pi$的下标是 π 函数，$\\pi$ 函数的值可反映在我们使用策略 π的时候，到底可以得到多少奖励。  $G_t$ 是回报函数 。$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $\n",
    "\n",
    "另一种价值函数：Q 函数。Q 函数里面包含两个变量：状态和动作。其定义为 $$ Q_{\\pi}(s, a) \\doteq \\mathbb{E}{\\pi}\\left[G{t} \\mid s_{t}=s, a_{t}=a\\right]=\\mathbb{E}{\\pi}\\left[\\sum{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} \\mid s_{t}=s, a_{t}=a\\right] $$ 所以我们未来可以获得奖励的期望取决于当前的状态和当前的动作。Q 函数是强化学习算法里面要学习的一个函数。因为当我们得到 Q 函数后，进入某个状态要采取的最优动作可以通过 Q 函数得到。\n",
    "\n",
    "\n",
    "- Model\n",
    "模型决定了下一步的状态。下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率即 $$ p_{s s^{\\prime}}^{a}=p\\left(s_{t+1}=s^{\\prime} \\mid s_{t}=s, a_{t}=a\\right) $$\n",
    "\n",
    "奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即 $$ R(s,a)=\\mathbb{E}\\left[r_{t+1} \\mid s_{t}=s, a_{t}=a\\right] $$ 当我们有了策略、价值函数和模型3个组成部分后，就形成了一个马尔可夫决策过程（Markov decision process）。\n",
    "\n",
    "### 强化学习类型\n",
    "- 基于价值学习。\n",
    " 显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的\n",
    "- 基于策略学习\n",
    " 基于策略的智能体。（policy-based agent）直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。把基于价值的智能体和基于策略的智能体结合起来就有了actor-评论员智能体（actor-critic agent）。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作\n",
    "- 二者区别:\n",
    "对于一个状态转移概率已知的马尔可夫决策过程，我们可以使用动态规划算法来求解。从决策方式来看，强化学习又可以划分为基于策略的方法和基于价值的方法。决策方式是智能体在给定状态下从动作集合中选择一个动作的依据，它是静态的，不随状态变化而变化。 在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。 而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于动作集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。 基于价值的强化学习算法有Q学习（Q-learning）、 Sarsa 等，而基于策略的强化学习算法有策略梯度（Policy Gradient，PG）算法等。此外，actor-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果\n",
    "\n",
    "\n",
    "- 有模型强化学习\n",
    "- 无模型强化学习\n",
    "- 二者区别:\n",
    "我们可以用马尔可夫决策过程来定义强化学习任务，并将其表示为四元组 $<S,A,P,R>$，即状态集合、动作集合、状态转移函数和奖励函数。如果这个四元组中所有元素均已知，且状态集合和动作集合在有限步数内是有限集，则智能体可以对真实环境进行建模，构建一个虚拟世界来模拟真实环境中的状态和交互反应。 具体来说，当智能体知道状态转移函数 $P(s_{t+1}|s_t,a_t)$ 和奖励函数 $R(s_t,a_t)$后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。这种学习方法称为有模型强化学习. 然而在实际应用中，智能体并不是那么容易就能知晓马尔可夫决策过程中的所有元素的。通常情况下，状态转移函数和奖励函数很难估计，甚至连环境中的状态都可能是未知的，这时就需要采用无模型强化学习。无模型强化学习没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新动作策略，这样反复迭代直到学习到最优策略。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb889fc",
   "metadata": {},
   "source": [
    "## Police Gradient\n",
    "### 算法数学推导\n",
    "基本整理自[蘑菇书RL](https://datawhalechina.github.io/easy-rl/#/chapter4/chapter4)\n",
    "#### expected reward\n",
    "![trajectory](./RL_basic/trajectory.png)\n",
    "在一场游戏里面，我们把环境输出的 $s$ 与演员输出的动作 $a$ 全部组合起来，就是一个轨迹，即\n",
    "$$\n",
    "    \\tau=\\left\\{s_{1}, a_{1}, s_{2}, a_{2}, \\cdots, s_{t}, a_{t}\\right\\}\n",
    "$$\n",
    "\n",
    "给定actor的参数 θ，我们可以计算某个轨迹$\\tau$发生的概率为 $$ \\begin{aligned} p_{\\theta}(\\tau) &=p\\left(s_{1}\\right) p_{\\theta}\\left(a_{1} | s_{1}\\right) p\\left(s_{2} | s_{1}, a_{1}\\right) p_{\\theta}\\left(a_{2} | s_{2}\\right) p\\left(s_{3} | s_{2}, a_{2}\\right) \\cdots \\ &=p\\left(s_{1}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(a_{t} | s_{t}\\right) p\\left(s_{t+1} | s_{t}, a_{t}\\right) \\end{aligned} $$\n",
    "\n",
    "我们先计算环境输出 $s_1$的概率 $p(s_1)$，再计算根据 $s1执行 $a1 的概率$p_{\\theta}\\left(a_{1} | s_{1}\\right)$，$p_{\\theta}\\left(a_{1} | s_{1}\\right)$是由策略里面的网络参数 θ所决定的。策略网络的输出是一个分布，actor根据这个分布进行采样，决定实际要采取的动作。接下来环境根据 $a1$与 $s1$ 产生 $s2$，因为 $s2$ 与 $s1$是有关系的（游戏画面是连续的，下一个游戏画面与上一个游戏画面通常是有关系的），所以给定上一个游戏画面 $s1$和actor采取的动作 $a1$，就会产生 $s2$。主机在决定输出游戏画面的时候，可能有概率，也可能没有概率，这取决于环境（主机内部设定）。\n",
    "\n",
    "某个轨迹出现的概率取决于环境的动作和智能体的动作。环境的动作是指环境根据其函数内部的参数或内部的规则采取的动作。$p(s_{t+1}|s_t,a_t)$代表的是环境，通常我们无法控制环境，因为环境是设定好的。我们能控制的是 $p_\\theta(a_t|s_t)$。给定一个 $s_t$，actor要采取的 $a_t$ 取决于actor的参数 $\\theta$， 所以智能体的动作是actor可以控制的。actor的动作不同，每个同样的轨迹就有不同的出现的概率。\n",
    "\n",
    "reward function: 奖励函数根据在某一个状态采取的某一个动作决定这个动作可以得到的分数。对奖励函数输入 $s_1$、$a_1$，它会输出$r_1$；输入 $s_2$、$a_2$，奖励函数会输出 $r_2$。 我们把轨迹所有的奖励 $r$ 都加起来，就得到了 $R(\\tau)$ ，其代表某一个轨迹 $\\tau$ 的奖励。 在某一场游戏的某一个回合里面，我们会得到 $R(\\tau)$。我们要做的就是调整actor内部的参数 $\\theta$， 使得$R(\\tau)$的值越大越好。 但实际上$R(\\tau)$并不只是一个标量（scalar），它是一个随机变量，因为actor在给定同样的状态下会采取什么样的动作，这是有随机性的。环境在给定同样的观测时要采取什么样的动作，要产生什么样的观测，本身也是有随机性的，所以$R(\\tau)$是一个随机变量。我们能够计算的是$R(\\tau)$的期望值。给定某一组参数 $\\theta$，我们可计算 $r_{\\theta}$ 的期望值为\n",
    "$$\n",
    "    \\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)\n",
    "$$\n",
    "我们要穷举所有可能的轨迹 $\\tau$， 每一个轨迹 $\\tau$ 都有一个概率。\n",
    "\n",
    "给定一个参数，我们可以计算期望值为\n",
    "$$\n",
    "    \\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[R(\\tau)]\n",
    "$$\n",
    "从分布 $p_{\\theta}(\\tau)$ 采样一个轨迹 $\\tau$，计算 $R(\\tau)$ 的期望值，就是期望奖励（expected reward）。我们要最大化期望奖励。\n",
    "\n",
    "#### 引入梯度\n",
    "**我们要让奖励越大越好**，所以可以使用梯度上升（gradient ascent）来最大化期望奖励。 要进行梯度上升，我们先要计算期望奖励 $\\bar{R}_{\\theta}$ 的梯度。我们对 $\\bar{R}_{\\theta}$ 做梯度运算\n",
    "$$\n",
    "    \\nabla \\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau)\n",
    "$$\n",
    "其中，只有 $p_{\\theta}(\\tau)$ 与 $\\theta$ 有关。\n",
    "\n",
    "奖励函数$R(\\tau)$ 不需要是可微分的（differentiable），这不影响我们解决接下来的问题。\n",
    "我们可以对 $\\nabla p_{\\theta}(\\tau)$ 使用式(4.1)，得到 $\\nabla p_{\\theta}(\\tau)=p_{\\theta}(\\tau)  \\nabla \\log p_{\\theta}(\\tau)$。\n",
    "$$\n",
    "    \\nabla f(x)=f(x)\\nabla \\log f(x) \\tag{4.1}\n",
    "$$\n",
    "\n",
    "接下来，代数运算移项\n",
    "$$\n",
    "    \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)}= \\nabla \\log p_{\\theta}(\\tau)\n",
    "$$\n",
    "\n",
    "注：上式用了对数函数的求导公式，对数函数 $f(x)=\\log x$ 的导数为 $\\frac{1}{x}$。\n",
    "\n",
    "我们对 $\\tau$ 进行求和，把 $R(\\tau)$  和  $\\log p_{\\theta}(\\tau)$ 这两项使用 $p_{\\theta}(\\tau)$ 进行加权， 既然使用 $p_{\\theta}(\\tau)$ 进行加权 ，它们就可以被写成期望的形式。也就是我们从 $p_{\\theta}(\\tau)$ 这个分布里面采样 $\\tau$ ， 去计算 $R(\\tau)$ 乘 $\\nabla\\log p_{\\theta}(\\tau)$，对所有可能的 $\\tau$ 进行求和，就是期望的值（expected value）。\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\nabla \\bar{R}_{\\theta}&=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau)\\\\&=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)} \\\\&=\n",
    "        \\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla \\log p_{\\theta}(\\tau) \\\\\n",
    "        &=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]\n",
    "        \\end{aligned} \\tag{4.2}\n",
    "$$\n",
    "\n",
    "note： 我觉得 $\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}$ 只是为了写起来方便。\n",
    "\n",
    "实际上期望值 $\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]$ 无法计算，所以我们用采样的方式采样 $N$ 个 $\\tau$并计算每一个的值，把每一个的值加起来，就可以得到梯度，即\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right] &\\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(\\tau^{n}\\right) \\\\\n",
    "        &=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n",
    "        \\end{aligned}\n",
    "$$\n",
    "$\\nabla \\log p_{\\theta}(\\tau)$ 的具体计算过程可写为\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\nabla \\log p_{\\theta}(\\tau) &= \\nabla \\left(\\log p(s_1)+\\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)+ \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t) \\right) \\\\\n",
    "        &= \\nabla \\log p(s_1)+ \\nabla \\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)+  \\nabla \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t) \\\\\n",
    "        &=\\nabla \\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)\\\\\n",
    "        &=\\sum_{t=1}^{T} \\nabla\\log p_{\\theta}(a_t|s_t)\n",
    "        \\end{aligned}\n",
    "$$\n",
    "注意， $p(s_1)$ 和 $p(s_{t+1}|s_t,a_t)$ 来自环境，$p_\\theta(a_t|s_t)$ 来自智能体。$p(s_1)$ 和 $p(s_{t+1}|s_t,a_t)$ 由环境决定，与 $\\theta$ 无关，因此 $\\nabla \\log p(s_1)=0$ ，$\\nabla \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t)=0$。\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\nabla \\bar{R}_{\\theta}&=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau)\\\\&=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)} \\\\&=\n",
    "        \\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla \\log p_{\\theta}(\\tau) \\\\\n",
    "        &=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]\\\\\n",
    "        &\\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(\\tau^{n}\\right) \\\\\n",
    "        &=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n",
    "        \\end{aligned} \\tag{4.3}\n",
    "$$\n",
    "我们可以直观地理解式(4.3)，也就是在我们采样到的数据里面，采样到在某一个状态 $s_t$ 要执行某一个动作 $a_t$，$(s_t,a_t)$ 是在整个轨迹 $\\tau$ 的里面的某一个状态和动作的对。假设我们在 $s_t$ 执行 $a_t$，最后发现 $\\tau$ 的奖励是正的，我们就要增加在 $s_t$ 执行 $a_t$ 的概率。反之，如果在 $s_t$ 执行 $a_t$ 会导致 $\\tau$ 的奖励变成负的， 我们就要减少在 $s_t$ 执行 $a_t$ 的概率。这怎么实现呢？我们用梯度上升来更新参数，原来有一个参数 $\\theta$ ，把 $\\theta$  加上梯度$\\nabla \\bar{R}_{\\theta}$，当然我们要有一个学习率 $\\eta$，学习率也是要调整的，可用 Adam、RMSProp 等方法来调整学习率，即\n",
    "$$\n",
    "    \\theta \\leftarrow \\theta+\\eta \\nabla \\bar{R}_{\\theta}\n",
    "$$\n",
    "\n",
    "**QQ**： 为什么要有两个加合 $\\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} $? \n",
    "- 外层求和 $\\frac{1}{N} \\sum_{n=1}^{N}$：\n",
    "\n",
    "这是因为使用了蒙特卡洛方法，通过采样 $N$ 个轨迹 $\\tau^n$（每个轨迹 $n$ 对应一个完整的状态-动作序列）来估计期望值。\n",
    "每个轨迹 $\\tau^n$ 都有自己的回报 $R(\\tau^n)$ 和对数概率的梯度 $\\nabla \\log p_{\\theta}(\\tau^n)$。通过对这 $N$ 个样本取平均，近似整个期望值。\n",
    "\n",
    "\n",
    "- 内层求和 $\\sum_{t=1}^{T_n}$：\n",
    "\n",
    "这是因为每个轨迹 $\\tau^n$ 是一个时间序列，由多个时间步 $t$ 组成（长度为 $T_n$）。\n",
    "在策略梯度中，$\\nabla \\log p_{\\theta}(\\tau)$ 被分解为对每个时间步 $t$ 的贡献之和，即 $\\sum_{t=1}^{T} \\nabla \\log p_{\\theta}(a_t | s_t)$。这反映了轨迹概率 $p_{\\theta}(\\tau)$ 是由每个时间步的动作概率 $p_{\\theta}(a_t | s_t)$ 连乘得到的。\n",
    "具体推导中，$\\nabla \\log p_{\\theta}(\\tau) = \\sum_{t=1}^{T} \\nabla \\log p_{\\theta}(a_t | s_t)$，因为初始状态 $s_1$ 和状态转移 $p(s_{t+1} | s_t, a_t)$ 的概率通常不依赖于参数 $\\theta$（在标准设置中），它们的梯度为零。\n",
    "\n",
    "#### 计算梯度\n",
    "实际上要计算梯度，如图 所示，首先我们要收集很多$s$与$a$的对（pair），还要知道这些$s$与$a$在与环境交互的时候，会得到多少奖励。 这些数据怎么收集呢？我们要用参数为 $\\theta$的智能体与环境交互， 也就是拿已经训练好的智能体先与环境交互，交互完以后，就可以得到大量游戏的数据，我们会记录在第一场游戏里面，我们在状态 $s_1$ 采取动作 $a_1$，在状态$s_2$ 采取动作 $a_2$。\n",
    "智能体本身是有随机性的，在同样的状态$s_1$下，不是每次都会采取动作 $a_1$的，所以我们要记录，在状态 $s_1^1$ 采取 $a_1^1$、在状态 $s_2^1$ 采取 $a_2^1$等，整场游戏结束以后，得到的奖励是 $R(\\tau^1)$。我们会采样到另外一些数据，也就是另外一场游戏。在另外一场游戏里面，在状态 $s_1^2$ 采取 $a_1^2$，在状态 $s_2^2$ 采取 $a_2^2$，我们采样到的就是 $\\tau^2$，得到的奖励是 $R(\\tau^2)$。\n",
    "\n",
    "这时我们就可以把采样到的数据代入式(4.4)里面，把梯度算出来。也就是把每一个$s$与$a$的对拿进来，计算在某一个状态下采取某一个动作的对数概率（log probability）$\\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)$。对这个概率取梯度，在梯度前面乘一个权重，权重就是这场游戏的奖励。我们计算出梯度后，就可以更新模型。\n",
    "\n",
    "$$\n",
    "    \\nabla \\bar{R}_{\\theta}=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\\tag{4.4}\n",
    "$$\n",
    "![howto](./RL_basic/police_gradient_howto.png)\n",
    "\n",
    "#### 实现方法\n",
    "后面具体实现技巧暂时略过\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
