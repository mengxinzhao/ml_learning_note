{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae158d9a",
   "metadata": {},
   "source": [
    "# 1 é‡åŒ– \n",
    "\n",
    "## 1.1 MinMax Quantizationï¼ˆINT8 åŸºç¡€ï¼‰\n",
    "\n",
    "**ç›®æ ‡ï¼š** äº†è§£æœ€åŸºç¡€çš„ MinMax é‡åŒ–ã€symmetric vs asymmetricã€percentile clippingã€‚\n",
    "\n",
    "### 1.1.1 å¯¹ç§°é‡åŒ–ï¼ˆsymmetricï¼‰\n",
    "\n",
    "ç»™å®šå®å€¼å¼ é‡ $x$ï¼ŒINT8 å¯¹ç§°é‡åŒ–èŒƒå›´ä¸º $[-127, 127]$ï¼š\n",
    "\n",
    "- é‡åŒ–æ­¥é•¿ï¼š\n",
    "\n",
    "$$\n",
    "s = \\frac{\\max(|x|)}{Q_{\\max}}, \\quad Q_{\\max} = 127\n",
    "$$\n",
    "\n",
    "- é‡åŒ–ï¼š\n",
    "\n",
    "$$\n",
    "q = \\mathrm{round}\\left( \\frac{x}{s} \\right)\n",
    "$$\n",
    "\n",
    "- åé‡åŒ–ï¼š\n",
    "\n",
    "$$\n",
    "\\hat{x} = s q\n",
    "$$\n",
    "\n",
    "### 1.1.2 éå¯¹ç§°é‡åŒ–ï¼ˆasymmetricï¼‰\n",
    "\n",
    "å¸¸ç”¨äº activationï¼š\n",
    "\n",
    "$$\n",
    "s = \\frac{x_{\\max} - x_{\\min}}{Q_{\\max} - Q_{\\min}}, \\quad\n",
    "z = Q_{\\min} - \\frac{x_{\\min}}{s}\n",
    "$$\n",
    "\n",
    "$$\n",
    "q = \\mathrm{round}\\left( \\frac{x}{s} + z \\right), \\quad\n",
    "\\hat{x} = s (q - z)\n",
    "$$\n",
    "\n",
    "### 1.1.3 Percentile clipping\n",
    "\n",
    "é¿å… outlier å½±å“ï¼š\n",
    "\n",
    "- ä½¿ç”¨ $[p, 1-p]$ åˆ†ä½æ•°æ›¿ä»£å…¨å±€ min/maxï¼Œå¸¸è§é€‰æ‹©ï¼š\n",
    "  - CNN: p â‰ˆ 0.001\n",
    "  - Transformer: p â‰ˆ 0.01\n",
    "\n",
    "### æ–‡çŒ®\n",
    "\n",
    "- Jacob et al., *Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference*, CVPR 2018.  \n",
    "- PyTorch é‡åŒ–å®è·µæ•™ç¨‹ï¼ˆQuantization in Practiceï¼‰  \n",
    "  https://pytorch.org/blog/quantization-in-practice/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3727296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentile=None, scale=0.0927, rel L2 error=8.8467e-03\n",
      "percentile=0.99, scale=0.0605, rel L2 error=4.6716e-02\n",
      "percentile=0.999, scale=0.0766, rel L2 error=1.4280e-02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def minmax_symmetric_quant(x, num_bits=8, percentile=None):\n",
    "    \"\"\"å¯¹ç§° MinMax é‡åŒ–ï¼Œæ”¯æŒ percentile clippingã€‚\"\"\"\n",
    "    x = x.detach()\n",
    "    if percentile is not None:\n",
    "        lo = torch.quantile(x, (1 - percentile) / 2)\n",
    "        hi = torch.quantile(x, 1 - (1 - percentile) / 2)\n",
    "        max_val = torch.max(lo.abs(), hi.abs())\n",
    "    else:\n",
    "        max_val = x.abs().max()\n",
    "\n",
    "    qmax = 2 ** (num_bits - 1) - 1  # INT8: 127\n",
    "    scale = max_val / (qmax + 1e-8)\n",
    "    scale = torch.clamp(scale, min=1e-8)\n",
    "\n",
    "    q = torch.round(x / scale).clamp(-qmax, qmax)\n",
    "    x_hat = q * scale\n",
    "    return q, x_hat, scale\n",
    "\n",
    "\n",
    "\n",
    "x = torch.randn(4096) * 3.0\n",
    "for p in [None, 0.99, 0.999]:\n",
    "    q, x_hat, s = minmax_symmetric_quant(x, num_bits=8, percentile=p)\n",
    "    err = torch.norm(x - x_hat) / torch.norm(x)\n",
    "    print(f\"percentile={p}, scale={s.item():.4f}, rel L2 error={err.item():.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddf732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on tensor with 10240000 elements\n",
      "============================================================\n",
      "\n",
      "1. Original torch.quantile method:\n",
      "   Time: 2.3569s\n",
      "   Threshold: 9.8730\n",
      "\n",
      "2. Histogram-based method:\n",
      "   Time: 0.0123s\n",
      "   Threshold: 9.8768\n",
      "   Speedup: 192.0x faster\n",
      "\n",
      "3. Full quantization (histogram-based percentile):\n",
      "   percentile=None, scale=0.1336, L2_err=1.2865e-02, time=0.0122s\n",
      "   percentile=0.99, scale=0.0608, L2_err=4.3415e-02, time=0.0239s\n",
      "   percentile=0.999, scale=0.0778, L2_err=1.3800e-02, time=0.0186s\n",
      "   percentile=0.9999, scale=0.0920, L2_err=9.4657e-03, time=0.0188s\n"
     ]
    }
   ],
   "source": [
    "# æå¤§tensor([1, 4096, 4096])ä¸Šç®—quantile ä¼šéå¸¸æ…¢ï¼ŒåŸå› $O(N logN)$ æ’åºã€‚å¿«é€Ÿç®—æ³•æ˜¯æ–¹æ³•æ˜¯histogram calibration\n",
    "def histogram_percentile_clipping(x, percentile=0.999, num_bins=2048):\n",
    "    \"\"\"\n",
    "    Fast percentile clipping using histogram instead of sorting.\n",
    "    This is O(N) instead of O(N log N) for torch.quantile.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor (any shape)\n",
    "        percentile: Target percentile (e.g., 0.999 = 99.9%)\n",
    "        num_bins: Number of histogram bins (more bins = more precision)\n",
    "    \n",
    "    Returns:\n",
    "        threshold: The value at the given percentile\n",
    "    \"\"\"\n",
    "    x_flat = x.view(-1).abs()  # Flatten and take absolute values\n",
    "    \n",
    "    # Get min and max for histogram range\n",
    "    x_min = x_flat.min()\n",
    "    x_max = x_flat.max()\n",
    "    \n",
    "    # Handle edge case: all values are the same\n",
    "    if x_max == x_min:\n",
    "        return x_max\n",
    "    \n",
    "    # Create histogram\n",
    "    hist = torch.histc(x_flat, bins=num_bins, min=x_min.item(), max=x_max.item())\n",
    "    \n",
    "    # Calculate cumulative sum\n",
    "    cumsum = torch.cumsum(hist, dim=0)\n",
    "    total = cumsum[-1]\n",
    "    \n",
    "    # Find the bin where cumulative sum exceeds percentile * total\n",
    "    target_count = total * percentile\n",
    "    bin_idx = torch.searchsorted(cumsum, target_count)\n",
    "    \n",
    "    # Convert bin index to actual value\n",
    "    # bin_idx corresponds to the right edge of that bin\n",
    "    bin_width = (x_max - x_min) / num_bins\n",
    "    threshold = x_min + (bin_idx + 1) * bin_width\n",
    "    \n",
    "    return threshold\n",
    "\n",
    "def minmax_symmetric_quant_fast(x, num_bits=8, percentile=None, num_bins=2048):\n",
    "    \"\"\"\n",
    "    Optimized symmetric MinMax quantization with fast histogram-based percentile clipping.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        num_bits: Quantization bit width (default: 8)\n",
    "        percentile: If provided, clip to this percentile (e.g., 0.999)\n",
    "        num_bins: Number of histogram bins for percentile calculation\n",
    "    \n",
    "    Returns:\n",
    "        q: Quantized tensor (integer)\n",
    "        x_hat: Dequantized tensor (float)\n",
    "        scale: Scale factor used\n",
    "    \"\"\"\n",
    "    x = x.detach()\n",
    "    \n",
    "    if percentile is not None:\n",
    "        # Use histogram-based percentile clipping (fast!)\n",
    "        max_val = histogram_percentile_clipping(x, percentile, num_bins)\n",
    "    else:\n",
    "        # Use simple max (fastest)\n",
    "        max_val = x.abs().max()\n",
    "    \n",
    "    qmax = 2 ** (num_bits - 1) - 1  # INT8: 127\n",
    "    scale = max_val / (qmax + 1e-8)\n",
    "    scale = torch.clamp(scale, min=1e-8)\n",
    "    \n",
    "    q = torch.round(x / scale).clamp(-qmax, qmax)\n",
    "    x_hat = q * scale\n",
    "    return q, x_hat, scale\n",
    "\n",
    "\n",
    "import time\n",
    "# Create a large tensor\n",
    "x = torch.randn(10000, 1024) * 3.0  # ~10M elements\n",
    "\n",
    "print(\"Testing on tensor with\", x.numel(), \"elements\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Original torch.quantile (slow)\n",
    "print(\"\\n1. Original torch.quantile method:\")\n",
    "start = time.time()\n",
    "lo = torch.quantile(x, 0.0005)\n",
    "hi = torch.quantile(x, 0.9995)\n",
    "max_val_slow = torch.max(lo.abs(), hi.abs())\n",
    "time_slow = time.time() - start\n",
    "print(f\"   Time: {time_slow:.4f}s\")\n",
    "print(f\"   Threshold: {max_val_slow.item():.4f}\")\n",
    "\n",
    "# Method 2: Histogram-based (fast)\n",
    "print(\"\\n2. Histogram-based method:\")\n",
    "start = time.time()\n",
    "max_val_fast = histogram_percentile_clipping(x, percentile=0.999, num_bins=2048)\n",
    "time_fast = time.time() - start\n",
    "print(f\"   Time: {time_fast:.4f}s\")\n",
    "print(f\"   Threshold: {max_val_fast.item():.4f}\")\n",
    "print(f\"   Speedup: {time_slow/time_fast:.1f}x faster\")\n",
    "\n",
    "# Method 3: Full quantization with histogram\n",
    "print(\"\\n3. Full quantization (histogram-based percentile):\")\n",
    "for p in [None, 0.99, 0.999, 0.9999]:\n",
    "    start = time.time()\n",
    "    q, x_hat, s = minmax_symmetric_quant_fast(x, num_bits=8, percentile=p, num_bins=2048)\n",
    "    elapsed = time.time() - start\n",
    "    err = torch.norm(x - x_hat) / torch.norm(x)\n",
    "    print(f\"   percentile={p}, scale={s.item():.4f}, \"\n",
    "            f\"L2_err={err.item():.4e}, time={elapsed:.4f}s\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac7789",
   "metadata": {},
   "source": [
    "## 1.2 KL Divergence Calibrationï¼ˆTensorRT é£æ ¼ç†µæ ¡å‡†ï¼‰\n",
    "\n",
    "**ç›®æ ‡ï¼š** ç”¨ KL æ•£åº¦å¯»æ‰¾æœ€ä¼˜ clipping é˜ˆå€¼ $\\alpha$ã€‚\n",
    "\n",
    "- çœŸå®åˆ†å¸ƒ $P$ï¼šFP32 activation çš„ç›´æ–¹å›¾  \n",
    "- é‡åŒ–é‡å»ºåˆ†å¸ƒ $Q(\\alpha)$ï¼šclip+é‡åŒ–+åé‡åŒ–åé‡æ–°ç»Ÿè®¡çš„ç›´æ–¹å›¾  \n",
    "- ç›®æ ‡ï¼š\n",
    "\n",
    "$$\n",
    "\\alpha^* = \\arg\\min_{\\alpha} D_{\\text{KL}}(P \\Vert Q(\\alpha))\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P\\Vert Q) = \\sum_i P(i)\\log\\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "### 1.2.1 å®é™…å·¥ç¨‹\n",
    "\n",
    "- TensorRT / OpenVINO ç­‰æ¡†æ¶éƒ½æ”¯æŒåŸºäº KL çš„ç†µæ ¡å‡†ï¼ˆentropy calibrationï¼‰ã€‚\n",
    "- é€šå¸¸ï¼š\n",
    "  - å…ˆç”¨ histogram ä¼°è®¡ Pï¼›\n",
    "  - æšä¸¾å¤šä¸ª $\\alpha$ï¼›\n",
    "  - é€‰æ‹© KL æœ€å°çš„é˜ˆå€¼ã€‚\n",
    "\n",
    "### 1.2.2 æ–‡çŒ®\n",
    "\n",
    "- Lei Maoï¼š*Cross Entropy, KL Divergence, and Maximum Likelihood Estimation*  \n",
    "  https://leimao.github.io/blog/Cross-Entropy-KL-Divergence-MLE/\n",
    "- NVIDIA TensorRT Developer Guide ä¸­å…³äº INT8 calibration çš„ç« èŠ‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c178734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 4.141835689544678 KL: 9.38576889038086\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def kl_divergence(p, q, eps=1e-8):\n",
    "    p = p + eps\n",
    "    q = q + eps\n",
    "    return torch.sum(p * torch.log(p / q))\n",
    "\n",
    "def kl_calibration(samples, num_bins=2048, num_candidates=80, bit=8):\n",
    "    \"\"\"KL calibration with consistent histogram ranges.\"\"\"\n",
    "    samples = samples.detach().view(-1)\n",
    "    abs_samples = samples.abs()\n",
    "    \n",
    "    # Reference histogram range (use max of original samples)\n",
    "    hist_max = abs_samples.max()\n",
    "    \n",
    "    # 1. é«˜ç²¾åº¦ç›´æ–¹å›¾ P (original distribution)\n",
    "    hist = torch.histc(abs_samples, bins=num_bins, min=0, max=hist_max)\n",
    "    P = hist / hist.sum()\n",
    "    \n",
    "    # 2. å€™é€‰ alpha ä½¿ç”¨åˆ†ä½æ•°\n",
    "    qs = torch.linspace(0.90, 0.9999, num_candidates)\n",
    "    alphas = torch.quantile(abs_samples, qs)\n",
    "    \n",
    "    Q_int = 2 ** (bit - 1) - 1  # å¯¹ç§° INT8\n",
    "    \n",
    "    best_kl = None\n",
    "    best_alpha = None\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        # 3. clip + é‡åŒ– + åé‡åŒ–\n",
    "        clipped = torch.clamp(samples, -alpha, alpha)\n",
    "        scale = alpha / (Q_int + 1e-8)\n",
    "        q = torch.round(clipped / scale).clamp(-Q_int, Q_int)\n",
    "        deq = q * scale\n",
    "        \n",
    "        # 4. è®¡ç®—é‡å»ºç›´æ–¹å›¾ Q(Î±) - USE SAME RANGE AS P\n",
    "        hist_q = torch.histc(deq.abs(), bins=num_bins, min=0, max=hist_max)\n",
    "        Q = hist_q / hist_q.sum()\n",
    "        \n",
    "        kl = kl_divergence(P, Q)\n",
    "        if best_kl is None or kl < best_kl:\n",
    "            best_kl = kl\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    return best_alpha, best_kl\n",
    "\n",
    "\n",
    "x = torch.randn(200000) * 2.5\n",
    "alpha, kl = kl_calibration(x)\n",
    "print(\"Best alpha:\", alpha.item(), \"KL:\", kl.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6303a",
   "metadata": {},
   "source": [
    "## 1.3 SmoothQuantï¼ˆå¤§æ¨¡å‹ INT8ï¼‰\n",
    "\n",
    "### 1.3.1 æ•°å­¦åŸç†\n",
    "SmoohQuant è§£å†³çš„æ˜¯å¤§æ¨¡å‹é‡åŒ–ä¸­ä¸€ä¸ªéå¸¸æ£˜æ‰‹çš„é—®é¢˜ï¼šæ¿€æ´»å€¼ï¼ˆActivationï¼‰ä¸­çš„å¼‚å¸¸å€¼ï¼ˆOutliersï¼‰éš¾ä»¥é‡åŒ–ã€‚\n",
    "\n",
    "ç®€å•æ¥è¯´ï¼Œå¤§æ¨¡å‹ï¼ˆå¦‚ GPTã€LLaMAï¼‰çš„ Activation é€šå¸¸éå¸¸â€œéš¾æâ€ï¼Œè€Œ Weight ç›¸å¯¹â€œå¹³æ»‘â€ã€‚SmoothQuant çš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯æŠŠ Activation çš„é‡åŒ–éš¾åº¦è½¬ç§»ä¸€éƒ¨åˆ†ç»™ Weightï¼Œè®©ä¸¤è€…éƒ½å˜å¾—â€œå¹³æ»‘â€ï¼Œä»è€Œéƒ½èƒ½è½»æ¾ç”¨ INT8 é‡åŒ–ã€‚\n",
    "\n",
    "1. é—®é¢˜çš„æ ¹æºï¼šActivation Outliers\n",
    "åœ¨ LLM ä¸­ï¼ŒæŸäº›ç‰¹å®šçš„é€šé“ï¼ˆChannelï¼‰çš„æ¿€æ´»å€¼ä¼šç‰¹åˆ«å¤§ï¼ˆå¯èƒ½æ˜¯å…¶ä»–é€šé“çš„ 100 å€ï¼‰ã€‚\n",
    "å¦‚æœä½ ä¸ºäº†ç…§é¡¾è¿™å‡ ä¸ªå¤§å€¼ï¼ŒæŠŠé‡åŒ–èŒƒå›´ï¼ˆScaleï¼‰è®¾å¾—å¾ˆå¤§ï¼Œé‚£ä¹ˆå…¶ä»– 99% çš„å°æ•°å€¼éƒ½ä¼šè¢«å‹ç¼©åˆ° 0 é™„è¿‘ï¼Œå¯¼è‡´ç²¾åº¦å½’é›¶ã€‚\n",
    "å¦‚æœä½ æŠŠèŒƒå›´è®¾å°ï¼Œè¿™å‡ ä¸ªå¤§å€¼å°±ä¼šè¢«æˆªæ–­ï¼ˆClippingï¼‰ï¼Œä¸¥é‡ç ´åæ¨¡å‹é€»è¾‘ã€‚\n",
    "ä¸æ­¤åŒæ—¶ï¼Œæƒé‡ï¼ˆWeightï¼‰ çš„åˆ†å¸ƒé€šå¸¸å¾ˆå‡åŒ€ï¼Œé‡åŒ–èµ·æ¥å¾ˆå®¹æ˜“ã€‚\n",
    "\n",
    "2. æ ¸å¿ƒæ€æƒ³ï¼šæ•°å­¦å˜æ¢\n",
    "SmoothQuant å¼•å…¥äº†ä¸€ä¸ªå¹³æ»‘ç³»æ•° $s$ï¼Œè¯•å›¾æŠŠ Activation çš„å¤§å€¼â€œå‹ä¸‹å»â€ï¼ŒåŒæ—¶æŠŠ Weight å¯¹åº”ç»´åº¦çš„å€¼â€œæ”¾å¤§ä¸Šæ¥â€ã€‚\n",
    "å¯¹äºçŸ©é˜µä¹˜æ³• $Y = X W$ï¼š\n",
    "$X$ æ˜¯è¾“å…¥æ¿€æ´»å€¼ï¼ˆå¾ˆéš¾é‡åŒ–ï¼Œæœ‰å°–å³°ï¼‰\n",
    "$W$ æ˜¯æƒé‡ï¼ˆå®¹æ˜“é‡åŒ–ï¼Œå¾ˆå¹³å¦ï¼‰\n",
    "æˆ‘ä»¬åœ¨ä¸­é—´æ’å…¥ä¸€ä¸ªé™¤æ³•å’Œä¸€ä¸ªä¹˜æ³•ï¼ˆæ•°å­¦ä¸Šç­‰äºä¹˜ 1ï¼Œä¸æ”¹å˜ç»“æœï¼‰ï¼š\n",
    "$$ Y = (X \\cdot \\text{diag}(s)^{-1}) \\cdot (\\text{diag}(s) \\cdot W) $$\n",
    "è¿™å°±å˜æˆäº†ä¸¤ä¸ªæ–°å˜é‡ï¼š\n",
    "æ–°çš„æ¿€æ´»å€¼ $X' = X / s$\n",
    "æ–°çš„æƒé‡ $W' = W \\cdot s$\n",
    "\n",
    "3. æ€ä¹ˆé€‰è¿™ä¸ª $s$ï¼Ÿ\n",
    "è¿™æ­£æ˜¯ SmoothQuant çš„ç²¾é«“ã€‚å®ƒå®šä¹‰äº†ä¸€ä¸ªè¶…å‚æ•° $\\alpha$ æ¥æ§åˆ¶â€œæ¬è¿â€å¤šå°‘éš¾åº¦ã€‚\n",
    "$$ s_j = \\frac{\\max(|X_j|)^\\alpha}{\\max(|W_j|)^{1-\\alpha}} $$\n",
    "å…¶ä¸­ $j$ ä»£è¡¨ç¬¬ $j$ ä¸ªè¾“å…¥é€šé“ï¼ˆInput Channelï¼‰ã€‚\n",
    "å¦‚æœ $\\alpha = 1$ï¼š$s$ å®Œå…¨ç”± Activation çš„æœ€å¤§å€¼å†³å®šã€‚è¿™æ„å‘³ç€æˆ‘ä»¬æŠŠ Activation çš„æ‰€æœ‰å¼‚å¸¸å€¼éƒ½é™¤æ‰äº†ï¼Œ$X'$ å˜å¾—æåº¦å¹³æ»‘ï¼ˆæ‰€æœ‰é€šé“æœ€å¤§å€¼éƒ½å˜æˆ 1ï¼‰ã€‚ä½†æ˜¯ï¼Œ$W'$ ä¼šå˜å¾—æ³¢åŠ¨å·¨å¤§ï¼ˆæŸäº›è¡Œè¢«æ”¾å¤§äº† 100 å€ï¼‰ï¼Œå¯¼è‡´ Weight å˜å¾—å¾ˆéš¾é‡åŒ–ã€‚\n",
    "å¦‚æœ $\\alpha = 0$ï¼š$s$ å®Œå…¨ç”± Weight å†³å®šï¼Œç›¸å½“äºå•¥ä¹Ÿæ²¡åšï¼Œä¿æŒåŸæ ·ã€‚\n",
    "SmoothQuant çš„é€‰æ‹©ï¼ˆ$\\alpha = 0.5$ï¼‰ï¼šæŠ˜ä¸­ï¼\n",
    "è®© $s \\approx \\sqrt{\\max(|X|)}$ã€‚\n",
    "è¿™æ · $X$ è¢«é™¤ä»¥äº†è‡ªå·±çš„å¹³æ–¹æ ¹ï¼Œå°–å³°å˜å°äº†ã€‚\n",
    "$W$ è¢«ä¹˜ä»¥äº† $X$ çš„å¹³æ–¹æ ¹ï¼Œæ³¢åŠ¨å˜å¤§äº†ï¼Œä½†æ²¡é‚£ä¹ˆç¦»è°±ã€‚\n",
    "ç»“æœï¼š $X'$ å’Œ $W'$ éƒ½å¤„äºä¸€ç§â€œæ¯”è¾ƒå¥½é‡åŒ–â€çš„çŠ¶æ€ï¼Œéƒ½èƒ½ç”¨ INT8 é‡åŒ–ã€‚\n",
    "\n",
    "4. å›¾è§£ç¤ºä¾‹\n",
    "\n",
    "å‡è®¾æŸä¸ªé€šé“ï¼š\n",
    "\n",
    "Activation $X$: [100, 0.1, 0.2] (æå¤§å€¼ 100ï¼Œå¾ˆéš¾é‡åŒ–)\n",
    "\n",
    "Weight $W$: [0.5, 0.5, 0.5] (å¾ˆå‡åŒ€)\n",
    "\n",
    "æˆ‘ä»¬é€‰ $s = 10$ ($\\alpha=0.5$ é™„è¿‘)ï¼š\n",
    "\n",
    "æ–° $X'$ = [10, 0.01, 0.02] (æœ€å¤§å€¼å˜æˆäº† 10ï¼Œå®¹æ˜“é‡åŒ–å¤šäº†)\n",
    "æ–° $W'$ = [5, 5, 5] (å˜æˆäº† 5ï¼Œè™½ç„¶å˜å¤§äº†ï¼Œä½†è¿˜æ˜¯å‡åŒ€çš„)\n",
    "\n",
    "ç»“è®ºï¼š é€šè¿‡è¿™ä¸ªç®€å•çš„æ•°å­¦å˜æ¢ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº† Activation ä¸­çš„ææ€–å°–å³°ï¼Œä»£ä»·ä»…ä»…æ˜¯è®© Weight ç¨å¾®éš¾ä¸€ç‚¹ç‚¹ï¼Œå®ç°äº†æ•´ä½“æœ€ä¼˜çš„ INT8 é‡åŒ–ã€‚\n",
    "\n",
    "æ•°å­¦ï¼š\n",
    "\n",
    "å¯¹çº¿æ€§å±‚ $y = XW$ï¼š\n",
    "\n",
    "- å®šä¹‰ per-channel å› å­ $s_c$\n",
    "- å˜æ¢ï¼š$X' = X D^{-\\alpha},\\ W' = D^{\\alpha} W$\n",
    "- ä½¿å¾— X'ã€W' éƒ½æ›´å®¹æ˜“è¢« INT8 é‡åŒ–ã€‚\n",
    "\n",
    "### 1.3.2 æ–‡çŒ®\n",
    "\n",
    "- *SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models*, arXiv:2211.10438."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9351ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 3.0517578125e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def smoothquant_linear(X, W, alpha=0.5, eps=1e-5):\n",
    "    \"\"\"\n",
    "    æŒ‰è¾“å…¥é€šé“åš SmoothQuant ç¼©æ”¾ã€‚\n",
    "    å‡è®¾ W ä¸º nn.Linear.weight, å½¢çŠ¶ä¸º [out_features, in_features]\n",
    "    X å½¢çŠ¶ä¸º [batch, seq, in_features]\n",
    "    \"\"\"\n",
    "    # 1. è®¡ç®— Activation çš„é€é€šé“æœ€å¤§å€¼ (æ²¿ç€ Batch å’Œ Seq ç»´åº¦)\n",
    "    # X: [B, L, D_in] -> act_max: [D_in]\n",
    "    act_max = X.abs().amax(dim=(0, 1))\n",
    "    \n",
    "    # 2. è®¡ç®— Weight çš„é€é€šé“æœ€å¤§å€¼ (æ²¿ç€ Output Channel ç»´åº¦ï¼Œå³ dim=0)\n",
    "    # W: [D_out, D_in] -> wt_max: [D_in]\n",
    "    # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»æ˜¯ dim=0ï¼Œå› ä¸ºæˆ‘ä»¬è¦çœ‹æ¯ä¸€åˆ—(æ¯ä¸ªè¾“å…¥é€šé“)çš„æœ€å¤§å€¼\n",
    "    wt_max = W.abs().amax(dim=0) \n",
    "\n",
    "    # 3. è®¡ç®—å¹³æ»‘ç³»æ•° s: [D_in]\n",
    "    s = (act_max ** alpha) / (wt_max ** (1 - alpha) + eps)\n",
    "    s = torch.clamp(s, min=eps)\n",
    "\n",
    "    # 4. åº”ç”¨ç¼©æ”¾\n",
    "    # X: [B, L, D_in] / [D_in] -> å¹¿æ’­æ­£ç¡®\n",
    "    X_sq = X / s\n",
    "    \n",
    "    # W: [D_out, D_in] * [D_in] -> PyTorch é»˜è®¤å¹¿æ’­æœ€åä¸€ç»´ï¼Œæ­£ç¡®\n",
    "    W_sq = W * s\n",
    "    \n",
    "    return X_sq, W_sq\n",
    "\n",
    "X = torch.randn(4, 128, 512)\n",
    "W = torch.randn(2048, 512)\n",
    "X2, W2 = smoothquant_linear(X, W)\n",
    "\n",
    "# original\n",
    "Y_orig = X @ W.T\n",
    "Y_new = X2 @ W2.T\n",
    "\n",
    "print(\"Max diff:\", (Y_orig - Y_new).abs().max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43ff45",
   "metadata": {},
   "source": [
    "## 1.4 GPTQï¼ˆLLM ä½æ¯”ç‰¹ PTQï¼‰\n",
    "\n",
    "GPTQ ä½¿ç”¨è¿‘ä¼¼ Hessianï¼ˆé€šå¸¸å¯¹è§’æˆ– block å¯¹è§’ï¼‰è¡¡é‡æ¯ä¸ªæƒé‡çš„é‡è¦æ€§ï¼Œåœ¨ç»™å®š bit ä¸‹æ±‚è§£è¿‘ä¼¼æœ€ä¼˜çš„æƒé‡é‡åŒ–æ–¹æ¡ˆã€‚\n",
    "\n",
    "### 1.4.1 æ•°å­¦åŸºç¡€\n",
    "GPTQ = â€œHessian åŠ æŒçš„æƒé‡é‡åŒ– PTQâ€ï¼š\n",
    "\n",
    "- åªåš weight-only quantizationï¼ˆæ¿€æ´»ä¿ç•™åœ¨ FP16/BF16ï¼‰ï¼Œä¸»è¦ä½œç”¨åœ¨ Transformer é‡Œçš„ Linear / Conv1Dï¼ˆQKV æŠ•å½±ã€MLP ç­‰ï¼‰\n",
    "\n",
    "- å…¸å‹é…ç½®ï¼š3â€“4 bit æƒé‡ + æ¯ groupï¼ˆä¾‹å¦‚ 128 ä¸ªé€šé“ï¼‰ä¸€ä¸ª scaleï¼Œå¯ä»¥æŠŠ LLM å‹åˆ° 4bit è¿˜èƒ½ä¿æŒä¸é”™ç²¾åº¦\n",
    "- æ˜¯ post-trainingã€one-shotï¼šåªç”¨ä¸€å°æ®µæ ¡å‡†æ•°æ®ï¼Œä¸éœ€è¦å†è®­ã€‚\n",
    "\n",
    "å…³é”®ç‚¹ï¼šä¸æ˜¯ naive round-to-nearestï¼Œè€Œæ˜¯ç”¨ä¸€å±‚çš„ è¾“å…¥ Hessianï¼ˆäºŒé˜¶ä¿¡æ¯ï¼‰æ¥åº¦é‡ã€Œå“ªäº›æ–¹å‘çš„è¯¯å·®æ›´è¦å‘½ã€ã€‚\n",
    "æœ‰ç‚¹åƒã€ŒOptimal Brain Surgeon/Optimal Brain Quantizationã€åœ¨ LLM ä¸Šçš„å·¥ç¨‹åŒ–ç‰ˆæœ¬ï¼š**é€šè¿‡ Hessian æƒé‡çš„äºŒé˜¶è¿‘ä¼¼ï¼ŒæŠŠé‡åŒ–è¯¯å·®å¾€â€œä¸é‡è¦çš„æƒé‡æ–¹å‘â€æ¨**ã€‚\n",
    "\n",
    "åšæ³•ï¼šå¯¹å•å±‚çº¿æ€§åšäºŒé˜¶è¿‘ä¼¼\n",
    "è€ƒè™‘ä¸€å±‚çº¿æ€§å±‚ï¼ˆå¿½ç•¥ biasï¼‰ï¼š\n",
    "$$Y=XW$$\n",
    "- $X \\in \\mathbb{R}^{n \\times d_\\text{in}}$ï¼š æ ¡å‡†æ•°æ®ä¸Šçš„è¾“å…¥ï¼ˆå¤š batch æ‹¼æ¥åçš„ activationï¼‰\n",
    "- $W \\in \\mathbb{R}^{d_\\text{in} \\times d_\\text{out}}$ï¼šåŸå§‹å…¨ç²¾åº¦æƒé‡ã€‚\n",
    "- æˆ‘ä»¬è¦æ‰¾é‡åŒ–åçš„ $\\tilde W$ï¼Œæ»¡è¶³ bit é™åˆ¶ï¼ˆæ¯”å¦‚ 4bitï¼‰ã€‚\n",
    "GPTQ æŠŠã€Œè¾“å‡ºé‡å»ºè¯¯å·®ã€è¿‘ä¼¼ä¸ºï¼š\n",
    "$$\\min_{\\tilde W \\in \\mathcal{Q}} \\| XW - X\\tilde W \\|_F^2$$\n",
    "åšäºŒé˜¶å±•å¼€ï¼Œå¯å†™æˆå¯¹æ¯ä¸€åˆ—$w_j$çš„äºŒæ¬¡å‹ï¼š\n",
    "$$L \\approx (w_j - \\tilde w_j)^\\top H (w_j - \\tilde w_j)$$\n",
    "å…¶ä¸­$H \\approx 2 X^\\top X + \\lambda I$æ˜¯è¾“å…¥çš„ Hessian è¿‘ä¼¼ï¼ˆğœ†æ˜¯ä¸€ä¸ªå°çš„ damping é¡¹ï¼‰\n",
    "\n",
    "æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "- ä¸æ˜¯ç®€å•æœ€å°åŒ– $\\|w_j - \\tilde w_j\\|_2^2$, è€Œæ˜¯æœ€å°åŒ–å¸¦æƒè·ç¦»$(w_j - \\tilde w_j)^\\top H (w_j - \\tilde w_j)$\n",
    "- å¦‚æœæŸæ–¹å‘åœ¨$H$é‡Œæƒé‡å¤§ï¼Œè¯´æ˜æ”¹åŠ¨è¿™ä¸ªæ–¹å‘å¯¼è‡´è¾“å‡ºå˜åŒ–å¾ˆå¤§ â†’ é‡åŒ–æ—¶è¦æ ¼å¤–å°å¿ƒï¼›åä¹‹ï¼Œå¯ä»¥æ›´â€œéšä¾¿â€åœ°å‹ç¼©ã€‚\n",
    "\n",
    "### 1.4.3 ç®—æ³•\n",
    "å¯¹æŸä¸€å±‚ï¼ˆä¾‹å¦‚ä¸€ä¸ª `nn.Linear`ï¼‰ï¼š\n",
    "\n",
    "1. æ”¶é›†æ ¡å‡†æ¿€æ´» X\n",
    "    - ç”¨ä¸€å°æ®µä»£è¡¨æ€§çš„æ–‡æœ¬è·‘ full-precision æ¨¡å‹ã€‚\n",
    "    - åœ¨è¯¥å±‚è¾“å…¥å¤„æŒ‚ forward hookï¼Œæ”¶é›†è‹¥å¹² batch çš„ activationsã€‚\n",
    "\n",
    "2. æ„å»º Hessian è¿‘ä¼¼ & å…¶é€†/Cholesky\n",
    "$$\n",
    "H = 2X^T X / N + \\lambda I, \\quad H^{-1} = \\operatorname{inv}(H)\n",
    "$$\n",
    "\n",
    "çœŸå®å®ç°é‡Œä¼šç”¨ Cholesky åˆ†è§£ $H^{-1} = LL^T$ æ¥ä¿è¯æ•°å€¼ç¨³å®šå¹¶åŠ é€ŸçŸ©é˜µæ“ä½œã€‚\n",
    "\n",
    "3. æŒ‰ block / group å¤„ç†æƒé‡åˆ—ï¼ˆout_featuresï¼‰\n",
    "æŠŠ $W$ çš„åˆ—ï¼ˆè¾“å‡ºé€šé“ï¼‰åˆ†æˆè‹¥å¹² blockï¼ˆå¤§å° Bï¼Œæ¯”å¦‚ 128ï¼‰ï¼Œæ¯ä¸ª block å†…é€åˆ—å¤„ç†ï¼š\n",
    "\n",
    "- å¯¹ block ä¸­çš„ç¬¬ $j$ åˆ— $w_j$ï¼š\n",
    "\n",
    "     - ä½¿ç”¨ k-bit å‡åŒ€å¯¹ç§°é‡åŒ– + group scale å¯¹ $w_j$ é‡åŒ–å¾—åˆ° $q_j$ï¼š\n",
    "   $$\n",
    "   q_j = \\operatorname{round}\\left(\\frac{w_j}{s}\\right) \\cdot s\n",
    "   $$\n",
    "\n",
    "     - è®¡ç®—é‡åŒ–è¯¯å·® $e_j = w_j - q_j$ã€‚\n",
    "\n",
    "     - ç”¨ $H^{-1}$ æŠŠè¿™æ¬¡è¯¯å·®å¯¹æœªæ¥åˆ—çš„å½±å“â€œé¢„å…ˆè¡¥å¿â€æ‰ï¼š\n",
    "   > ç›´è§‰æ˜¯ï¼šæ—¢ç„¶ç¬¬ j åˆ—æŸå¤±äº†ä¸€ç‚¹â€œé‡è¦æ–¹å‘â€çš„ä¿¡æ¯ï¼Œå°±åœ¨åé¢çš„åˆ—ä¸Šåå‘åŠ ä¸€ç‚¹å›æ¥ã€‚è¿™ä¸€å—å°±æ˜¯ GPTQ çš„ã€Œerror propagationã€ã€‚\n",
    "\n",
    "4. åªæŠŠ quantized æƒé‡ä¿å­˜ä¸‹æ¥\n",
    "    - æ¿€æ´»ã€KV Cache ç­‰ä¿æŒ FP16/BF16ã€‚\n",
    "    - æ¨ç†æ—¶ä½¿ç”¨ä½æ¯”ç‰¹æƒé‡ + å¯¹åº”çš„ scale/zero-point / group-wise å…ƒæ•°æ®\n",
    "\n",
    "### 1.4.3 å¸¸ç”¨é…ç½®\n",
    "åœ¨å…¬å¼€çš„ GPTQ æ¨¡å‹ / æ–‡æ¡£é‡Œï¼Œå¸¸ç”¨é…ç½®ï¼š\n",
    "\n",
    "- æƒé‡ä½å®½ï¼šwbits=4ï¼ˆæœ‰æ—¶ 3 æˆ– 2ï¼‰ã€‚\n",
    "\n",
    "- group sizeï¼šgroupsize=128 æˆ– 64ï¼ˆå¤šå°‘ä¸ª channel å…±äº«ä¸€ä¸ªé‡åŒ– scaleï¼‰ã€‚\n",
    "\n",
    "- åªé‡åŒ– language model core çš„çº¿æ€§å±‚ï¼Œå…¶ä»–éƒ¨åˆ†ï¼ˆembeddingã€lnã€vision tower ç­‰ï¼‰ä¿æŒ FP16/BF16ã€‚\n",
    "\n",
    "- æ¿€æ´»ä¿æŒ FP16/BF16ï¼Œæœ‰äº›åº“ä¼šé¢å¤–æ”¯æŒ activation INT8ï¼Œä½†ç»å…¸ GPTQ æ˜¯ weight-onlyã€‚ \n",
    "\n",
    "\n",
    "å®é™…å·¥ç¨‹é‡Œæœ€å¸¸ç”¨çš„ Python è°ƒç”¨ï¼šAutoGPTQ / HF GPTQConfig\n",
    "\n",
    "å¦‚æœä½ åªæ˜¯æƒ³åœ¨ GM/å®éªŒç¯å¢ƒé‡Œ å¿«é€ŸæŠŠä¸€ä¸ª LLM é‡åŒ–æˆ GPTQ æƒé‡å¹¶è·‘èµ·æ¥ï¼Œé€šå¸¸ä¸ä¼šè‡ªå·±ä»é›¶å†™ï¼Œè€Œæ˜¯ç”¨å·¥å…·åº“ï¼Œä¾‹å¦‚ [AutoGPTQ](https://pypi.org/project/auto-gptq) æˆ– transformers ä¸­çš„ GPTQ æ”¯æŒ. example\n",
    "\n",
    "```python\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "pretrained_model = \"facebook/opt-125m\"\n",
    "quantized_dir = \"opt-125m-gptq-4bit\"\n",
    "\n",
    "# 1) é…ç½® GPTQï¼ˆ4bit, group-size 128ï¼‰\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,                 # 3/4bit æƒé‡\n",
    "    group_size=128,         # æ¯ 128 channel ä¸€ä¸ª scale\n",
    "    damp_percent=0.01,      # Hessian damping\n",
    "    desc_act=False          # æ˜¯å¦æ’åºæ¿€æ´»ï¼ˆæœ‰äº›å®ç°ä¼šç”¨ï¼‰\n",
    ")\n",
    "\n",
    "# 2) åŠ è½½å…¨ç²¾åº¦æ¨¡å‹ï¼Œå¹¶è¿›è¡Œ GPTQ é‡åŒ–\n",
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    pretrained_model,\n",
    "    quantize_config=quantize_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3) ä½¿ç”¨ä¸€å°æ®µæ ¡å‡†æ•°æ®è·‘ GPTQï¼ˆåº“å†…éƒ¨ä¼šåš collect X + Hessian + è¯¯å·®è¡¥å¿ï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model, use_fast=True)\n",
    "calib_texts = [\n",
    "    \"Quantization is a powerful technique for deploying large language models efficiently.\",\n",
    "    \"GPTQ is a Hessian-aware post-training quantization algorithm.\"\n",
    "]\n",
    "calib_examples = [tokenizer(t, return_tensors=\"pt\") for t in calib_texts]\n",
    "\n",
    "model.quantize(calib_dataset=calib_examples)\n",
    "\n",
    "# 4) ä¿å­˜é‡åŒ–åçš„æƒé‡\n",
    "model.save_quantized(quantized_dir)\n",
    "\n",
    "# 5) å†æ¬¡åŠ è½½é‡åŒ–æ¨¡å‹è¿›è¡Œæ¨ç†\n",
    "quantized_model = AutoGPTQForCausalLM.from_quantized(\n",
    "    quantized_dir,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "```\n",
    "\n",
    "### 1.4.4 æ–‡çŒ®\n",
    "\n",
    "- Frantar et al., *GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers*, arXiv:2210.17323  \n",
    "- å®˜æ–¹å®ç°ï¼šhttps://github.com/IST-DASLab/gptq\n",
    "\n",
    "### 1.4.5 e2e drive modelæ³¨æ„ç‚¹\n",
    "1. GPTQ æ˜¯ weight-only quantization â†’ å¯¹ transformer E2E pipeline é™ä½ä¸äº†è¶³å¤Ÿçš„ latency\n",
    "GPTQ ä¼˜åŒ–ç‚¹ï¼š\n",
    "\n",
    "- åªå‹ weightsï¼ˆ3â€“4bitï¼‰\n",
    "\n",
    "- æ¿€æ´»ä»ç„¶æ˜¯ FP16/BF16\n",
    "\n",
    "- KV cache/attention ä»ç„¶æ˜¯ FP16/BF16\n",
    "\n",
    "- compute bound å¤§å¤´ä»ç„¶åœ¨ activation matmul / attentionï¼Œä¸æ˜¯æƒé‡æœ¬èº«\n",
    "\n",
    "ğŸ‘‰ å¯¹ LLAMA è¿™ç§ decoder LLM å¯ä»¥çœæ¨¡å‹å¤§å°\n",
    "ä½†å¯¹ AD çš„ compute-bound Vision/Transformer latency å‡ ä¹ä¸é™ã€‚\n",
    "\n",
    "æ‰€ä»¥ AD åœºæ™¯ç”¨ GPTQï¼šæ”¶ç›Šå¤ªå°ï¼Œä¸å€¼å¾—ã€‚\n",
    "\n",
    "2. è½¦ç«¯çš„æ¨¡å‹ä¸åƒ LLM æ˜¯â€œweights dominatedâ€ï¼›AD æ¨¡å‹æ˜¯ activation dominated\n",
    "\n",
    "è§†è§‰éƒ¨åˆ†ï¼ˆä¾‹å¦‚ FasterViTï¼‰å’Œ BEV encoder ç±»æ¨¡å—ï¼š\n",
    "\n",
    "- activation tensor å¤§ä¸” dynamic\n",
    "\n",
    "- conv/attention FLOPs ä¸»å¯¼ latency\n",
    "\n",
    "- weight size æœ¬æ¥å°±ä¸å¤§ï¼ˆå‡ å~å‡ ç™¾ MBï¼‰\n",
    "\n",
    "- GPTQ åªå‹ weightsï¼Œå¯¹ç®—åŠ›å æ¯”ä¸åˆ° 15% ï¼ˆæœ‰å¾…éªŒè¯ï¼‰çš„éƒ¨åˆ†åšä¼˜åŒ–ï¼Œå¯¹latencyæ²¡æœ‰å¤ªå¤§å½±å“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895d1db",
   "metadata": {},
   "source": [
    "### GPTQ äºŒé˜¶è¿‘ä¼¼æ¨å¯¼\n",
    "\n",
    "#### 1. é—®é¢˜è®¾å®šï¼šå•å±‚çº¿æ€§ä¸é‡åŒ–ç›®æ ‡\n",
    "\n",
    "è€ƒè™‘ä¸€å±‚çº¿æ€§å±‚ï¼ˆå¿½ç•¥ biasï¼‰ï¼š\n",
    "\n",
    "$Y = X W$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $X \\in \\mathbb{R}^{N \\times d_{\\text{in}}}$ï¼šæ ¡å‡†æ•°æ®ä¸Šçš„è¾“å…¥ï¼ˆæ‰€æœ‰ batch æ‹¼åœ¨ä¸€èµ·ï¼‰\n",
    "- $W \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$ï¼šåŸå§‹å…¨ç²¾åº¦æƒé‡\n",
    "- $\\tilde W$ï¼šé‡åŒ–åçš„æƒé‡ï¼ˆæ»¡è¶³ k-bit çº¦æŸï¼‰\n",
    "\n",
    "æˆ‘ä»¬å¸Œæœ›é‡åŒ–åè¾“å‡ºå°½é‡æ¥è¿‘åŸè¾“å‡ºï¼Œæ•´ä½“ç›®æ ‡å¯ä»¥å†™æˆï¼š\n",
    "\n",
    "$\\min_{\\tilde W \\in \\mathcal{Q}} \\|XW - X\\tilde W\\|_F^2$\n",
    "\n",
    "å¯¹ç¬¬ $j$ ä¸ªè¾“å‡ºé€šé“ï¼ˆå³ç¬¬ $j$ åˆ—æƒé‡ï¼‰å•ç‹¬æ¥çœ‹ï¼š\n",
    "\n",
    "- åŸå§‹åˆ—ï¼š$w_j \\in \\mathbb{R}^{d_{\\text{in}}}$\n",
    "- é‡åŒ–åˆ—ï¼š$\\tilde w_j \\in \\mathbb{R}^{d_{\\text{in}}}$\n",
    "\n",
    "è¯¥åˆ—å¯¹åº”çš„è¾“å‡ºè¯¯å·®ä¸ºï¼š\n",
    "\n",
    "$L = \\|X w_j - X \\tilde w_j\\|_2^2$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. å†™æˆæƒé‡å·®çš„å½¢å¼ï¼šå¾—åˆ°äºŒæ¬¡å‹\n",
    "\n",
    "å®šä¹‰è¯¥åˆ—çš„æƒé‡è¯¯å·®ï¼š\n",
    "\n",
    "$\\delta w_j = w_j - \\tilde w_j$\n",
    "\n",
    "åˆ™ï¼š\n",
    "\n",
    "$L = \\|X(w_j - \\tilde w_j)\\|_2^2 = \\|X \\delta w_j\\|_2^2$\n",
    "\n",
    "å±•å¼€ï¼š\n",
    "\n",
    "$L = (X \\delta w_j)^\\top (X \\delta w_j) = \\delta w_j^\\top (X^\\top X)\\, \\delta w_j$\n",
    "\n",
    "å…³é”®ç»“è®ºï¼š\n",
    "\n",
    "- æƒé‡è¯¯å·® $\\delta w_j$ ç»ç”±è¾“å…¥çŸ©é˜µ $X$ æ‰©æ•£åˆ°è¾“å‡ºç©ºé—´  \n",
    "- è¯¯å·®çš„â€œä¸¥é‡ç¨‹åº¦â€ç”± $X^\\top X$ å†³å®š  \n",
    "- è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„äºŒæ¬¡å‹ï¼ˆquadratic formï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. ä¸äºŒé˜¶æ³°å‹’å±•å¼€ï¼ˆHessian è¿‘ä¼¼ï¼‰çš„è”ç³»\n",
    "\n",
    "åœ¨ PTQ ä¸­ï¼Œæ¨¡å‹å·²è®­ç»ƒå®Œæ¯•ï¼Œæ»¡è¶³ï¼š\n",
    "\n",
    "$\\nabla \\mathcal{L}(w_j) \\approx 0$\n",
    "\n",
    "å¯¹ $\\mathcal{L}(w_j)$ åšäºŒé˜¶æ³°å‹’å±•å¼€ï¼š\n",
    "\n",
    "$\\mathcal{L}(w_j + \\delta w_j)\n",
    "\\approx\n",
    "\\mathcal{L}(w_j)\n",
    "+\n",
    "\\nabla \\mathcal{L}(w_j)^\\top \\delta w_j\n",
    "+\n",
    "\\frac{1}{2}\\delta w_j^\\top H \\, \\delta w_j$\n",
    "\n",
    "ç”±äºä¸€é˜¶æ¢¯åº¦ä¸ºé›¶ï¼ŒæŸå¤±å˜åŒ–ä¸»è¦æ¥è‡ªäºŒé˜¶é¡¹ï¼š\n",
    "\n",
    "$\\Delta \\mathcal{L}\n",
    "\\approx\n",
    "\\frac{1}{2}\\delta w_j^\\top H \\, \\delta w_j$\n",
    "\n",
    "GPTQ é‡‡ç”¨è¾“å…¥ç»Ÿè®¡çš„ Hessian è¿‘ä¼¼ï¼š\n",
    "\n",
    "$H \\approx 2 X^\\top X + \\lambda I$\n",
    "\n",
    "å…¶ä¸­ $\\lambda I$ æ˜¯ damping é¡¹ï¼Œç”¨äºæé«˜æ•°å€¼ç¨³å®šæ€§ã€‚\n",
    "\n",
    "ä»£å…¥å¾—ï¼š\n",
    "\n",
    "$L \\approx \\delta w_j^\\top H \\, \\delta w_j\n",
    "= (w_j - \\tilde w_j)^\\top H (w_j - \\tilde w_j)$\n",
    "\n",
    "è¿™å°±æ˜¯ GPTQ çš„æ ¸å¿ƒä¼˜åŒ–ç›®æ ‡ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. å¯¹æ¯”æ™®é€š L2 é‡åŒ–ï¼šä¸ºä»€ä¹ˆè¦å¼•å…¥ Hessianï¼Ÿ\n",
    "\n",
    "æ™®é€šçš„ PTQ ç­‰ä»·äºï¼š\n",
    "\n",
    "$\\min_{\\tilde w_j} \\|w_j - \\tilde w_j\\|_2^2$\n",
    "\n",
    "å‡è®¾æ‰€æœ‰æ–¹å‘é‡åŒ–æ•æ„Ÿåº¦ç›¸åŒã€‚\n",
    "\n",
    "GPTQ çš„ç›®æ ‡æ˜¯ï¼š\n",
    "\n",
    "$\\min_{\\tilde w_j}\n",
    "(w_j - \\tilde w_j)^\\top H (w_j - \\tilde w_j)$\n",
    "\n",
    "å«ä¹‰ï¼š\n",
    "\n",
    "- è‹¥ $H$ æŸæ–¹å‘ç‰¹å¾å€¼å¤§ â†’ å¯¹ loss æ›´æ•æ„Ÿ â†’ è¯¥æ–¹å‘é‡åŒ–è¯¯å·®å¿…é¡»æ›´å°  \n",
    "- è‹¥ç‰¹å¾å€¼å° â†’ å¯¹ loss ä¸æ•æ„Ÿ â†’ å¯ä»¥æ›´æ¿€è¿›å‹ç¼©  \n",
    "\n",
    "å› æ­¤ GPTQ èƒ½åœ¨ç›¸åŒ bit æ•°ä¸‹é™ä½ç²¾åº¦æŸå¤±ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. å°ç»“\n",
    "\n",
    "ä» $\\|XW - X\\tilde W\\|_F^2$ å‡ºå‘ï¼š\n",
    "\n",
    "1. åˆ†è§£å•åˆ—ï¼š  \n",
    "   $L = \\|X w_j - X \\tilde w_j\\|_2^2$\n",
    "\n",
    "2. æ”¹å†™ä¸ºæƒé‡è¯¯å·®ï¼š  \n",
    "   $L = \\delta w_j^\\top (X^\\top X) \\delta w_j$\n",
    "\n",
    "3. ç”¨äºŒé˜¶è¿‘ä¼¼ï¼ˆæ”¶æ•›ç‚¹ä¸€é˜¶é¡¹ä¸ºé›¶ï¼‰ï¼š  \n",
    "   $\\Delta \\mathcal{L} \\approx \\frac{1}{2}\\delta w_j^\\top H \\delta w_j$\n",
    "\n",
    "4. ç”¨ Hessian è¿‘ä¼¼ $H \\approx 2X^\\top X + \\lambda I$ï¼š  \n",
    "   $L \\approx (w_j - \\tilde w_j)^\\top H (w_j - \\tilde w_j)$\n",
    "\n",
    "æœ€ç»ˆ GPTQ çš„ç›®æ ‡ä¸ºï¼š\n",
    "\n",
    "$\\min_{\\tilde w_j \\in \\mathcal{Q}}\n",
    "(w_j - \\tilde w_j)^\\top H (w_j - \\tilde w_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be44971",
   "metadata": {},
   "source": [
    "### 1.5 AWQï¼ˆActivation-aware Weight Quantizationï¼‰\n",
    "\n",
    "AWQ æ˜¯ä¸€ç§ **weight-only ä½æ¯”ç‰¹é‡åŒ–** æ–¹æ³•ï¼ˆå…¸å‹æ˜¯ W4A16ï¼‰ï¼Œæ ¸å¿ƒä¸¤ä»¶äº‹ï¼š\n",
    "\n",
    "1. å‘ç°ï¼š**æƒé‡æœ¬èº«é•¿å¾—â€œæ€ªä¸æ€ªâ€ä¸é‡è¦ï¼Œé‡è¦çš„æ˜¯è¿™ä¸ªé€šé“åœ¨çœŸå®æ¿€æ´»åˆ†å¸ƒä¸‹çš„å½±å“æœ‰å¤šå¤§**ã€‚  \n",
    "2. åšæ³•ï¼šåœ¨ä¸æ”¹ kernel å½¢å¼ï¼ˆä»ç„¶æ˜¯ group-wise weight-only quantï¼‰çš„å‰æä¸‹ï¼Œé€šè¿‡ä¸€ä¸ª **æ•°å­¦ç­‰ä»·å˜æ¢**ï¼š\n",
    "   - å¯¹â€œé‡è¦é€šé“â€çš„æƒé‡æ”¾å¤§ï¼ˆscale upï¼‰ï¼Œ\n",
    "   - å¯¹åº”åœ°å¯¹è¾“å…¥æ¿€æ´»ç¼©æ”¾å›æ¥ï¼ˆscale downï¼‰ï¼Œ\n",
    "   - è®©è¿™äº›é€šé“åœ¨åŒä¸€ä¸ª group å†…å åˆ°æ›´å¤§æ¯”é‡ï¼Œä»è€Œ **åœ¨ group-wise é‡åŒ–ä¸‹å¾—åˆ°æ›´å°çš„é‡åŒ–è¯¯å·®**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.1 å…ˆä» group-wise weight-only quant çš„è¯¯å·®å‡ºå‘\n",
    "\n",
    "è€ƒè™‘ä¸€å±‚çº¿æ€§å±‚ï¼ˆå¿½ç•¥ biasï¼‰ï¼š\n",
    "\n",
    "- è¾“å…¥ï¼š$x \\in \\mathbb{R}^{d_{\\text{in}}}$\n",
    "- æƒé‡ï¼š$w \\in \\mathbb{R}^{d_{\\text{in}}}$ï¼ˆå…ˆåªçœ‹å•è¾“å‡ºé€šé“ï¼‰\n",
    "- è¾“å‡ºï¼š$y = w^\\top x$\n",
    "\n",
    "å¯¹æƒé‡åš **weight-only é‡åŒ–**ï¼Œæ¯”å¦‚å¯¹ $w$ åšç»Ÿä¸€çš„å®šç‚¹é‡åŒ–ï¼š\n",
    "\n",
    "- é‡åŒ–æ­¥é•¿ï¼ˆscaleï¼‰  \n",
    "  $$\n",
    "  \\Delta = \\frac{\\max\\lvert w \\rvert}{2^{b-1}-1}\n",
    "  $$\n",
    "- é‡åŒ–æƒé‡  \n",
    "  $$\n",
    "  Q(w) = \\Delta \\cdot \\text{Round}\\!\\left(\\frac{w}{\\Delta}\\right)\n",
    "  $$\n",
    "\n",
    "æ¨ç†æ—¶ç”¨çš„æ˜¯\n",
    "\n",
    "$$\n",
    "\\hat{y} = Q(w)^\\top x\n",
    "$$\n",
    "\n",
    "äºæ˜¯é‡åŒ–è¯¯å·®ä¸º\n",
    "\n",
    "$$\n",
    "e = \\hat{y} - y = \\big(Q(w) - w\\big)^\\top x\n",
    "= \\left(\\Delta \\cdot \\text{Round}\\!\\left(\\frac{w}{\\Delta}\\right) - w\\right)^\\top x\n",
    "$$\n",
    "\n",
    "æŠŠ **Round è¯¯å·®** å•ç‹¬è®°å‡ºæ¥ï¼š\n",
    "\n",
    "$$\n",
    "\\text{RoundErr}\\!\\left(\\frac{w}{\\Delta}\\right)\n",
    "= \\text{Round}\\!\\left(\\frac{w}{\\Delta}\\right) - \\frac{w}{\\Delta}\n",
    "$$\n",
    "\n",
    "ä»é‡åŒ–å®šä¹‰å‡ºå‘ï¼š\n",
    "$$Q(w)\n",
    "= \\Delta \\cdot \\text{Round}\\left(\\frac{w}{\\Delta}\\right)$$\n",
    "\n",
    "æŠŠå³è¾¹æ‹†æˆä¸¤é¡¹ï¼š\n",
    "$$Q(w)\n",
    "= \\Delta \\left[\n",
    "\\frac{w}{\\Delta}\n",
    "+ \\left(\n",
    "\\text{Round}\\left(\\frac{w}{\\Delta}\\right)\n",
    "- \\frac{w}{\\Delta}\n",
    "\\right)\n",
    "\\right]$$\n",
    "\n",
    "æ•´ç†æ‹¬å·ï¼š\n",
    "$$Q(w)\n",
    "= w + \\Delta \\cdot \n",
    "\\left(\n",
    "\\text{Round}\\left(\\frac{w}{\\Delta}\\right)\n",
    "- \\frac{w}{\\Delta}\n",
    "\\right)$$\n",
    "\n",
    "æˆ‘ä»¬å®šä¹‰çš„ RoundErr æ­£å¥½æ˜¯æ‹¬å·é‡Œçš„è¿™ä¸€é¡¹ï¼Œæ‰€ä»¥ï¼š\n",
    "$$Q(w)\n",
    "= w + \\Delta \\cdot \\text{RoundErr}\\left(\\frac{w}{\\Delta}\\right)$$\n",
    "\n",
    "äºæ˜¯é‡åŒ–è¯¯å·®ï¼š\n",
    "$$\n",
    "Q(w) - w\n",
    "= \\Delta \\cdot \\text{RoundErr}\\!\\left(\\frac{w}{\\Delta}\\right)\n",
    "$$\n",
    "\n",
    "ä»£å›å»ï¼š\n",
    "\n",
    "$$\n",
    "e = \\Delta \\cdot \\text{RoundErr}\\!\\left(\\frac{w}{\\Delta}\\right)^\\top x\n",
    "$$\n",
    "\n",
    "å¦‚æœå‡è®¾ $\\frac{w}{\\Delta}$ åœ¨æ¯ä¸ªé‡åŒ– bin å†…â€œæ¯”è¾ƒå‡åŒ€â€ï¼Œäºæ˜¯ **æœŸæœ›çš„é‡åŒ–ç»å¯¹è¯¯å·®** æ»¡è¶³ä¸€ä¸ªéå¸¸ç²—ä½†é‡è¦çš„å…³ç³»ï¼š\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\lvert e \\rvert\n",
    "\\propto \\Delta \\cdot \\mathbb{E}\\lvert x \\rvert\n",
    "$$\n",
    "\n",
    "- å¯¹åŒä¸€ä¸ª group æ¥è¯´ï¼Œ$\\Delta$ è¢« **æ‰€æœ‰é€šé“å…±äº«**ï¼ˆgroup-wise quantï¼‰ã€‚\n",
    "- å› æ­¤ï¼Œåœ¨åŒä¸€ä¸ª group å†…ï¼Œâ€œè°çš„ $x$ è¶Šå¤§ï¼Œè°çš„è¯¯å·®è´¡çŒ®è¶Šå¤§â€ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯ AWQ çš„å…³é”®è§‚å¯Ÿä¹‹ä¸€ï¼š\n",
    "\n",
    "> **åœ¨ group-wise weight-only quant é‡Œï¼Œé‡åŒ–è¯¯å·®å…¶å®æ˜¯ activation-aware çš„ï¼š  \n",
    "> æ¿€æ´»è¶Šå¤§çš„é€šé“è¶Šâ€œè„†å¼±â€ï¼Œæ›´å€¼å¾—è¢«ä¿æŠ¤ã€‚**\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.2 æ•°å­¦ç­‰ä»·å˜æ¢ï¼šscale æƒé‡ã€åå‘ scale æ¿€æ´»\n",
    "\n",
    "æˆ‘ä»¬å†çœ‹ä¸€å±‚çº¿æ€§å±‚çš„çŸ©é˜µå½¢å¼ï¼ˆå¿½ç•¥ biasï¼‰ï¼š\n",
    "\n",
    "- è¾“å…¥ batchï¼š$X \\in \\mathbb{R}^{n \\times d_{\\text{in}}}$\n",
    "- æƒé‡ï¼š$W \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$\n",
    "- è¾“å‡ºï¼š$Y = X W$\n",
    "\n",
    "AWQ çš„æ ¸å¿ƒ trick æ˜¯ä¸€ä¸ª **ç­‰ä»·å˜æ¢**ï¼š\n",
    "\n",
    "å¼•å…¥ä¸€ä¸ªå¯¹ output é€šé“çš„ç¼©æ”¾å‘é‡ $s \\in \\mathbb{R}^{d_{\\text{out}}}$ï¼Œæ„é€ å¯¹è§’çŸ©é˜µ  \n",
    "$S = \\mathrm{diag}(s) \\in \\mathbb{R}^{d_{\\mathrm{out}} \\times d_{\\mathrm{out}}}$ï¼Œåˆ™æœ‰\n",
    "\n",
    "$$\n",
    "Y = X W\n",
    "= X (W S) S^{-1}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­\n",
    "\n",
    "- $\\tilde{W} = W S \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$ï¼š**æŠŠæƒé‡æŒ‰é€šé“æ”¾å¤§/ç¼©å°**\n",
    "- $Y_j = (X W_j) \\cdot s_j^{-1}, \\quad j = 1 \\dots d_{\\mathrm{out}}$ï¼š**æŠŠè¾“å…¥æ¿€æ´»æŒ‰ç›¸åå› å­ç¼©æ”¾**\n",
    "- $S^{-1}$ æ˜¯(å®ƒä½œç”¨åœ¨è¾“å‡ºé€šé“ç»´åº¦ï¼Œä¸æ˜¯è¾“å…¥é€šé“ç»´åº¦)ï¼š\n",
    "$$\\begin{array}{c} S^{-1} = \n",
    "\\begin{bmatrix}\n",
    "s_1^{-1} & 0 & \\cdots & 0 \\\\\n",
    "0 & s_2^{-1} & \\cdots & 0 \\\\\n",
    "\\vdots & & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & s_{d_{\\mathrm{out}}}^{-1}\n",
    "\\end{bmatrix} \\end{array}$$\n",
    "\n",
    "\n",
    "æ˜¾ç„¶è¿™æ˜¯å®Œå…¨ç­‰ä»·çš„å˜æ¢ï¼ŒFP32 ä¸‹æ•°å€¼ç†è®ºä¸Šä¸å˜ã€‚\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å¯¹ $\\tilde{W}$ åš group-wise weight-only é‡åŒ–ï¼š\n",
    "\n",
    "$$\n",
    "\\hat{Y}\n",
    "= \\tilde{X} \\, Q(\\tilde{W})\n",
    "= X S^{-1} Q(W S)\n",
    "$$\n",
    "\n",
    "æ‰€ä»¥æˆ‘ä»¬å®é™…åšçš„æ˜¯ï¼š\n",
    "\n",
    "- **ç¦»çº¿**ï¼šæ‰¾åˆ°åˆé€‚çš„ $S$ï¼Œå¯¹ $W S$ åš group-wise é‡åŒ–ï¼Œå­˜æˆå®šç‚¹æƒé‡ã€‚  \n",
    "- **åœ¨çº¿æ¨ç†**ï¼šåœ¨ Linear å‰é¢æ’ä¸€ä¸ª cheap çš„ per-channel scaleï¼ˆæˆ–è€… fuse è¿› kernelï¼‰ï¼š\n",
    "  $$\n",
    "  \\hat{Y} = (X S^{-1}) \\cdot Q(W S)\n",
    "  $$\n",
    "\n",
    "AWQ çš„é—®é¢˜å°±å˜æˆäº†ï¼šå¦‚ä½•é€‰ $S$ ä½¿å¾—\n",
    "\n",
    "$$\n",
    "\\hat{Y} = X S^{-1} Q(W S) \\approx Y = X W\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.3 ç›®æ ‡å‡½æ•°ï¼šç”¨ activation ç»Ÿè®¡é€‰æ‹©â€œé‡è¦é€šé“â€\n",
    "\n",
    "åœ¨ä¸€å±‚é‡Œï¼Œæˆ‘ä»¬å¸Œæœ›å‡å°é‡åŒ–åçš„è¾“å‡ºè¯¯å·®ï¼š\n",
    "\n",
    "$$\n",
    "\\min_{S} \\; \\mathbb{E}_{X}\\big\\|X W - X S^{-1} Q(W S)\\big\\|_F^2\n",
    "$$\n",
    "\n",
    "çœŸå®æƒ…å†µä¸­ï¼š\n",
    "\n",
    "- æˆ‘ä»¬åªæœ‰ä¸€ä¸ª **æ ¡å‡†é›†**ï¼ˆcalibration setï¼‰$\\{X^{(k)}\\}_{k=1}^K$ï¼Œé€šå¸¸æ˜¯ä»è®­ç»ƒ/æ¨ç†åˆ†å¸ƒé‡Œé‡‡æ ·è‹¥å¹² batchã€‚\n",
    "- AWQ åœ¨å®ç°ä¸Šé‡‡ç”¨è‹¥å¹²è¿‘ä¼¼ï¼Œä½¿å¾—æœç´¢å˜å¾— **å±€éƒ¨ã€åˆ†ç»„ã€one-shot**ï¼Œä¸éœ€è¦åå‘ä¼ æ’­ã€‚\n",
    "\n",
    "å…³é”® heuristicï¼š\n",
    "\n",
    "1. **é‡è¦é€šé“åˆ¤å®š**  \n",
    "   å¯¹äºæŸä¸€å±‚çš„è¾“å‡ºé€šé“ $j$ï¼Œç”¨æ ¡å‡†æ•°æ®è®¡ç®—ï¼š\n",
    "   $$\n",
    "   I_j = \\mathbb{E}_X\\lvert Y_{:, j} \\rvert \n",
    "   = \\mathbb{E}_X\\lvert (X W)_{:, j} \\rvert\n",
    "   $$\n",
    "   æˆ–è€…ç”¨ max/percentile æ¥åº¦é‡è¿™ä¸ªé€šé“åœ¨çœŸå®æ¿€æ´»ä¸‹çš„â€œèƒ½é‡â€ã€‚\n",
    "\n",
    "   $I_j$ è¶Šå¤§ï¼Œè¯´æ˜è¿™ä¸ªé€šé“åœ¨è¾“å‡ºä¸­è´¡çŒ®è¶Šå¤§ï¼Œ**è¶Šè„†å¼±**ï¼Œåº”è¯¥åœ¨é‡åŒ–æ—¶è¢«ä¿æŠ¤ã€‚\n",
    "\n",
    "2. **åªå¯¹â€œé‡è¦é€šé“â€æé«˜ scale**  \n",
    "   å¯¹ $I_j$ æ’åºï¼Œåªé€‰ top-$p\\%$ çš„é€šé“ï¼ˆæ¯”å¦‚ 1%â€“5%ï¼‰èµ‹äºˆæ›´å¤§çš„ç¼©æ”¾å› å­ $s_j > 1$ï¼Œ  \n",
    "   å…¶ä½™é€šé“ä¿æŒ $s_j \\approx 1$ã€‚\n",
    "\n",
    "3. **åœ¨ group å†… search / greedily ä¼˜åŒ– $s_j$**  \n",
    "   åœ¨åŒä¸€ä¸ª group é‡Œï¼Œæƒé‡å…±äº«ä¸€ä¸ªé‡åŒ–æ­¥é•¿ $\\Delta_{\\text{group}}$ã€‚  \n",
    "   æˆ‘ä»¬å¸Œæœ›æ”¾å¤§é‡è¦é€šé“ $j$ çš„æƒé‡ $W_{:, j}$ï¼ŒåŒæ—¶ä¸è¦æŠŠ group çš„æ€»ä½“èŒƒå›´æ”¾å¤§å¾—å¤ªç¦»è°±ï¼Œå¦åˆ™ $\\Delta_{\\text{group}}$ å˜å¤§åˆä¼šä¼¤å®³å…¶ä»–é€šé“ã€‚  \n",
    "   å®ç°é‡Œé€šå¸¸åœ¨ä¸€ä¸ªç¦»æ•£å€™é€‰é›†åˆä¸­ï¼ˆæ¯”å¦‚ $\\{1, 2, 4, 8\\}$ï¼‰æœç´¢åˆé€‚çš„ $s_j$ï¼Œç”¨ calibration çš„é‡å»ºè¯¯å·®æ¥è¯„ä¼°ã€‚\n",
    "\n",
    "ä»è¯¯å·®å½¢å¼å¯ä»¥çœ‹å‡ºï¼š\n",
    "\n",
    "- å¦‚æœæŸä¸ªé€šé“ $j$ çš„è¾“å‡ºæ¿€æ´» $Y_{:, j}$ å¾ˆå¤§ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡ $s_j$ è®©å®ƒåœ¨ group-wise é‡åŒ–åæ›´æ¥è¿‘åŸå§‹å€¼ï¼›\n",
    "- åŒæ—¶ï¼Œç”±äºæˆ‘ä»¬æ˜¯ç”¨ $S^{-1}$ å»ç¼©æ”¾è¾“å…¥æ¿€æ´»ï¼Œå¯¹äºè¢«æ”¾å¤§çš„é€šé“ $j$ï¼Œè¾“å…¥æ¿€æ´»å®é™…ä¸Šè¢« **ç¼©å°**ï¼Œä»è€Œé™ä½äº†å‰é¢æ¨å¯¼ä¸­çš„ $\\mathbb{E}\\lvert x\\rvert$ï¼Œè¿›ä¸€æ­¥é™ä½é‡åŒ–è¯¯å·®ã€‚\n",
    "\n",
    "æœ€ç»ˆä½¿é‡è¦é€šé“çš„è¯¯å·®å˜å°ã€‚\n",
    "\n",
    "å› æ­¤ AWQ çš„æ•ˆæœä»‹äºï¼š\n",
    "\n",
    "- ä¼ ç»Ÿçš„ group-wise weight-only quantï¼ˆè¯¯å·®è¾ƒå¤§ä½† kernel é«˜æ•ˆï¼‰\n",
    "- å’Œ per-channel quantï¼ˆè¯¯å·®æ›´å°ä½† kernel/metadata å¼€é”€æ›´å¤§ï¼‰\n",
    "\n",
    "ä¹‹é—´ï¼Œ**ç”¨ activation-aware çš„ scale æ¥â€œæ¥è¿‘â€ per-channel çš„æ•ˆæœï¼ŒåŒæ—¶ä¿ç•™ group-wise kernel çš„æ•ˆç‡**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.4 ç®€åŒ–ç‰ˆå•å±‚ AWQ ç®—æ³•ï¼ˆä¼ªä»£ç ï¼‰\n",
    "\n",
    "å¯¹æŸä¸€å±‚ Linearï¼š$Y = X W$ï¼š\n",
    "\n",
    "1. **æ”¶é›†æ ¡å‡†æ¿€æ´»**\n",
    "   - ç”¨æ ¡å‡†æ•°æ®è·‘è‹¥å¹² batchï¼Œè®°å½•è¿™å±‚çš„è¾“å…¥ $X$ ä¸è¾“å‡º $Y = X W$ã€‚\n",
    "2. **è®¡ç®—é€šé“é‡è¦æ€§**\n",
    "   - æ¯ä¸ªè¾“å‡ºé€šé“ $j$ï¼š\n",
    "     $$\n",
    "     I_j = \\frac{1}{N}\\sum_{k=1}^N \\lvert Y^{(k)}_{:, j} \\rvert_{\\text{mean or max}}\n",
    "     $$\n",
    "3. **é€‰é‡è¦é€šé“é›†åˆ $\\mathcal{C}$**\n",
    "   - å– $I_j$ top-$p\\%$ çš„é€šé“ç´¢å¼•ã€‚\n",
    "4. **åœ¨æ¯ä¸ª group å†…æœç´¢ç¼©æ”¾å› å­**\n",
    "   - å‡è®¾ group å¤§å°ä¸º $G$ï¼ˆä¾‹å¦‚ 128 è¾“å‡ºé€šé“ä¸ºä¸€ç»„ï¼‰ã€‚\n",
    "   - å¯¹æ¯ä¸ª group $g$ï¼Œåœ¨å€™é€‰ç¼©æ”¾é›†åˆï¼ˆä¾‹å¦‚ $\\{1,2,4,8\\}$ï¼‰ä¸Šï¼Œå¯¹ group å†…çš„é€šé“ $j \\in g \\cap \\mathcal{C}$ è¿›è¡Œæœç´¢ï¼š\n",
    "     - æš‚æ—¶è®¾å®š $s_j$ï¼Œå½¢æˆ $S$ã€‚\n",
    "     - è®¡ç®— $\\hat{Y}^{(k)} = X^{(k)} S^{-1} Q(W S)$ çš„é‡å»ºè¯¯å·®ã€‚\n",
    "     - é€‰æ‹©è¯¯å·®æœ€å°çš„ç»„åˆï¼ˆé€šå¸¸æ˜¯ greedy/coordinate descentï¼Œè€Œä¸æ˜¯çœŸæ­£çš„å…¨å±€æœç´¢ï¼‰ã€‚\n",
    "5. **ä¿å­˜é‡åŒ–åçš„æƒé‡å’Œ scale**ï¼š\n",
    "   - å­˜å‚¨ $Q(W S)$ å’Œæ¯ä¸ªé€šé“çš„ $s_j$ï¼ˆæˆ–è€…æŠ˜å åˆ°ä¸‹æ¸¸å±‚é‡Œï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.4 PyTorch é£æ ¼çš„ AWQ toy å®ç°ç¤ºä¾‹\n",
    "\n",
    "> è¯´æ˜ï¼šä¸‹é¢æ˜¯ **æ•™å­¦ç”¨ç®€åŒ–ç‰ˆ**ï¼Œä¸æ˜¯å®˜æ–¹ `llm-awq` çš„å®Œæ•´å®ç°ï¼Œåªæ˜¯æŠŠä¸Šé¢çš„æ•°å­¦æ€è·¯å˜æˆå¯è·‘çš„ä»£ç ç»“æ„ï¼Œæ–¹ä¾¿ä½ åœ¨å°æ¨¡å‹ä¸Šåšå®éªŒã€‚çœŸæ­£å·¥ç¨‹ç‰ˆ AWQ æ”¯æŒ LLaMA ç­‰ LLMã€INT3/4ã€é«˜æ•ˆ CUDA kernelã€æ¨¡å‹ zoo ç­‰ï¼Œå¯ä»¥ç›´æ¥å‚è€ƒå®˜æ–¹ä»“åº“ã€‚\n",
    "\n",
    "æ€è·¯ï¼š\n",
    "\n",
    "- å†™ä¸€ä¸ª `awq_quantize_linear`ï¼Œå¯¹å•ä¸ª `nn.Linear` åšï¼š\n",
    "  - é‡‡æ ·ä¸€æ‰¹æ ¡å‡†è¾“å…¥ï¼Œå‰å‘å¾—åˆ°è¾“å‡ºæ¥ç®—é€šé“é‡è¦æ€§ï¼›\n",
    "  - ç”Ÿæˆ per-channel ç¼©æ”¾ $s_j$ï¼›\n",
    "  - æ„é€ æ–°çš„é‡åŒ– Linear æ¨¡å—ï¼ˆå­˜ int æƒé‡ + scaleï¼‰ã€‚\n",
    "- å†™ä¸€ä¸ªç®€å•çš„ `AWQLinear`ï¼Œåœ¨ `forward` é‡Œå®ç°ï¼š\n",
    "  - $x' = x \\cdot S^{-1}$  \n",
    "  - $y = x' @ W_{\\text{int}}^\\top \\cdot \\Delta$ï¼ˆgroup-wiseï¼‰  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "class AWQLinear(nn.Module):\n",
    "    def __init__(self, W_fp32, bias_fp32=None, group_size=128, nbit=4, s=None):\n",
    "        super().__init__()\n",
    "        assert W_fp32.ndim == 2\n",
    "        self.in_features, self.out_features = W_fp32.shape\n",
    "        self.group_size = group_size\n",
    "        self.nbit = nbit\n",
    "\n",
    "        if s is None:\n",
    "            s = torch.ones(self.out_features, device=W_fp32.device, dtype=W_fp32.dtype)\n",
    "        self.register_buffer(\"s\", s)\n",
    "\n",
    "        # bias ä¸é‡åŒ–ï¼Œç›´æ¥æ‹·è´ä¿å­˜\n",
    "        if bias_fp32 is not None:\n",
    "            self.register_buffer(\"bias\", bias_fp32.clone())\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # å…ˆæŒ‰é€šé“ scale æƒé‡\n",
    "        W_scaled = W_fp32 * self.s  # (d_in, d_out)\n",
    "        W_int, group_scales = self._groupwise_quantize(W_scaled, group_size, nbit)\n",
    "\n",
    "        self.register_buffer(\"W_int\", W_int)\n",
    "        self.register_buffer(\"group_scales\", group_scales)\n",
    "\n",
    "    def _groupwise_quantize(self, W, group_size, nbit):\n",
    "        d_in, d_out = W.shape\n",
    "        num_groups = (d_out + group_size - 1) // group_size\n",
    "        W_int = torch.zeros_like(W)\n",
    "        group_scales = torch.zeros(num_groups, device=W.device, dtype=W.dtype)\n",
    "        qmax = 2 ** (nbit - 1) - 1\n",
    "\n",
    "        for g in range(num_groups):\n",
    "            start, end = g * group_size, min((g + 1) * group_size, d_out)\n",
    "            W_group = W[:, start:end]\n",
    "            maxv = W_group.abs().max()\n",
    "            if maxv < 1e-8:\n",
    "                scale = torch.tensor(1.0, device=W.device, dtype=W.dtype)\n",
    "            else:\n",
    "                scale = maxv / qmax\n",
    "            group_scales[g] = scale\n",
    "\n",
    "            W_q = torch.clamp(torch.round(W_group / scale), -qmax - 1, qmax)\n",
    "            W_int[:, start:end] = W_q\n",
    "\n",
    "        return W_int.to(torch.int8), group_scales\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        æŠŠ group_scales å’Œ s è¿˜åŸæˆ per-channel çš„ fp32 æƒé‡å† matmulã€‚\n",
    "        å·¥ç¨‹é‡Œä¼š fuse åˆ° kernelï¼Œè¿™é‡Œåªæ˜¯æ¼”ç¤ºã€‚\n",
    "        \"\"\"\n",
    "        d_out = self.out_features\n",
    "        group_size = self.group_size\n",
    "        num_groups = self.group_scales.numel()\n",
    "        device = x.device\n",
    "\n",
    "        # å±•å¼€ group_scales â†’ æ¯ä¸ªè¾“å‡ºé€šé“ä¸€ä¸ª scale\n",
    "        per_channel_scale = torch.zeros(d_out, device=device, dtype=self.group_scales.dtype)\n",
    "        for g in range(num_groups):\n",
    "            start, end = g * group_size, min((g + 1) * group_size, d_out)\n",
    "            per_channel_scale[start:end] = self.group_scales[g]\n",
    "\n",
    "        # åé‡åŒ– + é™¤ä»¥ sï¼ˆç­‰ä»·å˜æ¢ï¼‰\n",
    "        W_int_fp = self.W_int.to(per_channel_scale.dtype)\n",
    "        W_dequant = W_int_fp * per_channel_scale  # (d_in, d_out)\n",
    "        W_dequant = W_dequant / self.s            # æ¯ä¸ªè¾“å‡ºé€šé“é™¤ä»¥ s_j\n",
    "\n",
    "        y = x @ W_dequant\n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias\n",
    "        return y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def awq_search_scales_for_linear(\n",
    "    linear,\n",
    "    calib_loader,\n",
    "    nsamples=128,\n",
    "    group_size=128,\n",
    "    nbit=4,\n",
    "    top_p=0.01,\n",
    "    alpha=0.5,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    linear = linear.to(device).eval()\n",
    "    # è½¬ç½®æˆ (d_in, d_out)\n",
    "    W = linear.weight.data.t().clone().to(device)\n",
    "    bias = None\n",
    "    if linear.bias is not None:\n",
    "        bias = linear.bias.data.clone().to(device)\n",
    "\n",
    "    d_out = W.shape[1]\n",
    "\n",
    "    acts_in, acts_out = [], []\n",
    "    ncollected = 0\n",
    "    for batch in calib_loader:\n",
    "        x = batch.to(device)\n",
    "        y = linear(x)\n",
    "        acts_in.append(x)\n",
    "        acts_out.append(y)\n",
    "        ncollected += x.shape[0]\n",
    "        if ncollected >= nsamples:\n",
    "            break\n",
    "\n",
    "    X = torch.cat(acts_in, dim=0)[:nsamples]\n",
    "    Y = torch.cat(acts_out, dim=0)[:nsamples]\n",
    "\n",
    "    # é€šé“é‡è¦æ€§ï¼šç®€å•ç”¨ mean |Y|\n",
    "    importance = Y.abs().mean(dim=0)\n",
    "    k = max(1, int(d_out * top_p))\n",
    "    _, idx = torch.topk(importance, k)\n",
    "    important_mask = torch.zeros(d_out, dtype=torch.bool, device=device)\n",
    "    important_mask[idx] = True\n",
    "\n",
    "    # æ„é€  per-channel s\n",
    "    # å®é™…è®ºæ–‡åœ¨ç»™å®šçš„æ ¡å‡†æ•°æ®Xä¸Šï¼ŒçœŸæ­£å»ç®—ã€Œé‡åŒ–å‰åè¾“å‡ºçš„é‡å»ºè¯¯å·®L2ã€ï¼š\n",
    "    # L(s)=||XWâˆ’Xâ€²Wqâ€²â€‹(s)|| \n",
    "    # ç„¶åå¯¹é‡è¦é€šé“åšè´ªå¿ƒæœç´¢\n",
    "    # æ­¤å¤„åªæ˜¯ç”¨è¾“å‡ºå¤§å° proxy äº†â€œé‡è¦æ€§â€ï¼Œå´æ²¡æœ‰åšçœŸæ­£çš„è¯¯å·®æœç´¢\n",
    "    s = torch.ones(d_out, device=device, dtype=W.dtype)\n",
    "    ratio = (importance / (importance.mean() + 1e-6)).clamp(min=1.0)\n",
    "    s[important_mask] = (ratio[important_mask] ** alpha).clamp(max=8.0)\n",
    "\n",
    "    awq_linear = AWQLinear(\n",
    "        W_fp32=W,\n",
    "        bias_fp32=bias,\n",
    "        group_size=group_size,\n",
    "        nbit=nbit,\n",
    "        s=s,\n",
    "    )\n",
    "    return awq_linear\n",
    "\n",
    "\n",
    "# å¯¹å° MLP åš AWQï¼Œå¹¶å¯¹æ¯”è¯¯å·®\n",
    "class SmallMLP(nn.Module):\n",
    "    def __init__(self, d_in=64, d_hidden=128, d_out=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "\n",
    "def build_calib_loaders(model, n_samples=512, batch_size=32, device=\"cuda\"):\n",
    "    model = model.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        # åŸå§‹è¾“å…¥æ ¡å‡†æ•°æ® (N, 64)\n",
    "        x_calib = torch.randn(n_samples, 64, device=device)\n",
    "        # é€šè¿‡ fc1+GELU å¾—åˆ°ä¸­é—´æ¿€æ´» (N, 128)\n",
    "        h_calib = model.act(model.fc1(x_calib))\n",
    "\n",
    "    def make_loader(data):\n",
    "        return [data[i:i + batch_size] for i in range(0, n_samples, batch_size)]\n",
    "\n",
    "    calib_loader_fc1 = make_loader(x_calib)  # ç»™ fc1 ç”¨\n",
    "    calib_loader_fc2 = make_loader(h_calib)  # ç»™ fc2 ç”¨\n",
    "    return calib_loader_fc1, calib_loader_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30c8ca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== W4 Quantization Comparison ====\n",
      "Plain W4 L2 error : 0.021071\n",
      "Plain W4 Max error: 0.078114\n",
      "---\n",
      "AWQ  W4 L2 error  : 0.021870\n",
      "AWQ  W4 Max error : 0.064620\n"
     ]
    }
   ],
   "source": [
    "def plain_w4(linear, group_size=32, device=\"cuda\"):\n",
    "    W = linear.weight.data.t().clone().to(device)\n",
    "    bias = linear.bias.data.clone().to(device) if linear.bias is not None else None\n",
    "\n",
    "    d_in, d_out = W.shape\n",
    "    num_groups = (d_out + group_size - 1) // group_size\n",
    "    W_int = torch.zeros_like(W)\n",
    "    group_scales = torch.zeros(num_groups, device=W.device)\n",
    "\n",
    "    qmax = 2 ** (4 - 1) - 1\n",
    "\n",
    "    for g in range(num_groups):\n",
    "        start = g * group_size\n",
    "        end = min((g+1) * group_size, d_out)\n",
    "        W_group = W[:, start:end]\n",
    "        maxv = W_group.abs().max()\n",
    "        scale = maxv / qmax if maxv > 1e-8 else 1.0\n",
    "        group_scales[g] = scale\n",
    "        W_q = torch.clamp(torch.round(W_group / scale), -qmax - 1, qmax)\n",
    "        W_int[:, start:end] = W_q\n",
    "\n",
    "    # å±•å¼€ scale\n",
    "    per_channel_scale = torch.zeros(d_out, device=W.device)\n",
    "    for g in range(num_groups):\n",
    "        start = g * group_size\n",
    "        end = min((g+1) * group_size, d_out)\n",
    "        per_channel_scale[start:end] = group_scales[g]\n",
    "\n",
    "    W_dequant = W_int * per_channel_scale\n",
    "\n",
    "    # æ„é€ ä¸€ä¸ªæ™®é€š Linear\n",
    "    qlinear = nn.Linear(W.shape[0], W.shape[1], bias=bias is not None).to(device)\n",
    "    qlinear.weight.data = W_dequant.t().contiguous()\n",
    "    if bias is not None:\n",
    "        qlinear.bias.data = bias\n",
    "    return qlinear\n",
    "\n",
    "\n",
    "def main():\n",
    "    import random\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    model = SmallMLP().to(device).eval()\n",
    "\n",
    "    calib_loader_fc1, calib_loader_fc2 = build_calib_loaders(\n",
    "        model, n_samples=512, batch_size=32, device=device\n",
    "    )\n",
    "\n",
    "    x_test = torch.randn(16, 64, device=device)\n",
    "    with torch.no_grad():\n",
    "        y_ref = model(x_test)\n",
    "\n",
    "    # -------------------------\n",
    "    #  Plain W4ï¼ˆæ—  AWQï¼‰ baseline\n",
    "    # -------------------------\n",
    "    plain_fc1 = plain_w4(model.fc1, group_size=128, device=device)\n",
    "    plain_fc2 = plain_w4(model.fc2, group_size=128, device=device)\n",
    "\n",
    "    class PlainMLP(nn.Module):\n",
    "        def __init__(self, q1, q2):\n",
    "            super().__init__()\n",
    "            self.fc1 = q1\n",
    "            self.act = nn.GELU()\n",
    "            self.fc2 = q2\n",
    "        def forward(self, x): return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "    plain_model = PlainMLP(plain_fc1, plain_fc2).to(device)\n",
    "    with torch.no_grad():\n",
    "        y_plain = plain_model(x_test)\n",
    "\n",
    "    plain_l2 = (y_plain - y_ref).pow(2).mean().sqrt().item()\n",
    "    plain_max = (y_plain - y_ref).abs().max().item()\n",
    "\n",
    "    # -------------------------\n",
    "    #        AWQ  W4\n",
    "    # -------------------------\n",
    "    awq_fc1 = awq_search_scales_for_linear(\n",
    "        model.fc1, calib_loader_fc1, nsamples=256,\n",
    "        group_size=128, nbit=4, top_p=0.02, alpha=0.5, device=device\n",
    "    )\n",
    "    awq_fc2 = awq_search_scales_for_linear(\n",
    "        model.fc2, calib_loader_fc2, nsamples=256,\n",
    "        group_size=128, nbit=4, top_p=0.02, alpha=0.5, device=device\n",
    "    )\n",
    "\n",
    "    class QuantMLP(nn.Module):\n",
    "        def __init__(self, q1, q2):\n",
    "            super().__init__()\n",
    "            self.fc1 = q1\n",
    "            self.act = nn.GELU()\n",
    "            self.fc2 = q2\n",
    "        def forward(self, x): return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "    qmodel = QuantMLP(awq_fc1, awq_fc2).to(device)\n",
    "    with torch.no_grad():\n",
    "        y_q = qmodel(x_test)\n",
    "\n",
    "    awq_l2 = (y_q - y_ref).pow(2).mean().sqrt().item()\n",
    "    awq_max = (y_q - y_ref).abs().max().item()\n",
    "\n",
    "    # -------------------------\n",
    "    # Print results\n",
    "    # -------------------------\n",
    "    print(\"==== W4 Quantization Comparison ====\")\n",
    "    print(f\"Plain W4 L2 error : {plain_l2:.6f}\")\n",
    "    print(f\"Plain W4 Max error: {plain_max:.6f}\")\n",
    "    print(\"---\")\n",
    "    print(f\"AWQ  W4 L2 error  : {awq_l2:.6f}\")\n",
    "    print(f\"AWQ  W4 Max error : {awq_max:.6f}\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39026abc",
   "metadata": {},
   "source": [
    "ä¸æ˜¯ä¸€ä¸ªå¥½ä¾‹å­ã€‚\n",
    "- SmallMLP weights éšæœºç”Ÿæˆï¼Œ \n",
    "- æ²¡æœ‰ learned structureï¼Œ\n",
    "- é€šé“ importanceï¼ˆmean |Y|ï¼‰åˆ†å¸ƒå‡ ä¹æ˜¯å™ªå£°ï¼Œæ ¹æœ¬ä¸å­˜åœ¨ä»€ä¹ˆâ€œæç«¯é‡è¦çš„é€šé“â€ã€‚ \n",
    "- å®ç°ä¹Ÿä¸æ˜¯å®Œå…¨å®˜æ–¹ç®—æ³•\n",
    "- åŒæ—¶activate-awareåº”è¯¥æ˜¯é’ˆå¯¹äºLLMã€‚ä½†æ˜¯è‡³å°‘AWQ â‰ˆ Plain W4 ç¨ç¨å¥½ä¸€ç‚¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c49e0e",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.5.5 æ–‡çŒ®\n",
    "- Lin et al., *AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration*, arXiv:2306.00978  \n",
    "- å®˜æ–¹å®ç°ï¼šhttps://github.com/mit-han-lab/llm-awq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bfa77d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b35f2fc",
   "metadata": {},
   "source": [
    "## 1.6 INT4 / INT3 / Hybrid Quantization\n",
    "\n",
    "- ä¸ºä»€ä¹ˆå¤§æ¨¡å‹å¸¸ç”¨ **æƒé‡ä½æ¯”ç‰¹ï¼ˆINT4/INT3ï¼‰+ æ¿€æ´»é«˜æ¯”ç‰¹ï¼ˆFP16/BF16ï¼‰**  \n",
    "- QLoRA ä¸­çš„ NF4ã€Block Quantizationã€Double Quantization æ•°å­¦å½¢å¼  \n",
    "- ç®€å•çš„ PyTorch NF4 æŒ‰å—é‡åŒ–å®ç°ç¤ºä¾‹  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.6.1 Hybrid Quantization æ¦‚å¿µ\n",
    "\n",
    "Hybrid Quantization = ä½æ¯”ç‰¹æƒé‡ + é«˜ç²¾åº¦æ¿€æ´»ï¼š\n",
    "\n",
    "- æƒé‡ï¼šINT4 / INT3ï¼ˆweight-only quantizationï¼‰\n",
    "- æ¿€æ´»ï¼šFP16 / BF16\n",
    "- æ•æ„Ÿå±‚ï¼šä½¿ç”¨ FP16 æˆ– INT8\n",
    "\n",
    "åŸå› ï¼š\n",
    "\n",
    "1. æƒé‡æ˜¯é™æ€å¼ é‡ï¼Œåªéœ€é‡åŒ–ä¸€æ¬¡ï¼Œè¯¯å·®ä¸ä¼šéšæ—¶é—´ç´¯ç§¯ã€‚  \n",
    "2. æ¿€æ´»åŠ¨æ€èŒƒå›´å¤§ï¼Œé‡åŒ–ä¼šå¯¼è‡´æ•°å€¼ä¸ç¨³å®šï¼ˆå¦‚ LayerNormã€Softmaxã€GELUï¼‰ã€‚  \n",
    "3. åœ¨è½¦ç«¯ GPU/NPU ä¸Šï¼Œå‡å°‘æƒé‡å¸¦å®½æ˜¯æå‡åå/é™ä½åŠŸè€—çš„å…³é”®ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6.2. QLoRA çš„ä¸‰å¤§è´¡çŒ®ï¼ˆæ•°å­¦è§†è§’ï¼‰\n",
    "\n",
    "####  NF4ï¼šNormalFloat 4-bit codebook\n",
    "\n",
    "QLoRA ä½¿ç”¨ **NormalFloat 4-bitï¼ˆNF4ï¼‰**ï¼Œç”¨ä¸€ç»„éå‡åŒ€é‡åŒ–ç‚¹è¿‘ä¼¼é«˜æ–¯åˆ†å¸ƒï¼ˆè€Œéç­‰é—´è·æ•´æ•°ï¼‰ã€‚\n",
    "\n",
    "å®šä¹‰ä¸€ä¸ª 4-bit codebookï¼š\n",
    "\n",
    "$$\n",
    "\\mathcal{Q}_{\\text{NF4}} = \\{q_0, q_1, \\dots, q_{15}\\}\n",
    "$$\n",
    "\n",
    "å®é™…å¯ä»¥è¿‘ä¼¼ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "[-1.0,\\,-0.66,\\,-0.45,\\,-0.32,\\,-0.21,\\,-0.14,\\,-0.08,\\,-0.03,\\,\n",
    "\\;0.03,\\;0.08,\\;0.14,\\;0.21,\\;0.32,\\;0.45,\\;0.66,\\;1.0]\n",
    "$$\n",
    "\n",
    "å¯¹ä¸€ä¸ªæƒé‡æ ‡é‡ $w$ï¼Œå…ˆæŒ‰ç¼©æ”¾å› å­ $s$ å½’ä¸€åŒ–å†é‡åŒ–ï¼š\n",
    "\n",
    "$$\n",
    "w_{\\text{norm}} = \\frac{w}{s}, \\quad\n",
    "\\hat{q} = \\arg\\min_{q \\in \\mathcal{Q}_{\\text{NF4}}} \\lvert w_{\\text{norm}} - q \\rvert,\n",
    "$$\n",
    "\n",
    "åé‡åŒ–ï¼š\n",
    "\n",
    "$$\n",
    "\\hat{w} = s \\cdot \\hat{q}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Block Quantizationï¼ˆæŒ‰å—é‡åŒ–ï¼‰\n",
    "\n",
    "è®¾æƒé‡çŸ©é˜µï¼š\n",
    "\n",
    "$$\n",
    "W \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}}\n",
    "$$\n",
    "\n",
    "æŒ‰è¡Œåˆ‡åˆ†ä¸ºå¤šä¸ª blockï¼š\n",
    "\n",
    "$$\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "W^{(1)} \\\\\n",
    "W^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "W^{(B)}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "W^{(k)} \\in \\mathbb{R}^{b \\times d_{\\text{in}}},\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $b$ æ˜¯ block sizeï¼ˆå¦‚ 32 æˆ– 64ï¼‰ã€‚\n",
    "\n",
    "å¯¹ç¬¬ $k$ ä¸ª blockï¼š\n",
    "\n",
    "1. è®¡ç®—ç¼©æ”¾å› å­ï¼ˆä¾‹å¦‚å–æœ€å¤§ç»å¯¹å€¼ï¼‰ï¼š\n",
    "\n",
    "$$\n",
    "s_k = \\max_{i,j} \\lvert W^{(k)}_{ij} \\rvert + \\varepsilon\n",
    "$$\n",
    "\n",
    "2. å½’ä¸€åŒ–å¹¶æ˜ å°„åˆ° NF4 codebookï¼š\n",
    "\n",
    "$$\n",
    "\\hat{W}^{(k)} = s_k \\cdot Q_{\\text{NF4}}\\!\\left(\\frac{W^{(k)}}{s_k}\\right),\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $Q_{\\text{NF4}}$ è¡¨ç¤ºé€å…ƒç´ é€‰æ‹©æœ€è¿‘çš„ NF4 é‡åŒ–ç‚¹ã€‚\n",
    "\n",
    "æ•´ä½“å¯ä»¥å†™æˆï¼š\n",
    "\n",
    "$$\n",
    "\\hat{W} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{W}^{(1)} \\\\\n",
    "\\hat{W}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{W}^{(B)}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Double Quantizationï¼ˆä¸¤æ¬¡é‡åŒ–ï¼‰\n",
    "\n",
    "ç¬¬ä¸€æ¬¡é‡åŒ–ï¼ˆæƒé‡ï¼‰ï¼š\n",
    "\n",
    "$$\n",
    "W \\rightarrow (q_i, s_k)\n",
    "$$\n",
    "\n",
    "å«ä¹‰ï¼š\n",
    "\n",
    "- åŸå§‹æƒé‡ $W$ ç”¨ 4-bit NF4 ç´¢å¼• $q_i$ è¡¨ç¤º  \n",
    "- æ¯ä¸ª block æœ‰ä¸€ä¸ª FP16/FP32 çš„ç¼©æ”¾å› å­ $s_k$\n",
    "\n",
    "å¯¹äºå¤§æ¨¡å‹ï¼Œæ‰€æœ‰ $s_k$ çš„å­˜å‚¨å¼€é”€ä»ç„¶æ˜¾è‘—ã€‚  \n",
    "Double Quantization çš„æƒ³æ³•æ˜¯ï¼š**è¿ç¼©æ”¾å› å­æœ¬èº«ä¹Ÿé‡åŒ–**ã€‚\n",
    "\n",
    "ç¬¬äºŒæ¬¡é‡åŒ–ï¼ˆå¯¹ $s_k$ å†é‡åŒ–æˆä½æ¯”ç‰¹ï¼Œå¦‚ INT8ï¼‰ï¼š\n",
    "\n",
    "$$\n",
    "s_k \\rightarrow s_{k,q}.\n",
    "$$\n",
    "\n",
    "ä¸¾ä¸ªç®€å•å½¢å¼ï¼š\n",
    "\n",
    "- é€‰ä¸€ä¸ªå…¨å±€ scale $\\alpha$ï¼ŒæŠŠæ‰€æœ‰ $s_k$ å½’ä¸€åŒ–  \n",
    "- å­˜å‚¨æ•´æ•° $z_k$ï¼š\n",
    "\n",
    "$$\n",
    "z_k = \\text{round}\\!\\left(\\frac{s_k}{\\alpha}\\right), \\quad s_k \\approx \\alpha \\cdot z_k.\n",
    "$$\n",
    "\n",
    "äºæ˜¯æœ€ç»ˆå­˜å‚¨å†…å®¹ä¸ºï¼š\n",
    "\n",
    "- æƒé‡ç´¢å¼•ï¼š$q_i$ï¼ˆ4-bitï¼‰  \n",
    "- ç¼©æ”¾å› å­ç´¢å¼•ï¼š$z_k$ï¼ˆ8-bitï¼‰  \n",
    "\n",
    "è¿™æ ·å¯ä»¥æŠŠæ•´ä½“æ¨¡å‹çš„æœ‰æ•ˆæ¯”ç‰¹æ•°æ›´æ¥è¿‘ â€œçœŸæ­£çš„ 4-bitâ€ï¼Œè€Œä¸æ˜¯ â€œ4-bit æƒé‡ + å¤§é‡ 16-bit scaleâ€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6.3. ä¸ºä»€ä¹ˆ INT4 / INT3 ä»ç„¶å¯ä»¥ä¿æŒé«˜ç²¾åº¦ï¼Ÿ\n",
    "\n",
    "è€ƒè™‘ä¸€å±‚çº¿æ€§å±‚ï¼ˆå¿½ç•¥ biasï¼‰ï¼š\n",
    "\n",
    "$$\n",
    "Y = X W,\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $X \\in \\mathbb{R}^{n \\times d_{\\text{in}}}$ï¼Œ$W \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$ã€‚\n",
    "\n",
    "é‡åŒ–åçš„æƒé‡ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\hat{W} = W + \\Delta W.\n",
    "$$\n",
    "\n",
    "å¯¹åº”è¾“å‡ºä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\hat{Y} = X \\hat{W} = X (W + \\Delta W) = XW + X \\Delta W.\n",
    "$$\n",
    "\n",
    "è¯¯å·®ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\Delta Y = \\hat{Y} - Y = X \\Delta W.\n",
    "$$\n",
    "\n",
    "åªè¦æ»¡è¶³ï¼š\n",
    "\n",
    "- $\\lVert \\Delta W \\rVert$ è¶³å¤Ÿå°ï¼ˆNF4 çš„é‡åŒ–è¯¯å·®ç›¸å¯¹ä¼ ç»Ÿ INT4 æ›´å°ï¼‰ï¼›  \n",
    "- æ¿€æ´» $X$ ä½¿ç”¨ FP16 / BF16ï¼Œä¸å†å¼•å…¥é¢å¤–çš„ä½æ¯”ç‰¹è¯¯å·®ï¼›  \n",
    "\n",
    "åˆ™ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\lVert \\Delta Y \\rVert}{\\lVert Y \\rVert}\n",
    "$$\n",
    "\n",
    "å¯ä»¥è¢«æ§åˆ¶åœ¨è¾ƒå°èŒƒå›´ï¼Œå³æ¨¡å‹ç²¾åº¦åŸºæœ¬ä¸æ‰ã€‚\n",
    "\n",
    "QLoRA çš„å®éªŒè¡¨æ˜ï¼Œåœ¨ LLaMA-65B è¿™ç±»å¤§æ¨¡å‹ä¸Šï¼Œ**INT4 æƒé‡ + FP16 æ¿€æ´»** çš„ç²¾åº¦å‡ ä¹å¯ä»¥åŒ¹é…åŸå§‹ FP16ï¼ˆæŸå¤± < 1%ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6.4. å·¥ç¨‹è½åœ°ï¼ˆè½¦ç«¯ / è¾¹ç¼˜è®¾å¤‡ï¼‰\n",
    "\n",
    "åœ¨å…¸å‹ Transformer ç»“æ„ä¸­ï¼Œå¯é‡‡å–ä»¥ä¸‹æ··åˆç²¾åº¦é…ç½®ï¼š\n",
    "\n",
    "| å±‚ç±»å‹ | æ¨èç²¾åº¦ | è¯´æ˜ |\n",
    "|-------|----------|------|\n",
    "| Q/K/V projection | INT4 | æƒé‡æ¥è¿‘é«˜æ–¯åˆ†å¸ƒï¼Œå†—ä½™åº¦é«˜ |\n",
    "| FFN up / down projection | INT4 æˆ– INT3 | å‚æ•°é‡å¤§ï¼Œå‹ç¼©æ”¶ç›Šé«˜ |\n",
    "| Output projection | INT4 | å®¹é”™æ€§è¾ƒé«˜ |\n",
    "| Embedding | INT8 / FP16 | lookup + ä¸è§„åˆ™è®¿é—®ï¼Œä¸é€‚åˆ 4bit |\n",
    "| LayerNorm | FP16 | å¯¹æ•°å€¼ç¨³å®šé«˜åº¦æ•æ„Ÿ |\n",
    "| Softmax / attention score | FP16 | æ¶‰åŠ $\\\\exp$ å’Œå½’ä¸€åŒ–ï¼Œéœ€é«˜ç²¾åº¦ |\n",
    "\n",
    "éƒ¨ç½²ç­–ç•¥ç¤ºä¾‹ï¼š\n",
    "\n",
    "1. **å…ˆåšå…¨æ¨¡å‹ weight-only INT4ï¼ˆNF4ï¼‰é‡åŒ–**ã€‚  \n",
    "2. é€šè¿‡æ ¡å‡†æ•°æ®ï¼Œæµ‹ per-layer è¯¯å·®ï¼ˆå¦‚è¾“å‡º $L_2$ ç›¸å¯¹è¯¯å·®ï¼‰ï¼Œç”» heatmapã€‚  \n",
    "3. å¯¹è¯¯å·®æœ€å¤§çš„è‹¥å¹²å±‚å›é€€ä¸º INT8 æˆ– FP16ï¼ˆHybrid Quantizationï¼‰ã€‚  \n",
    "4. å°è¯•ä¸åŒ block sizeï¼ˆ32 / 64 / 128ï¼‰ï¼Œåœ¨ç²¾åº¦ä¸æ€§èƒ½é—´å¯»æ‰¾å¹³è¡¡ç‚¹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6.5 æ–‡çŒ®\n",
    "\n",
    "- Dettmers et al., *QLoRA: Efficient Finetuning of Quantized LLMs*, arXiv:2305.14314\n",
    "- è®ºæ–‡å®ç° https://github.com/artidoro/qlora\n",
    "\n",
    "repo é‡ŒåŒ…å«ï¼š\n",
    "\n",
    "- æ”¯æŒ 4-bit NormalFloat (NF4) quantization\n",
    "\n",
    "- â€œDouble Quantizationâ€ï¼ˆä¹Ÿå°±æ˜¯ä¸ä»… quantize æƒé‡ï¼Œè¿˜ quantize quantization constantsï¼scalesï¼‰\n",
    "\n",
    "- ä¸ä¸»æµåº“é›†æˆï¼Œä¾‹å¦‚ï¼šç”¨ bitsandbytes + transformers + PEFT è”åˆå¾®è°ƒ / æ¨ç†æµç¨‹ \n",
    "\n",
    "- è„šæœ¬ã€ç¤ºä¾‹ã€ä»¥åŠè®­ç»ƒï¼æ¨ç† pipeline\n",
    "\n",
    "âš ï¸ æ³¨æ„äº‹é¡¹\n",
    "- è¿™ä¸ªå®˜æ–¹ repo åå‘ â€œfinetuning + LoRA + quantized checkpointâ€ åœºæ™¯ â€”â€” å¦‚æœä½ åªæ˜¯æƒ³åšè‡ªå®šä¹‰é‡åŒ– / æ¨ç†ä¼˜åŒ– / æˆ–è€…åº•å±‚éƒ¨ç½²ï¼ˆæ¯”å¦‚ INT4 + activation FP16 + custom kernelï¼‰ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹æˆ–æ‰©å±• repoã€‚\n",
    "\n",
    "- é‡åŒ–ç»†èŠ‚ï¼ˆblock-size, double quant, scale çš„ç®¡ç†ç­‰ï¼‰æ˜¯ç”± bitsandbytes + QLoRA å®ç°ï¼Œä½†ä½ å¦‚æœæƒ³ç”¨å…¶ä»– quantization æ ¼å¼ï¼ˆæ¯”å¦‚ä½ ä¹‹å‰æåˆ°çš„ INT3 / hybrid quantization / æ··åˆå±‚ç²¾åº¦ï¼‰ â€”â€” å¯èƒ½è¦åŸºäºè¿™ä¸ª repo åšä¸€äº›å·¥ç¨‹çº§æ”¹é€ ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c44d09",
   "metadata": {},
   "source": [
    "# 1.7 æ€»ç»“\n",
    "| æ–¹æ³• | PTQï¼ˆè®­ç»ƒåé‡åŒ–ï¼‰ | Training-Aware / QAT | Deployment-Timeï¼ˆéƒ¨ç½²å¯ç‹¬ç«‹åšï¼‰ | è¯´æ˜ |\n",
    "|------|-------------------|------------------------|----------------------------------|------|\n",
    "| MinMax / Percentile / Basic INT8 | âœ… æ˜¯ | âŒ å¦ | âœ… æ˜¯ | æœ€åŸºç¡€çš„æ¿€æ´»/æƒé‡é‡åŒ–ï¼Œåªéœ€æ ¡å‡†æ•°æ® |\n",
    "| KL Divergence / Histogram Calibration | âœ… æ˜¯ | âŒ å¦ | âœ… æ˜¯ | é€‰æ¿€æ´» clipping thresholdï¼Œéƒ¨ç½²å¸¸ç”¨ |\n",
    "| GPTQ (Hessian-based PTQ) | âœ… æ˜¯ | âŒ å¦ | âœ… æ˜¯ | æœ€å¼º PTQ æƒé‡é‡åŒ–ï¼Œåªéœ€æ ¡å‡†è¾“å…¥ |\n",
    "| AWQ (Activation-aware Weight Quant) | âœ… æ˜¯ | âŒ å¦ | âœ… æ˜¯ | åŸºäº activation çš„ PTQï¼Œä¸éœ€è®­ç»ƒ |\n",
    "| SmoothQuant | âš ï¸ PTQ-ish | âŒ å¦ | âœ… æ˜¯ | è®­ç»ƒåä¸€æ¬¡æ€§ scalingï¼Œéƒ¨ç½²å¯åš |\n",
    "| INT4/INT3/NF4 weight-only | âœ… æ˜¯ | âŒ å¦ | âœ… æ˜¯ | çº¯æƒé‡ä½æ¯”ç‰¹é‡åŒ–ï¼Œéƒ¨ç½²å¯å¤„ç† |\n",
    "| NF4 + LoRA fine-tuning | âš ï¸ éƒ¨åˆ† | âœ… æ˜¯ | âŒ å¦ | é‡åŒ–æœ¬èº«æ˜¯ PTQï¼Œä½† LoRA éœ€è¦è®­ç»ƒ |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
