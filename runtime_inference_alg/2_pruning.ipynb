{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edbe00ba",
   "metadata": {},
   "source": [
    "# 2 Pruning å‰ªæ\n",
    "## 2.1 Magnitude-Based Unstructured Pruning\n",
    "\n",
    "### 2.1.1 æ•°å­¦åŸç†\n",
    "\n",
    "Magnitude pruning å‡è®¾ï¼šæƒé‡ç»å¯¹å€¼è¶Šå°ï¼Œå¯¹è¾“å‡ºå½±å“è¶Šå¼±ï¼Œå› æ­¤å¯ä»¥ä¼˜å…ˆåˆ é™¤ã€‚è¿™æ˜¯ä¸€ç§å…¸å‹çš„ **unstructured å‰ªæ** æ–¹æ³•ã€‚\n",
    "\n",
    "è®¾åŸå§‹æƒé‡ä¸º $w \\in \\mathbb{R}^d$ï¼Œå‰ªæ mask ä¸º $m \\in \\{0,1\\}^d$ï¼Œå‰ªæåæƒé‡ä¸º $w \\odot m$ã€‚ç†æƒ³ç›®æ ‡æ˜¯ï¼š\n",
    "\n",
    "$$\n",
    "\\min_m L(w \\odot m) \\quad \\text{s.t.}\\quad \\|m\\|_0 = k,\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $\\|m\\|_0$ æ˜¯ä¿ç•™ä¸‹æ¥çš„æƒé‡ä¸ªæ•°ï¼ˆç¨€ç–åº¦çº¦æŸï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œç›´æ¥æ±‚è§£æ˜¯ NP-hardã€‚\n",
    "\n",
    "Magnitude pruning é‡‡ç”¨ä¸€ä¸ªéå¸¸ç®€å•çš„å¯å‘å¼ï¼š\n",
    "\n",
    "> æŒ‰æƒé‡çš„ç»å¯¹å€¼ $|w_i|$ ä»å°åˆ°å¤§æ’åºï¼Œåˆ é™¤æœ€å°çš„ $s\\%$ æƒé‡ã€‚\n",
    "\n",
    "ä¹Ÿå¯ä»¥å†™æˆï¼šåœ¨æ‰€æœ‰å¤§å°ä¸º $sd$ çš„é›†åˆ $S$ ä¸­ï¼Œé€‰æ‹©\n",
    "\n",
    "$$\n",
    "S^* = \\arg\\min_{S:\\,|S| = sd} \\sum_{i \\in S} |w_i|,\n",
    "$$\n",
    "\n",
    "å¹¶å°† $S^*$ ä¸­çš„æƒé‡ç½®é›¶ã€‚\n",
    "\n",
    "### 2.1.2 ç†è®ºä¾æ®ï¼ˆOptimal Brain Damage è§†è§’ï¼‰\n",
    "\n",
    "Optimal Brain Damageï¼ˆLeCun, 1990ï¼‰ä»äºŒé˜¶æ³°å‹’å±•å¼€ç»™å‡ºäº†æƒé‡é‡è¦æ€§çš„ç»å…¸å½¢å¼ã€‚å¯¹æŸå¤± $L(w)$ï¼Œå¯¹æŸä¸ªæƒé‡ $w_i$ è¿›è¡Œå¾®å°æ‰°åŠ¨ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta L_i \\approx g_i \\Delta w_i + \\frac{1}{2} h_{ii} (\\Delta w_i)^2,\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $g_i = \\partial L/\\partial w_i$ï¼Œ$h_{ii}$ æ˜¯ Hessian å¯¹è§’å…ƒç´ ã€‚è‹¥åœ¨æ”¶æ•›ç‚¹é™„è¿‘ï¼Œ$g_i \\approx 0$ï¼Œå¹¶ä»¤å‰ªææ—¶ $\\Delta w_i = -w_i$ï¼Œåˆ™æœ‰ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta L_i \\approx \\frac{1}{2} h_{ii} w_i^2.\n",
    "$$\n",
    "\n",
    "å¦‚æœåœ¨åŒä¸€å±‚å†… $h_{ii}$ å˜åŒ–ä¸å¤§ï¼ˆæˆ–ä½¿ç”¨å¯¹è§’è¿‘ä¼¼ï¼‰ï¼Œåˆ™æœ‰\n",
    "\n",
    "$$\n",
    "\\Delta L_i \\propto w_i^2.\n",
    "$$\n",
    "\n",
    "å› æ­¤ï¼Œ**æƒé‡ç»å¯¹å€¼è¶Šå°ï¼Œå¯¹æŸå¤±çš„å½±å“è¶Šå°ï¼Œè¶Šé€‚åˆè¢«å‰ªæ‰**ï¼Œè¿™ä¸º magnitude pruning æä¾›äº†ä¸€ä¸ªç®€å•ä½†åˆç†çš„ç†è®ºåŸºç¡€ã€‚\n",
    "\n",
    "### 2.1.3 Global vs Layerwise\n",
    "\n",
    "- **Global pruning**ï¼šåœ¨å…¨æ¨¡å‹èŒƒå›´å†…å¯¹æ‰€æœ‰æƒé‡çš„ $|w_i|$ åšä¸€æ¬¡æ’åºï¼Œåˆ é™¤æ€»æ•°ä¸­æœ€å°çš„ $s\\%$ã€‚  \n",
    "  - æ›´æ¥è¿‘ä¸Šé¢çš„ä¼˜åŒ–å½¢å¼ï¼Œä¸€èˆ¬æ€§èƒ½æ›´å¥½ã€‚\n",
    "\n",
    "- **Layerwise pruning**ï¼šæ¯ä¸€å±‚å•ç‹¬æŒ‰ç…§ $|w_i|$ æ’åºï¼Œå„è‡ªå‰ªæ‰åŒæ ·æ¯”ä¾‹ã€‚  \n",
    "  - å®ç°ç®€å•ï¼Œä½†å¯¹æœ‰äº›å±‚å¯èƒ½è¿‡åº¦å‰ªæï¼Œå¯¹å¦ä¸€äº›å±‚å‰ªå¾—å¤ªå°‘ï¼Œæ•´ä½“ suboptimalã€‚\n",
    "\n",
    "åœ¨å·¥ç¨‹å®è·µä¸­ï¼Œglobal magnitude pruning é€šå¸¸ä½œä¸ºé»˜è®¤é€‰æ‹©ã€‚\n",
    "\n",
    "### 2.1.4 å®é™…å·¥ç¨‹æµç¨‹\n",
    "\n",
    "1. è®­ç»ƒä¸€ä¸ªå…¨ç²¾åº¦åŸºçº¿æ¨¡å‹ï¼Œå¾—åˆ°æƒé‡ $w^*$ï¼›  \n",
    "2. æ”¶é›†æ‰€æœ‰æƒé‡çš„ç»å¯¹å€¼ $|w_i^*|$ï¼›  \n",
    "3. åœ¨å…¨æ¨¡å‹èŒƒå›´æŒ‰ $|w_i^*|$ æ’åºï¼›  \n",
    "4. é€‰æ‹© sparsity æ¯”ä¾‹ $s$ï¼Œåˆ é™¤æœ€å°çš„ $s\\%$ æƒé‡ï¼ˆå¯¹åº”ä½ç½®çš„ $m_i = 0$ï¼‰ï¼›  \n",
    "5. å†»ç»“å‰ªæç»“æ„ï¼Œå¯¹å‰©ä½™æƒé‡è¿›è¡Œè‹¥å¹² epoch çš„ fine-tuneï¼Œæ¢å¤ç²¾åº¦ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯æœ€ç»å…¸ã€æœ€å¸¸ç”¨çš„ **unstructured magnitude pruning pipeline**ã€‚\n",
    "\n",
    "### 2.1.5 æ–‡çŒ®\n",
    "\n",
    "- **LeCun et al., *Optimal Brain Damage*, 1990**  \n",
    "  <http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf>  \n",
    "  - æœ€æ—©ç³»ç»Ÿè®¨è®ºåŸºäºæƒé‡é‡è¦æ€§çš„è¿æ¥å‰ªæï¼Œç»™å‡ºäºŒé˜¶æ³°å‹’å±•å¼€ä¸ Hessian å¯¹è§’è¿‘ä¼¼ã€‚\n",
    "\n",
    "- **Molchanov et al., *Pruning Convolutional Neural Networks for Resource Efficient Inference*, ICLR 2017**  \n",
    "  <https://arxiv.org/abs/1611.06440>  \n",
    "  - ç°ä»£å‰ªæå·¥ä½œä¸­å¸¸ç”¨çš„åŸºçº¿ä¹‹ä¸€ï¼Œæ˜ç¡®å°† â€œæŒ‰æƒé‡ç»å¯¹å€¼æ’åºåå‰ªæ‰æœ€å°éƒ¨åˆ†â€ ä½œä¸º magnitude-based pruning baselineã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc4a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params before: 203530\n",
      "k=101632, total weights=203264\n",
      "Non-zero params after prune: 101898\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, in_dim=784, hidden=256, out_dim=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "def global_magnitude_prune(model: nn.Module, sparsity: float):\n",
    "    \"\"\"\n",
    "    å¯¹æ¨¡å‹åšä¸€æ¬¡ global magnitude å‰ªæï¼ˆåªå¯¹ weight å‚æ•°ï¼‰.\n",
    "\n",
    "    Args:\n",
    "        model: nn.Module\n",
    "        sparsity: è¦å‰ªæ‰çš„æ¯”ä¾‹ (0 ~ 1)ï¼Œä¾‹å¦‚ 0.5 è¡¨ç¤ºå‰ªæ‰ 50% æœ€å°æƒé‡\n",
    "    \"\"\"\n",
    "    # æ”¶é›†æ‰€æœ‰ weight\n",
    "    weight_tensors = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            weight_tensors.append(p.data.view(-1))\n",
    "\n",
    "    all_weights = torch.cat(weight_tensors)\n",
    "    k = int(sparsity * all_weights.numel())\n",
    "    print(f\"{k=}, total weights={all_weights.numel()}\")\n",
    "\n",
    "    # kth-value æ˜¯ä»å°åˆ°å¤§æ’åºåç¬¬ k ä¸ªå…ƒç´ \n",
    "    threshold = all_weights.abs().kthvalue(k).values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, p in model.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                mask = (p.data.abs() > threshold).float()\n",
    "                p.data.mul_(mask)\n",
    "\n",
    "\n",
    "# quick smoke test\n",
    "model = TinyMLP()\n",
    "print(\"Total params before:\", sum(p.numel() for p in model.parameters()))\n",
    "global_magnitude_prune(model, sparsity=0.5)\n",
    "print(\"Non-zero params after prune:\",\n",
    "      sum((p != 0).sum().item() for p in model.parameters() if p.dim() > 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3ec79",
   "metadata": {},
   "source": [
    "## 2.2 Taylor-Based Pruningï¼ˆä¸€é˜¶ / äºŒé˜¶è¿‘ä¼¼ï¼‰\n",
    "\n",
    "### 2.2.1  æ•°å­¦åŸç†\n",
    "\n",
    "æˆ‘ä»¬æƒ³ä¼°è®¡ï¼šå°†æŸä¸ªå‚æ•° $w_i$ ç½® 0 æ—¶ï¼ŒæŸå¤±ä¼šå˜åŒ–å¤šå°‘ã€‚\n",
    "\n",
    "è®° $L(w)$ ä¸ºå¹³å‡æŸå¤±ï¼Œè€ƒè™‘åªæ”¹åŠ¨ç¬¬ $i$ ç»´ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta L_i = L(w_1,\\dots,0,\\dots,w_d) - L(w)\n",
    "$$\n",
    "\n",
    "ç”¨ä¸€ç»´ Taylor å±•å¼€è¿‘ä¼¼ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta L_i \\approx g_i \\Delta w_i + \\frac{1}{2} h_{ii} (\\Delta w_i)^2\n",
    "$$\n",
    "\n",
    "å…¶ä¸­\n",
    "\n",
    "- $g_i = \\frac{\\partial L}{\\partial w_i}$\n",
    "- $h_{ii} = \\frac{\\partial^2 L}{\\partial w_i^2}$\n",
    "- å‰ªææ—¶ä»¤ $\\Delta w_i = -w_i$\n",
    "\n",
    "ä»£å…¥å¾—åˆ°ï¼š\n",
    "\n",
    "- ä¸€é˜¶è¿‘ä¼¼ï¼š\n",
    "  $$\n",
    "  \\Delta L_i^{(1)} \\approx - g_i w_i\n",
    "  $$\n",
    "- äºŒé˜¶è¿‘ä¼¼ï¼š\n",
    "  $$\n",
    "  \\Delta L_i^{(2)} \\approx - g_i w_i + \\frac{1}{2} h_{ii} w_i^2\n",
    "  $$\n",
    "\n",
    "è‹¥åœ¨ã€Œæ¥è¿‘æ”¶æ•›ã€çš„ç‚¹å‰ªæï¼Œä¸€èˆ¬æœ‰ $\\mathbb{E}[g_i] \\approx 0$ï¼Œæ‰€ä»¥å¸¸ç”¨ **äºŒé˜¶é¡¹**ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta L_i^{(2)} \\approx \\frac{1}{2} h_{ii} w_i^2\n",
    "$$\n",
    "\n",
    "äºæ˜¯å¯ä»¥å®šä¹‰ **é‡è¦æ€§åˆ†æ•°(importance scoreï¼Œsaliency score)**ï¼š\n",
    "\n",
    "$$\n",
    "I_i \\propto h_{ii} w_i^2\n",
    "$$\n",
    "\n",
    "ä¸€é˜¶ç‰ˆæœ¬ï¼ˆMolchanovï¼‰åˆ™ä½¿ç”¨\n",
    "\n",
    "$$\n",
    "I_i^{(1)} = \\big|\\mathbb{E}[g_i w_i]\\big|\n",
    "$$\n",
    "\n",
    "åœ¨ mini-batch ä¸Šåšå¹³å‡ã€‚\n",
    "è¯¥é‡è¦æ€§æŒ‡æ ‡å¯ä»¥æŒ‰ weightã€filterã€channel èšåˆï¼Œç”¨äºç»“æ„åŒ–å‰ªæã€‚\n",
    "\n",
    "\n",
    "åœ¨å®è·µä¸­ï¼š\n",
    "\n",
    "- ç²¾ç¡® Hessian å¾ˆè´µï¼Œå¸¸ç”¨ diagonal è¿‘ä¼¼æˆ– Fisher è¿‘ä¼¼ã€‚\n",
    "- ä¸€é˜¶ç‰ˆæœ¬ï¼ˆMolchanovï¼‰åªéœ€è¦æ¢¯åº¦å’Œæƒé‡ï¼Œå°±èƒ½å¯¹ filter / channel åšç»“æ„åŒ–å‰ªæã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2.2. å·¥ç¨‹è½åœ°ï¼šHessian / Fisher è¿‘ä¼¼ä¸å®é™…æµç¨‹\n",
    "\n",
    "ç›´æ¥æ±‚ Hessian éå¸¸æ˜‚è´µï¼Œå› æ­¤å®è·µä¸­å¸¸ç”¨è¿‘ä¼¼ã€‚\n",
    "\n",
    "#### Hessian diagonalï¼ˆOBD / OBSï¼‰\n",
    "\n",
    "- Optimal Brain Damageï¼ˆLeCun, 1990ï¼‰ä½¿ç”¨ Hessian å¯¹è§’é¡¹ $h_{ii}$ï¼Œæå‡ºäº† $\\Delta L_i \\approx \\tfrac{1}{2} h_{ii} w_i^2$ çš„å½¢å¼ã€‚  \n",
    "- Optimal Brain Surgeonï¼ˆHassibi & Stork, 1993ï¼‰è¿›ä¸€æ­¥è€ƒè™‘ Hessian é€†çŸ©é˜µï¼Œæ›´ç²¾ç¡®ä½†ä»£ä»·æ›´å¤§ã€‚\n",
    "\n",
    "#### Fisher diagonal è¿‘ä¼¼ï¼ˆæœ€å¸¸ç”¨æ›¿ä»£ Hessianï¼‰\n",
    "\n",
    "åœ¨å¾ˆå¤šå·¥ç¨‹ç³»ç»Ÿä¸­ï¼Œç”¨ **ç»éªŒ Fisher ä¿¡æ¯çŸ©é˜µ** çš„å¯¹è§’æ¥è¿‘ä¼¼ Hessianï¼š\n",
    "\n",
    "$$\n",
    "F_{ii} \\approx \\mathbb{E}\\bigg[\\Big(\\frac{\\partial L}{\\partial w_i}\\Big)^2\\bigg].\n",
    "$$\n",
    "\n",
    "äºæ˜¯äºŒé˜¶åˆ†æ•°å¯ä»¥å†™æˆ\n",
    "\n",
    "$$\n",
    "I_i^{(F)} = F_{ii} w_i^2.\n",
    "$$\n",
    "\n",
    "ä¼˜åŠ¿ï¼š\n",
    "\n",
    "- ä¸éœ€è¦äºŒé˜¶æ¢¯åº¦ï¼Œåªéœ€åœ¨æ ¡å‡†æ•°æ®ä¸Šç»Ÿè®¡æ¢¯åº¦å¹³æ–¹ï¼›  \n",
    "- ä¸ EWC ç­‰å·¥ä½œé‡Œâ€œå‚æ•°é‡è¦æ€§â€ä¼°è®¡æ–¹æ³•ä¸€è‡´ï¼Œå·¥ç¨‹ä¸Šéå¸¸å¸¸è§ã€‚\n",
    "\n",
    "\n",
    "### 2.2.3. å®é™…å¯ç”¨ç®—æ³•ï¼ˆé€‚åˆ CNN / Transformer å­æ¨¡å—ï¼‰\n",
    "\n",
    "ä¸€ä¸ªå¯ç›´æ¥è½åœ°çš„ Taylor/Fisher å‰ªææµç¨‹ï¼š\n",
    "\n",
    "1. è®­ç»ƒå¥½åŸºçº¿æ¨¡å‹ï¼Œå›ºå®šå‚æ•° $w^*$ï¼›  \n",
    "2. å‡†å¤‡ä¸€ä»½ä»£è¡¨çœŸå®åˆ†å¸ƒçš„æ ¡å‡†æ•°æ®é›†ï¼ˆå‡ ç™¾åˆ°å‡ åƒä¸ª batch å³å¯ï¼‰ï¼›  \n",
    "3. åœ¨æ ¡å‡†æ•°æ®ä¸Šå¤šæ¬¡å‰å‘ + åå‘ï¼Œç´¯è®¡  \n",
    "   - ä¸€é˜¶åˆ†æ•°ï¼š$\\lvert g_i w_i \\rvert$  \n",
    "   - æˆ–äºŒé˜¶ï¼ˆFisherï¼‰åˆ†æ•°ï¼š$g_i^2 w_i^2$ï¼›  \n",
    "4. å°†æ‰€æœ‰åˆ†æ•°å±•å¼€ï¼Œåš **global æ’åº**ï¼›  \n",
    "5. æŒ‰ç›®æ ‡ sparsityï¼ˆä¾‹å¦‚ 50%ï¼‰æ‰¾åˆ°é˜ˆå€¼ï¼Œæ„é€ å…¨å±€å‰ªæ maskï¼›  \n",
    "6. åº”ç”¨å‰ªæï¼ˆmask æ‰å¯¹åº”æƒé‡æˆ–æ•´æ¡é€šé“ï¼‰ï¼›  \n",
    "7. ç”¨å‰ªæåçš„ç»“æ„åšè‹¥å¹² epoch çš„ fine-tuneï¼Œæ¢å¤æ€§èƒ½ã€‚\n",
    "\n",
    "å¦‚æœè¦åš **ç»“æ„åŒ–å‰ªæï¼ˆchannel/filterï¼‰**ï¼Œå¯ä»¥æŠŠåŒä¸€é€šé“å†…æ‰€æœ‰æƒé‡çš„åˆ†æ•°æ±‚å’Œæˆ–æ±‚å¹³å‡ä½œä¸ºè¯¥é€šé“çš„ importanceï¼Œå†å¯¹é€šé“æ’åºã€‚\n",
    "\n",
    "### 2.2.4 æ–‡çŒ®\n",
    "\n",
    "- **LeCun et al., â€œOptimal Brain Damageâ€, NeurIPS 1989.**  \n",
    "  <https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf>\n",
    "\n",
    "- **Hassibi & Stork, â€œSecond Order Derivatives for Network Pruning: Optimal Brain Surgeonâ€, NeurIPS 1992.**  \n",
    "  <https://proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf>   \n",
    "\n",
    "- **Amari, â€œNatural Gradient Works Efficiently in Learningâ€, Neural Computation, 1998.**  \n",
    "  <https://www.semanticscholar.org/paper/Natural-Gradient-Works-Eciently-in-Learning-Amari/e1c2a2fd6a26947e5bbb8df47e30c1199ab1270d>\n",
    "\n",
    "- **Kirkpatrick et al., â€œOvercoming Catastrophic Forgetting in Neural Networksâ€, PNAS 2017ï¼ˆEWCï¼‰.**  \n",
    "  <https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114>\n",
    "\n",
    "- **Molchanov et al., â€œPruning Convolutional Neural Networks for Resource Efficient Inferenceâ€, ICLR 2017.**  \n",
    "  <https://arxiv.org/abs/1611.06440>\n",
    "\n",
    "### 2.2.5 Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f6a94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1336f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 20042, Non-zero after pruning: 10074\n",
      "before: {'loss': 2.3057269527435302, 'acc': 0.0992}\n",
      "after : {'loss': 2.305605529785156, 'acc': 0.1012}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def collect_fisher_scores(model, dataloader, device=\"cpu\"):\n",
    "    \"\"\"åœ¨ä¸€ä»½æ ¡å‡†æ•°æ®ä¸Šç»Ÿè®¡ Fisher-based Taylor åˆ†æ•° g^2 * w^2.\"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    fisher_scores = {\n",
    "        name: torch.zeros_like(p)\n",
    "        for name, p in model.named_parameters()\n",
    "        if p.requires_grad and p.dim() > 1\n",
    "    }\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        model.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, p in model.named_parameters():\n",
    "                if name in fisher_scores and p.grad is not None:\n",
    "                    fisher_scores[name] += (p.grad ** 2) * (p.data ** 2)\n",
    "\n",
    "    return fisher_scores\n",
    "\n",
    "def collect_taylor_scores(model, dataloader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    taylor_scores = {name: torch.zeros_like(p.data) for name, p in model.named_parameters() if \"weight\" in name}\n",
    "    n_batches = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        model.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for name, p in model.named_parameters():\n",
    "                if \"weight\" in name and p.grad is not None:\n",
    "                    # è®¡ç®— |g * w| ä¸€é˜¶ Taylor æ•æ„Ÿåº¦åˆ†æ•°\n",
    "                    taylor_scores[name].add_((p.grad * p).abs())\n",
    "        n_batches += 1\n",
    "    for name in taylor_scores:\n",
    "        taylor_scores[name] /= max(1, n_batches)\n",
    "    return taylor_scores\n",
    "\n",
    "\n",
    "def prune_by_scores(model, score_dict, sparsity):\n",
    "    \"\"\"ç»™å®š importance score å­—å…¸ï¼Œåšä¸€æ¬¡ global å‰ªæ.\"\"\"\n",
    "    # æ‹‰å¹³æˆä¸€ä¸ªå‘é‡\n",
    "    all_scores = torch.cat([s.view(-1) for s in score_dict.values()])\n",
    "    k = int(all_scores.numel() * sparsity)\n",
    "    # é€‰å‡ºè¦å‰ªæ‰éƒ¨åˆ†çš„æœ€å¤§åˆ†æ•°ä½œä¸ºé˜ˆå€¼\n",
    "    threshold = all_scores.kthvalue(k).values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, p in model.named_parameters():\n",
    "            if name in score_dict:\n",
    "                mask = (score_dict[name] > threshold).float()\n",
    "                p.data.mul_(mask)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    tot_loss, tot_correct, tot = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            tot_loss += loss.item() * y.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            tot_correct += (preds == y).sum().item()\n",
    "            tot += y.size(0)\n",
    "    return {\"loss\": tot_loss / tot, \"acc\": tot_correct / tot}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "calib_ds_full = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "calib_ds = Subset(calib_ds_full, list(range(512)))  # use first 512 samples\n",
    "calib_loader = DataLoader(calib_ds, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "val_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "model = SmallCNN(num_classes=10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "orig_metrics = evaluate(model, val_loader, device)\n",
    "\n",
    "scores = collect_taylor_scores(model, calib_loader, device=device)\n",
    "prune_by_scores(model, scores, sparsity=0.5)\n",
    "pruned_metrics = evaluate(model, val_loader, device)\n",
    "\n",
    "# compare original model and pruned model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "nonzero_params = sum((p != 0).sum().item() for p in model.parameters() if p.dim() > 0)\n",
    "print(f\"Total params: {total_params}, Non-zero after pruning: {nonzero_params}\")\n",
    "\n",
    "# ignore accuracy number comparision the dataset is too small. the difference is just random noise.\n",
    "# But we can see with 50% weights pruned, the model size is halved. \n",
    "print(\"before:\", orig_metrics)\n",
    "print(\"after pruning by taylor score :\", pruned_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3628b778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 20042, Non-zero after pruning: 10074\n",
      "before: {'loss': 2.3052502223968507, 'acc': 0.112}\n",
      "after pruning by fisher score : {'loss': 2.3052051361083983, 'acc': 0.1248}\n"
     ]
    }
   ],
   "source": [
    "model = SmallCNN(num_classes=10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "orig_metrics = evaluate(model, val_loader, device)\n",
    "\n",
    "scores = collect_fisher_scores(model, calib_loader, device=device)\n",
    "prune_by_scores(model, scores, sparsity=0.5)\n",
    "pruned_metrics = evaluate(model, val_loader, device)\n",
    "\n",
    "# compare original model and pruned model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "nonzero_params = sum((p != 0).sum().item() for p in model.parameters() if p.dim() > 0)\n",
    "print(f\"Total params: {total_params}, Non-zero after pruning: {nonzero_params}\")\n",
    "\n",
    "# ignore accuracy number comparision the dataset is too small. the difference is just random noise.\n",
    "# But we can see with 50% weights pruned, the model size is halved. \n",
    "print(\"before:\", orig_metrics)\n",
    "print(\"after pruning by fisher score :\", pruned_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee607b4c",
   "metadata": {},
   "source": [
    "### 2.2.6 é™„ï¼šFisher Information Matrix\n",
    "#### - æ·±åº¦å­¦ä¹ ä¸­çš„æŸå¤±ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰\n",
    "\n",
    "åˆ†ç±»æ¨¡å‹ä¼˜åŒ–çš„ç›®æ ‡é€šå¸¸æ˜¯æœ€å¤§åŒ–æ­£ç¡®æ ‡ç­¾çš„æ¦‚ç‡ï¼š\n",
    "\n",
    "$ p(y \\mid x; \\theta) $\n",
    "\n",
    "è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆå³äº¤å‰ç†µï¼‰å®šä¹‰ä¸ºï¼š\n",
    "\n",
    "$ L(\\theta) = - \\mathbb{E}_{(x,y)\\sim \\text{data}} \\left[ \\log p(y \\mid x;\\theta) \\right] $\n",
    "\n",
    "å®é™…è®­ç»ƒä¸­ç”¨ mini-batch è¿‘ä¼¼ï¼š\n",
    "\n",
    "$ L(\\theta) \\approx - \\frac{1}{N} \\sum_{i=1}^{N} \\log p(y_i \\mid x_i; \\theta) $\n",
    "\n",
    "---\n",
    "\n",
    "#### - Fisher Information Matrix çš„å®šä¹‰\n",
    "\n",
    "Fisher ä¿¡æ¯çŸ©é˜µå®šä¹‰ä¸º log-likelihood æ¢¯åº¦çš„äºŒé˜¶çŸ©ï¼ˆå¤–ç§¯çš„æœŸæœ›ï¼‰ï¼š\n",
    "\n",
    "$ F(\\theta) = \\mathbb{E}_{x,y} \\left[ \\nabla_\\theta \\log p(y \\mid x; \\theta) \\, \\nabla_\\theta \\log p(y \\mid x; \\theta)^\\top \\right] $\n",
    "\n",
    "å¯¹è§’éƒ¨åˆ†\n",
    "$$F_{ii} = \\mathbb{E} \\left[\\left(\\frac{\\partial}{\\partial\\theta_i}\\log p(y\\mid x;\\theta)\\right)^2\\right]$$\n",
    "\n",
    "å®ƒè¡¡é‡ï¼š\n",
    "\n",
    "**å‚æ•°æ”¹å˜æ—¶ï¼Œæ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¼šå˜åŒ–å¤šå°‘ã€‚**\n",
    "\n",
    "å˜åŒ–è¶Šå¤§ï¼Œå‚æ•°è¶Šé‡è¦ï¼ŒFisher è¶Šå¤§ã€‚æ¢å¥è¯è¯´ï¼ŒFisher æ˜¯ log-likelihood æ¢¯åº¦çš„äºŒé˜¶çŸ© / åæ–¹å·®çŸ©\n",
    "\n",
    "---\n",
    "\n",
    "#### - ä¸ºä»€ä¹ˆ Fisher å¯ä»¥è¿‘ä¼¼ Hessianï¼Ÿ\n",
    "\n",
    "æ·±åº¦å­¦ä¹ æœ€å¸¸ç”¨çš„æŸå¤±æ˜¯è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼š\n",
    "\n",
    "$ L(\\theta) = - \\log p(y \\mid x;\\theta) $\n",
    "äºæ˜¯ï¼š\n",
    "$$\\frac{\\partial L}{\\partial\\theta_i} = -\\frac{\\partial}{\\partial \\theta_i} \\log p(y\\mid x;\\theta)$$\n",
    "\n",
    "å¯¹æ¯”ä¸Šé¢fisher å¯¹è§’éƒ¨åˆ†è®¡ç®—$\\left(\\frac{\\partial}{\\partial\\theta_i}\\log p(y\\mid x;\\theta)\\right)^2$\n",
    "\n",
    "$$F_{ii} = \\mathbb{E} \\left[\\left(\\frac{\\partial}{\\partial\\theta_i}\\log p(y\\mid x;\\theta)\\right)^2\\right]$$\n",
    "å°±å˜æˆï¼š\n",
    "$$F_{ii} = \\mathbb{E} \\left[\\left(\\frac{\\partial L}{\\partial\\theta_i}\\right)^2\\right]$$\n",
    "\n",
    "- Fisher diagonal å¯ä»¥ç›´æ¥ç”¨ loss å¯¹æƒé‡çš„æ¢¯åº¦å¹³æ–¹ä¼°è®¡\n",
    "- æ‰€ä»¥å‰ªæä»£ç é‡Œä¼šçœ‹åˆ° (grad ** 2) è€Œä¸æ˜¯ grad_log_probã€‚\n",
    "\n",
    "åœ¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ç†è®ºä¸­æœ‰ä¸€ä¸ªå…³é”®ç»“è®ºï¼š\n",
    "\n",
    "$ H(\\theta) = \\nabla_\\theta^2 L(\\theta) \\approx F(\\theta) $\n",
    "\n",
    "åŸå› å¦‚ä¸‹ï¼š\n",
    "\n",
    "- åœ¨æœ€ä¼˜ç‚¹ï¼Œæ¢¯åº¦æœŸæœ›ä¸º 0  \n",
    "- å¯¹ log-likelihood çš„äºŒé˜¶å¯¼åœ¨æ•°å­¦ä¸Šä¸æ¢¯åº¦å¤–ç§¯çš„æœŸæœ›ç­‰ä»·  \n",
    "- å› æ­¤ Hessian å’Œ Fisher åœ¨æ”¶æ•›åŒºåŸŸå…·æœ‰ç›¸åŒç»“æ„\n",
    "\n",
    "è¿™ç§°ä¸º **Hessianâ€“Fisher ç­‰ä»·æ€§**ã€‚\n",
    "\n",
    "å› æ­¤ï¼š\n",
    "\n",
    "$ h_{ii} \\approx F_{ii} $\n",
    "\n",
    "---\n",
    "\n",
    "#### - ä¸ºä»€ä¹ˆå‰ªæä¸­ä½¿ç”¨ Fisher diagonalï¼Ÿ\n",
    "\n",
    "Taylor äºŒé˜¶å±•å¼€ç»™å‡ºçš„æƒé‡é‡è¦æ€§ï¼š\n",
    "\n",
    "$ \\Delta L_i^{(2)} \\approx \\frac{1}{2} h_{ii} w_i^2 $\n",
    "\n",
    "è‹¥ Hessian diagonal éš¾è®¡ç®—ï¼Œå¯ç”¨ Fisher diagonal æ›¿ä»£ï¼š\n",
    "\n",
    "$ F_{ii} \\approx \\mathbb{E} \\left[ \\left( \\frac{\\partial L}{\\partial w_i} \\right)^2 \\right] $\n",
    "\n",
    "äºæ˜¯é‡è¦æ€§åˆ†æ•°å˜ä¸ºï¼š\n",
    "\n",
    "$ I_i = F_{ii} \\, w_i^2 $\n",
    "\n",
    "è¿™æ˜¯ç°ä»£å‰ªæï¼ˆå¦‚ Molchanov 2017ï¼‰ä¸è¿ç»­å­¦ä¹ ï¼ˆEWC 2017ï¼‰çš„ç»Ÿä¸€åŸºç¡€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### - å·¥ç¨‹ä¸Šå¦‚ä½•è®¡ç®— Fisher diagonal\n",
    "\n",
    "å¯¹æ ¡å‡†æ•°æ®æ‰§è¡Œå¤šæ¬¡ forward/backwardï¼Œå¹¶ç´¯ç§¯ï¼š\n",
    "\n",
    "$ F_{ii} \\approx \\frac{1}{N} \\sum_{n=1}^{N} g_{i,n}^2 $\n",
    "\n",
    "å…¶ä¸­ $ g_{i,n} $ ä¸ºç¬¬ n ä¸ª batch çš„æ¢¯åº¦ã€‚\n",
    "\n",
    "ç‰¹ç‚¹ï¼š\n",
    "\n",
    "- æ— éœ€äºŒé˜¶æ¢¯åº¦  \n",
    "- æ— éœ€ Hessian-vector product  \n",
    "- ä¸è®­ç»ƒæµç¨‹å…¼å®¹  \n",
    "- å¯åº”ç”¨äº CNN / Transformer / MLP\n",
    "\n",
    "---\n",
    "\n",
    "#### - å·¥ä¸šåº”ç”¨æ€»ç»“\n",
    "\n",
    "- **å‰ªæï¼ˆTaylor-based / Movement pruningï¼‰**ï¼šå‚æ•°æˆ–é€šé“é‡è¦æ€§  \n",
    "- **é‡åŒ–ï¼ˆå¦‚ SmoothQuantï¼‰**ï¼šæ¿€æ´»/æƒé‡é‡åŒ–æ•æ„Ÿåº¦  \n",
    "- **è¿ç»­å­¦ä¹ ï¼ˆEWCï¼‰**ï¼šå‚æ•°é‡è¦æ€§  \n",
    "- **è‡ªç„¶æ¢¯åº¦ä¼˜åŒ–**ï¼šFisher ä½œä¸ºå‚æ•°ç©ºé—´ä¸­çš„ Riemann åº¦é‡  \n",
    "\n",
    "Fisher diagonal æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„â€œè¿‘ä¼¼äºŒé˜¶ä¿¡æ¯â€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6aa655",
   "metadata": {},
   "source": [
    "## 2.3 â€” Global vs Layerwise & LAMPï¼ˆLayer-Adaptive Magnitude Pruningï¼‰\n",
    "\n",
    "### 2.3.1. é—®é¢˜ï¼šæ¯ä¸€å±‚å‰ªå¤šå°‘ï¼Ÿ\n",
    "\n",
    "Magnitude/Taylor ç»™äº†æ¯ä¸ªæƒé‡/é€šé“çš„åˆ†æ•°ï¼Œä½†æ²¡æœ‰å›ç­”ï¼š\n",
    "\n",
    "- æ¯ä¸€å±‚åº”è¯¥å‰ªå¤šå°‘ï¼Ÿ\n",
    "- å‡åŒ€å‰ªï¼Ÿæ·±å±‚å‰ªå¾—å¤šä¸€ç‚¹ï¼Ÿè¿˜æ˜¯æ‰‹è°ƒä¸€å †è¶…å‚ï¼Ÿ\n",
    "\n",
    "**LAMP**ï¼ˆLayer-Adaptive Magnitude-based Pruningï¼‰çš„ç›®æ ‡ï¼š\n",
    "\n",
    "> ç”¨ä¸€ä¸ªç®€å•çš„é‡æ ‡å®šï¼ˆrescalingï¼‰ï¼Œè®© global magnitude æ’åºè‡ªç„¶å¸¦å‡ºåˆç†çš„ã€Œå±‚è‡ªé€‚åº” sparsityã€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.2. LAMP æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "- æŠŠæŸä¸€å±‚çš„æƒé‡ $W$ å±•å¹³æˆä¸€ç»´ï¼Œå¹¶æŒ‰ç»å¯¹å€¼ä»å°åˆ°å¤§æ’åºï¼š\n",
    "$$|W[1]| \\le |W[2]| \\le \\dots \\le |W[n]|$$\n",
    "- å¯¹æ’åºä¹‹åçš„ç¬¬ $u$ ä¸ªå…ƒç´ ï¼Œå®šä¹‰\n",
    "$$\\text{score}(u; W) = \\frac{W[u]^2}{\\sum_{v \\ge u} W[v]^2}$$\n",
    "\n",
    "ä¹Ÿå°±æ˜¯**â€œè¿™ä¸ªæƒé‡çš„èƒ½é‡ / å‰©ä¸‹æ‰€æœ‰è¿˜æ²¡è¢«å‰ªæ‰çš„èƒ½é‡â€**\n",
    "\n",
    "ç›´è§‰ï¼š\n",
    "\n",
    "- æœ¬æ¥å°±ã€Œèƒ½é‡å¾ˆé›†ä¸­ã€å†—ä½™å°‘ã€çš„å±‚ï¼Œå¦‚æœå†å‰ªï¼Œä¼š hurt å¾—æ›´å¤šï¼Œå› æ­¤å…¶åˆ†æ•°è¢«æ”¾å¤§ã€‚\n",
    "- å†—ä½™å¾ˆå¤šçš„å±‚åˆ†æ•°è¢«å‹ç¼©ï¼Œå¯ä»¥å¤šå‰ªä¸€äº›ã€‚\n",
    "\n",
    "ç„¶ååšçš„äº‹å¾ˆç®€å•ï¼š\n",
    "\n",
    "- åœ¨æ¯ä¸€å±‚ç®—å‡ºè¿™ä¸ª scoreï¼›\n",
    "\n",
    "- æŠŠæ‰€æœ‰å±‚çš„ score æ‰”åœ¨ä¸€èµ·ï¼Œåšä¸€æ¬¡ global æ’åº + å…¨å±€é˜ˆå€¼ï¼›\n",
    "\n",
    "- é˜ˆå€¼ä»¥ä¸‹çš„éƒ½å‰ªæ‰ï¼Œå°±è‡ªåŠ¨ç»™å‡ºäº†æ¯å±‚è¯¥å‰ªå¤šå°‘ï¼ˆlayer-adaptiveï¼‰ã€‚\n",
    "\n",
    "ç»“æœï¼šç›´æ¥å¯¹ $s_i$ åš global æ’åºå‰ªæï¼Œå°±ä¼šè‡ªåŠ¨ç»™å‡ºã€Œå±‚è‡ªé€‚åº”ã€çš„é…é¢ã€‚\n",
    "\n",
    "### 2.3.3 ç®—æ³•\n",
    "#### 2.3.3.1 å•å±‚æƒ…å†µ\n",
    "å¯¹å•å±‚å…¨è¿æ¥ $y = W x$ï¼Œå¦‚æœæˆ‘ä»¬è¦åœ¨å›ºå®š sparsity $\\kappa$ ä¸‹æ‰¾ä¸€ä¸ª mask $M$ï¼ˆ0/1 çŸ©é˜µï¼‰å»å‰ªæï¼š\n",
    "\n",
    "å‰ªå®Œä¹‹åæ˜¯ $W_f = M \\odot W$ã€‚\n",
    "\n",
    "æƒ³è¦æœ€å°åŒ–æœ€åæƒ…å†µä¸‹çš„è¾“å‡º $â„“_2$ è¯¯å·®ï¼š\n",
    "$$\\min_{M, \\|M\\|_0 \\le \\kappa} \\ \\sup_{\\|x\\|_2 \\le 1} \\|W x - (M\\odot W) x\\|_2$$\n",
    "ç”¨è°±èŒƒæ•°çš„å®šä¹‰ + ä¸Šç•Œï¼Œå¯ä»¥æŠŠå®ƒ relax åˆ° Frobenius norm\n",
    "$$\\approx \\min_{M,\\|M\\|_0 \\le \\kappa} \\|W - M\\odot W\\|_F$$\n",
    "\n",
    "è¿™ä¸€æ­¥çš„ç»“è®ºï¼š\n",
    "åœ¨å•å±‚ä¸Šï¼Œè¦åœ¨ Frobenius å¤±çœŸæœ€å°çš„å‰æä¸‹é€‰ $\\kappa$ ä¸ªä¿ç•™çš„ weightï¼Œæ˜¾ç„¶å°±æ˜¯ã€Œä¿ç•™ $|W_{ij}|$ æœ€å¤§çš„é‚£å‡ ä¸ªã€â€”â€”ä¹Ÿå°±æ˜¯ layerwise magnitude pruning æœ¬èº«ã€‚https://arxiv.org/pdf/2010.07611\n",
    "\n",
    "æ‰€ä»¥ MPï¼ˆmagnitude pruningï¼‰å¯ä»¥è¢«ç†è§£æˆï¼š\n",
    "\n",
    "å¯¹æ¯ä¸€å±‚ï¼Œæ±‚è§£â€œåœ¨ç»™å®šå±‚å†… sparsity ä¸‹ï¼Œä½¿æƒé‡çš„ $â„“_2$ å¤±çœŸæœ€å°â€çš„æœ€ä¼˜æ–¹æ¡ˆ\n",
    "\n",
    "#### 2.3.3.2 æ•´ä¸ªç½‘ç»œï¼šæ¨¡å‹çº§è¾“å‡ºå¤±çœŸä¸Šç•Œ\n",
    "çœŸæ­£æƒ³è¦çš„æ˜¯ï¼šåœ¨ç»™å®šå…¨å±€ sparsity ä¸‹ï¼Œæœ€å°åŒ–æ¨¡å‹è¾“å‡ºçš„ $â„“_2$ å¤±çœŸï¼š\n",
    "$$\\min_{\\{M^{(i)}\\}} \\sup_{\\|x\\|_2\\le 1} \\big\\| f(x; W^{(1:d)}) - f(x; W_f^{(1:d)})\\big\\|_2$$\n",
    "å…¶ä¸­ $W^{(i)}$ æ˜¯ç¬¬ $i$ å±‚æƒé‡ï¼Œ$W_f^{(i)} = M^{(i)}\\odot W^{(i)}$ æ˜¯å‰ªæ‰åçš„æƒé‡.\n",
    "\n",
    "ç›´æ¥è§£è¿™ä¸ªå¤ªéš¾ï¼Œäºæ˜¯ä»–ä»¬æäº†ä¸€ä¸ªè´ªå¿ƒ + ä¸Šç•Œ:\n",
    "\n",
    "- å‡è®¾æ¯æ¬¡åªå‰ªæ‰ä¸€ä¸ªæƒé‡ï¼›\n",
    "\n",
    "- æ¨å‡ºä¸€ä¸ªæ¨¡å‹è¾“å‡ºè¯¯å·®çš„ä¸Šç•Œï¼ˆè¿™é‡Œç”¨äº†å¾ˆå¤šä¸ç­‰å¼å’Œå±‚é—´èŒƒæ•°çš„ä¹˜ç§¯ï¼‰ï¼š\n",
    "$$\\begin{array}{c} \\sup_{\\|x\\|_2\\le 1} \n",
    "\\big\\| f(x; W^{(1:d)}) - f(x; W^{(1:i-1)}, W_f^{(i)}, W^{(i+1:d)})\\big\\|_2\n",
    "\\\\\n",
    "\\le\n",
    "\\frac{\\|W^{(i)} - W_f^{(i)}\\|_F}{\\|W^{(i)}\\|_F} \\cdot \\prod_{j=1}^{d}\\|W^{(j)}\\|_F \\end{array}$$\n",
    "å³è¾¹é™¤äº† $|W^{(i)} - W_f^{(i)}|_F / |W^{(i)}|_F$ ä¹‹å¤–ï¼Œå…¶ä»–éƒ½æ˜¯å¸¸æ•°ï¼Œå®ƒä»¬å¯¹ã€Œå‰ªè°ã€è¿™ä»¶äº‹ä¸äº§ç”Ÿæ’åºå½±å“ã€‚\n",
    "\n",
    "äºæ˜¯é—®é¢˜å˜æˆï¼š\n",
    "\n",
    "æ¯å‰ªæ‰ä¸€ä¸ª weightï¼Œå¸Œæœ›è®©ã€Œæœ¬å±‚ç›¸å¯¹å¤±çœŸã€ $|W^{(i)} - W_f^{(i)}|_F / |W^{(i)}|_F$ çš„å¢é‡å°½é‡å°\n",
    "\n",
    "#### 2.3.3.3 LAMP score çš„å½¢å¼\n",
    "ç”±æ­¤æ¨å‡º LAMP score çš„å½¢å¼\n",
    "\n",
    "æŠŠä¸€å±‚çš„æƒé‡æŒ‰ä»å°åˆ°å¤§æ’åºåï¼Œæƒ³è±¡ä¸€ä¸ªä»å°åˆ°å¤§é€ä¸ªå‰ªæ‰çš„è¿‡ç¨‹ï¼š\n",
    "\n",
    "- å½“å‰åœ¨ä½ç½® $u$ ä¸Šçš„ weight æ˜¯ $W[u]$ï¼›\n",
    "\n",
    "- å‡è®¾ä¹‹å‰ $1,\\dots,u-1$ éƒ½å·²ç»è¢«å‰ªæ‰äº†ï¼›\n",
    "\n",
    "- å‰©ä½™çš„èƒ½é‡æ˜¯ $\\sum_{v\\ge u} W[v]^2$ï¼›\n",
    "\n",
    "- è¿™æ—¶å€™å¦‚æœå†å‰ªæ‰ $W[u]$ï¼Œå…¶å¸¦æ¥çš„æœ¬å±‚å¤±çœŸå¢é‡å’Œ $W[u]^2$ æˆæ­£æ¯”ï¼›\n",
    "\n",
    "- ä½†ã€Œç›¸å¯¹åç¨‹åº¦ã€å–å†³äºè¿™ä¸ªå¢é‡å å½“å‰å‰©ä½™é‡çš„æ¯”ä¾‹ã€‚\n",
    "\n",
    "æ‰€ä»¥å°±å¾—åˆ°ï¼š\n",
    "$$\\text{score}(u; W) = \\frac{W[u]^2}{\\sum_{v\\ge u} W[v]^2}$$\n",
    "è§£é‡Šï¼š\n",
    "\n",
    "- åˆ†å­ï¼šå½“å‰è¿™æ¡è¿æ¥çš„èƒ½é‡ï¼›\n",
    "\n",
    "- åˆ†æ¯ï¼šå‰ªæ‰å®ƒä¹‹å‰æœ¬å±‚å‰©ä½™è¿˜æ´»ç€çš„æ€»èƒ½é‡ï¼ˆæŠŠä¹‹å‰æ›´å°çš„éƒ½è§†ä¸ºå·²ç»å‰ªæ‰äº†ï¼‰ï¼›\n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "\n",
    "- å¦‚æœä½ å†å‰ªè¿™ä¸€æ¡ï¼Œä¼šæŠŠå‰©ä½™çš„èƒ½é‡åƒæ‰å¤šå°‘æ¯”ä¾‹ï¼Ÿ\n",
    "\n",
    "- é«˜åˆ†çš„æƒé‡ = ä¸€æ—¦å‰ªæ‰ï¼Œæœ¬å±‚ã€Œæœ€åä¸€ç‚¹è¡€ã€ä¼šæ‰å¾ˆå¤š â†’ æ›´è¯¥è¢«ä¿ç•™ï¼›\n",
    "- ä½åˆ†çš„æƒé‡ = æ‰€å¤„å±‚è¿˜å¾ˆå¯Œè£• / å†—ä½™å¤šï¼Œå¯ä»¥å¤šå‰ªã€‚\n",
    "\n",
    "ç„¶ååœ¨å…¨ç½‘ç»œä¸Šï¼Œæˆ‘ä»¬åªéœ€è¦ï¼š\n",
    "\n",
    "- å¯¹æ¯å±‚éƒ½ç®—è¿™ä¸ª scoreï¼›\n",
    "\n",
    "- ç”¨ä¸€ä¸ªå…¨å±€ threshold ç»Ÿä¸€å‰ªæï¼›\n",
    "\n",
    "- ç»“æœå°±æ˜¯ï¼šè‡ªç„¶å‡ºç°ã€ŒæŸäº›å±‚è¢«å¤šå‰ªï¼ŒæŸäº›å±‚è¢«å°‘å‰ªã€ï¼Œè€Œä¸éœ€è¦æ‰‹è°ƒå„å±‚ sparsityã€‚\n",
    "\n",
    "#### pytorch LAMP score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3afbd6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def lamp_LAMP_scores(w: torch.Tensor):\n",
    "    \"\"\"\n",
    "    æ ¹æ®åŸå§‹è®ºæ–‡çš„æ•°å­¦å®šä¹‰è®¡ç®— LAMP åˆ†æ•°ã€‚\n",
    "    Score(W_u) = W_u^2 / Sum(W_v^2 for all v >= u)\n",
    "    è¿”å›åŒå½¢çŠ¶çš„ score å¼ é‡ã€‚\n",
    "    \"\"\"\n",
    "    # 1. å±•å¹³å¼ é‡\n",
    "    flat = w.view(-1)\n",
    "\n",
    "    # 2. æŒ‰ç»å¯¹å€¼ä»å°åˆ°å¤§æ’åº\n",
    "    abs_w, idx = flat.abs().sort()\n",
    "\n",
    "    # 3. è®¡ç®—æ’åºåæƒé‡çš„å¹³æ–¹\n",
    "    sq = abs_w.pow(2)\n",
    "\n",
    "    # 4. è®¡ç®—å¹³æ–¹åçš„å‰ç¼€å’Œ (ä»å°åˆ°å¤§ç´¯åŠ )\n",
    "    cumsum = torch.cumsum(sq, dim=0)\n",
    "\n",
    "    # 5. è®¡ç®—å‰©ä½™èƒ½é‡ï¼ˆæ€»èƒ½é‡ - å‰ç¼€å’Œï¼‰\n",
    "    total_energy = sq.sum()\n",
    "    # remaining_energy_sum[i] ä»£è¡¨æ’åœ¨ç¬¬ i ä½ï¼ˆå«ï¼‰ä¹‹åæ‰€æœ‰æƒé‡çš„èƒ½é‡å’Œ\n",
    "    remaining_energy_sum = total_energy - cumsum + sq # åŠ ä¸Š sq[i] å› ä¸º cumsum[i] å‡å»äº†å®ƒ\n",
    "\n",
    "    # 6. è®¡ç®—åˆ†æ•°ï¼šå½“å‰æƒé‡çš„å¹³æ–¹ é™¤ä»¥ å‰©ä½™èƒ½é‡æ€»å’Œ\n",
    "    # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œæ·»åŠ ä¸€ä¸ªå°çš„ epsilon\n",
    "    epsilon = 1e-8\n",
    "    # åˆ†å­ï¼šå½“å‰æƒé‡å¹³æ–¹ (sq)ï¼Œåˆ†æ¯ï¼šå‰©ä½™èƒ½é‡å’Œ (remaining_energy_sum)\n",
    "    scores_sorted = sq / (remaining_energy_sum + epsilon)\n",
    "\n",
    "    # 7. è¿˜åŸåˆ°åŸå§‹å¼ é‡çš„å½¢çŠ¶å’Œä½ç½®\n",
    "    inv_idx = torch.empty_like(idx)\n",
    "    inv_idx[idx] = torch.arange(len(idx), device=w.device)\n",
    "    flat_scores = scores_sorted[inv_idx]\n",
    "    \n",
    "    return flat_scores.view_as(w)\n",
    "\n",
    "\n",
    "def compute_lamp_scores(model: nn.Module):\n",
    "    lamp_scores = {}\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            lamp_scores[name] = lamp_scores_for_tensor(p.data)\n",
    "    return lamp_scores\n",
    "\n",
    "\n",
    "# ä½¿ç”¨æ–¹å¼ï¼š\n",
    "# 1. scores = compute_lamp_scores(model)\n",
    "# 2. prune_by_scores(model, scores, sparsity=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28444ede",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3.4 æ–‡çŒ®\n",
    "Lee et al., *Layer-adaptive Sparsity for the Magnitude-based Pruning*, NeurIPS 2020.  \n",
    "<https://arxiv.org/abs/2010.07611>\n",
    "\n",
    "### 2.3.5 LAMP çš„å­¦æœ¯åœ°ä½ä¸å·¥ä¸šè½åœ°æƒ…å†µ\n",
    "åœ¨ **2021 å¹´ä¹‹åçš„å‰ªææ–‡çŒ®ä¸­**ï¼ŒLAMP ç»å¸¸è¢«å½“ä½œ  \n",
    "- ã€Œmagnitude-based global pruning ä¸‹çš„ layer-adaptive sparsity baselineã€ï¼Œ  \n",
    "- ç”¨æ¥å’Œä¿¡æ¯è®ºã€æ¦‚ç‡å›¾ã€é—¨æ§æœºåˆ¶ã€æ‹“æ‰‘æ–¹æ³•ç­‰æ›´å¤æ‚çš„å‰ªæç­–ç•¥åšå¯¹æ¯”\n",
    "\n",
    "å¯¹æ¯”å½“å‰ï¼ˆ2022â€“2025ï¼‰ä¸»æµå‹ç¼© / éƒ¨ç½²æ–¹å‘\n",
    "\n",
    "å¼•ç”¨æœ€è¿‘çš„æ¨¡å‹å‹ç¼©ç»¼è¿°ä¸å‰ªæè®ºæ–‡ï¼š\n",
    "\n",
    "- **A Survey on Model Compression for Deep Neural Networks (2023)**  \n",
    "  ğŸ”— https://arxiv.org/abs/2308.07610  \n",
    "\n",
    "- **Whatâ€™s Left? Pruning LLMs (2023)**  \n",
    "  ğŸ”— https://arxiv.org/abs/2302.10403  \n",
    "\n",
    "- **SparseGPTï¼ˆLLM å‰ªæä¸»æµæ–¹æ³•ï¼‰**  \n",
    "  ğŸ”— https://arxiv.org/abs/2301.00774  \n",
    "\n",
    "- **Wanda: Weights Are Not Equalï¼ˆLLM å‰ªæå¦ä¸€ä¸ªä¸»æµï¼‰**  \n",
    "  ğŸ”— https://arxiv.org/abs/2306.11695  \n",
    "\n",
    "è¿™äº›å·¥ä½œæœ‰å¦‚ä¸‹å…±è¯†ï¼š\n",
    "\n",
    "---\n",
    "\n",
    " âœ” Quantization æ˜¯å·¥ä¸šè½åœ°æ ¸å¿ƒï¼ˆINT8 / FP8 / BF16ï¼‰\n",
    "\n",
    "ç›¸æ¯”é‡åº¦å‰ªæï¼Œé‡åŒ–æ›´ç¨³å®šã€æ”¶ç›Šæ›´å¤§ï¼š\n",
    "\n",
    "- INT8ï¼šæ˜¾è‘—å‡å¸¦å®½  \n",
    "- FP8ï¼šNVIDIA Hopper æ¨åŠ¨æˆä¸ºæ–°æ¨ç†æ ‡å‡†  \n",
    "- BF16ï¼šè½¦ç«¯/äº‘ç«¯æœ€å¸¸è§çš„ mixed precision format  \n",
    "\n",
    "---\n",
    "\n",
    "âœ” çœŸè¦å‰ªæï¼Œå„æ¨¡å‹çš„ä¸»æµæ–¹æ³•å¦‚ä¸‹ï¼š\n",
    "\n",
    " **â‘  CNN / Visionï¼šStructured Pruning + Distillation**\n",
    "\n",
    "- Channel / Filter / Block å‰ªæ  \n",
    "- è¾…ä»¥è’¸é¦æ¥ä¿æŒç²¾åº¦  \n",
    "\n",
    "ç¤ºä¾‹ï¼š  \n",
    "- **Network Slimming**  \n",
    "  ğŸ”— https://arxiv.org/abs/1708.06519  \n",
    "\n",
    "- **DMCP**  \n",
    "  ğŸ”— https://arxiv.org/abs/2005.03354  \n",
    "\n",
    "---\n",
    "\n",
    "**â‘¡ Transformer / LLMï¼šMovement Pruning / SparseGPT / Wanda**\n",
    "\n",
    "Transformer å¯¹ unstructured magnitude pruningï¼ˆåŒ…æ‹¬å…¨å±€ magnitude + LAMP æ‰“åˆ†ï¼‰éå¸¸è„†å¼±ï¼Œå› æ­¤ä¸šç•Œè½¬å‘æ›´ç¨³å®šçš„å‰ªææ–¹å¼ï¼š\n",
    "\n",
    "- **Movement Pruningï¼ˆGoogle, 2020ï¼‰**  \n",
    "  ğŸ”— https://arxiv.org/abs/2005.07683  \n",
    "\n",
    "- **SparseGPTï¼ˆ2023ï¼‰**  \n",
    "  ğŸ”— https://arxiv.org/abs/2301.00774  \n",
    "\n",
    "- **Wandaï¼ˆ2023ï¼‰**  \n",
    "  ğŸ”— https://arxiv.org/abs/2306.11695  \n",
    "\n",
    "è¿™äº›æ–¹æ³•åœ¨ 70â€“90% sparsity ä¸‹çš„ç¨³å®šæ€§è¿œä¼˜äº magnitude-based å‰ªæã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**â‘¢ N:M ç¨€ç–ï¼ˆ2:4ï¼‰æ˜¯çœŸæ­£èƒ½åŠ é€Ÿçš„ç¨€ç–æ¨¡å¼**\n",
    "\n",
    "- æ”¯æŒ NVIDIA Ampere/Hopper çš„ sparse tensor core  \n",
    "- æ˜¯å”¯ä¸€çœŸæ­£èƒ½å¸¦æ¥ååæå‡çš„ç¨€ç–æ–¹å¼  \n",
    "- å¹¿æ³›ç”¨äºå·¥ä¸šæ¨ç†å¼•æ“  \n",
    "- ä¹Ÿå¯ä»¥ç»“åˆ LAMP åš **per-layer group-level sparsity allocation**\n",
    "\n",
    "ç›¸å…³è®ºæ–‡ï¼š  \n",
    "**Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch**  \n",
    "ğŸ”— https://arxiv.org/abs/2111.10988  \n",
    "\n",
    "---\n",
    "ç»¼åˆç»“è®ºï¼ˆå­¦æœ¯ vs å·¥ä¸šï¼‰\n",
    "\n",
    "\n",
    " ğŸ­ å·¥ä¸šè½åœ°è§’åº¦  \n",
    "LAMP **ä¸æ˜¯å·¥ä¸šç•Œçš„æœ€ç»ˆå‰ªææ–¹æ¡ˆ**ï¼Œè€Œæ˜¯ï¼š\n",
    "\n",
    "- ä¸€ç§ **layer-wise sparsity allocator**  \n",
    "- å¸¸ç”¨äºåˆ†æå“ªäº›å±‚å†—ä½™å¤§ã€å“ªäº›å±‚ä¸èƒ½ç¢°  \n",
    "- å¯èƒ½ä¸ N:Mã€structured pruningã€QATã€distillation ç­‰ç»“åˆä½¿ç”¨  \n",
    "\n",
    "çœŸæ­£ä¸Šçº¿çš„ pipeline ä¸€èˆ¬æ˜¯ï¼š\n",
    "\n",
    "1. **Structured pruningï¼ˆchannel/head/blockï¼‰**  \n",
    "2. **N:M ç¨€ç–ï¼ˆ2:4ï¼‰**  \n",
    "3. **é‡åŒ–ï¼ˆINT8, FP8, BF16ï¼‰**  \n",
    "4. **è’¸é¦æ¢å¤ç²¾åº¦**  \n",
    "5. é’ˆå¯¹ç¡¬ä»¶çš„ **è‡ªå®šä¹‰ kernel / compiler pass**\n",
    "\n",
    "è€Œä¸æ˜¯ï¼š\n",
    "\n",
    "> ã€Œä»…é  LAMP åš 90% unstructured pruning åç›´æ¥éƒ¨ç½²ã€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**ä¸€å¥è¯æ€»ç»“ï¼š**\n",
    "\n",
    "> **LAMP = å¼ºå¤§çš„å±‚è‡ªé€‚åº” sparsity åˆ†é…å™¨ï¼ˆacademic SOTA baselineï¼‰ï¼Œä½†ä¸æ˜¯å·¥ä¸šç«¯æœ€ç»ˆçš„é«˜æ€§èƒ½å‰ªææ–¹æ¡ˆã€‚**  \n",
    "> å·¥ä¸šç•Œæ›´ä¾èµ– structured/N:M sparsity + quantization + hardware-specific kernelsã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972044f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
