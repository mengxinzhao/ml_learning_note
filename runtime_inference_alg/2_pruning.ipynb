{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edbe00ba",
   "metadata": {},
   "source": [
    "# 2 Pruning å‰ªæ\n",
    "## 2.1 Magnitude-Based Unstructured Pruning\n",
    "\n",
    "### 2.1.1 æ•°å­¦åŸç†\n",
    "\n",
    "Magnitude pruning å‡è®¾ï¼šæƒé‡ç»å¯¹å€¼è¶Šå°ï¼Œå¯¹è¾“å‡ºå½±å“è¶Šå¼±ï¼Œå› æ­¤å¯ä»¥ä¼˜å…ˆåˆ é™¤ã€‚è¿™æ˜¯ä¸€ç§å…¸å‹çš„ **unstructured å‰ªæ** æ–¹æ³•ã€‚\n",
    "\n",
    "è®¾åŸå§‹æƒé‡ä¸º $w \\in \\mathbb{R}^d$ï¼Œå‰ªæ mask ä¸º $m \\in \\{0,1\\}^d$ï¼Œå‰ªæåæƒé‡ä¸º $w \\odot m$ã€‚ç†æƒ³ç›®æ ‡æ˜¯ï¼š\n",
    "\n",
    "$$\n",
    "\\min_m L(w \\odot m) \\quad \\text{s.t.}\\quad \\|m\\|_0 = k,\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $\\|m\\|_0$ æ˜¯ä¿ç•™ä¸‹æ¥çš„æƒé‡ä¸ªæ•°ï¼ˆç¨€ç–åº¦çº¦æŸï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œç›´æ¥æ±‚è§£æ˜¯ NP-hardã€‚\n",
    "\n",
    "Magnitude pruning é‡‡ç”¨ä¸€ä¸ªéå¸¸ç®€å•çš„å¯å‘å¼ï¼š\n",
    "\n",
    "> æŒ‰æƒé‡çš„ç»å¯¹å€¼ $|w_i|$ ä»å°åˆ°å¤§æ’åºï¼Œåˆ é™¤æœ€å°çš„ $s\\%$ æƒé‡ã€‚\n",
    "\n",
    "ä¹Ÿå¯ä»¥å†™æˆï¼šåœ¨æ‰€æœ‰å¤§å°ä¸º $sd$ çš„é›†åˆ $S$ ä¸­ï¼Œé€‰æ‹©\n",
    "\n",
    "$$\n",
    "S^* = \\arg\\min_{S:\\,|S| = sd} \\sum_{i \\in S} |w_i|,\n",
    "$$\n",
    "\n",
    "å¹¶å°† $S^*$ ä¸­çš„æƒé‡ç½®é›¶ã€‚\n",
    "\n",
    "### 2.1.2 ç†è®ºä¾æ®ï¼ˆOptimal Brain Damage è§†è§’ï¼‰\n",
    "\n",
    "Optimal Brain Damageï¼ˆLeCun, 1990ï¼‰ä»äºŒé˜¶æ³°å‹’å±•å¼€ç»™å‡ºäº†æƒé‡é‡è¦æ€§çš„ç»å…¸å½¢å¼ã€‚å¯¹æŸå¤± $L(w)$ï¼Œå¯¹æŸä¸ªæƒé‡ $w_i$ è¿›è¡Œå¾®å°æ‰°åŠ¨ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta L_i \\approx g_i \\Delta w_i + \\frac{1}{2} h_{ii} (\\Delta w_i)^2,\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $g_i = \\partial L/\\partial w_i$ï¼Œ$h_{ii}$ æ˜¯ Hessian å¯¹è§’å…ƒç´ ã€‚è‹¥åœ¨æ”¶æ•›ç‚¹é™„è¿‘ï¼Œ$g_i \\approx 0$ï¼Œå¹¶ä»¤å‰ªææ—¶ $\\Delta w_i = -w_i$ï¼Œåˆ™æœ‰ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta L_i \\approx \\frac{1}{2} h_{ii} w_i^2.\n",
    "$$\n",
    "\n",
    "å¦‚æœåœ¨åŒä¸€å±‚å†… $h_{ii}$ å˜åŒ–ä¸å¤§ï¼ˆæˆ–ä½¿ç”¨å¯¹è§’è¿‘ä¼¼ï¼‰ï¼Œåˆ™æœ‰\n",
    "\n",
    "$$\n",
    "\\Delta L_i \\propto w_i^2.\n",
    "$$\n",
    "\n",
    "å› æ­¤ï¼Œ**æƒé‡ç»å¯¹å€¼è¶Šå°ï¼Œå¯¹æŸå¤±çš„å½±å“è¶Šå°ï¼Œè¶Šé€‚åˆè¢«å‰ªæ‰**ï¼Œè¿™ä¸º magnitude pruning æä¾›äº†ä¸€ä¸ªç®€å•ä½†åˆç†çš„ç†è®ºåŸºç¡€ã€‚\n",
    "\n",
    "### 2.1.3 Global vs Layerwise\n",
    "\n",
    "- **Global pruning**ï¼šåœ¨å…¨æ¨¡å‹èŒƒå›´å†…å¯¹æ‰€æœ‰æƒé‡çš„ $|w_i|$ åšä¸€æ¬¡æ’åºï¼Œåˆ é™¤æ€»æ•°ä¸­æœ€å°çš„ $s\\%$ã€‚  \n",
    "  - æ›´æ¥è¿‘ä¸Šé¢çš„ä¼˜åŒ–å½¢å¼ï¼Œä¸€èˆ¬æ€§èƒ½æ›´å¥½ã€‚\n",
    "\n",
    "- **Layerwise pruning**ï¼šæ¯ä¸€å±‚å•ç‹¬æŒ‰ç…§ $|w_i|$ æ’åºï¼Œå„è‡ªå‰ªæ‰åŒæ ·æ¯”ä¾‹ã€‚  \n",
    "  - å®ç°ç®€å•ï¼Œä½†å¯¹æœ‰äº›å±‚å¯èƒ½è¿‡åº¦å‰ªæï¼Œå¯¹å¦ä¸€äº›å±‚å‰ªå¾—å¤ªå°‘ï¼Œæ•´ä½“ suboptimalã€‚\n",
    "\n",
    "åœ¨å·¥ç¨‹å®è·µä¸­ï¼Œglobal magnitude pruning é€šå¸¸ä½œä¸ºé»˜è®¤é€‰æ‹©ã€‚\n",
    "\n",
    "### 2.1.4 å®é™…å·¥ç¨‹æµç¨‹\n",
    "\n",
    "1. è®­ç»ƒä¸€ä¸ªå…¨ç²¾åº¦åŸºçº¿æ¨¡å‹ï¼Œå¾—åˆ°æƒé‡ $w^*$ï¼›  \n",
    "2. æ”¶é›†æ‰€æœ‰æƒé‡çš„ç»å¯¹å€¼ $|w_i^*|$ï¼›  \n",
    "3. åœ¨å…¨æ¨¡å‹èŒƒå›´æŒ‰ $|w_i^*|$ æ’åºï¼›  \n",
    "4. é€‰æ‹© sparsity æ¯”ä¾‹ $s$ï¼Œåˆ é™¤æœ€å°çš„ $s\\%$ æƒé‡ï¼ˆå¯¹åº”ä½ç½®çš„ $m_i = 0$ï¼‰ï¼›  \n",
    "5. å†»ç»“å‰ªæç»“æ„ï¼Œå¯¹å‰©ä½™æƒé‡è¿›è¡Œè‹¥å¹² epoch çš„ fine-tuneï¼Œæ¢å¤ç²¾åº¦ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯æœ€ç»å…¸ã€æœ€å¸¸ç”¨çš„ **unstructured magnitude pruning pipeline**ã€‚\n",
    "\n",
    "### 2.1.5 æ–‡çŒ®\n",
    "\n",
    "- **LeCun et al., *Optimal Brain Damage*, 1990**  \n",
    "  <http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf>  \n",
    "  - æœ€æ—©ç³»ç»Ÿè®¨è®ºåŸºäºæƒé‡é‡è¦æ€§çš„è¿æ¥å‰ªæï¼Œç»™å‡ºäºŒé˜¶æ³°å‹’å±•å¼€ä¸ Hessian å¯¹è§’è¿‘ä¼¼ã€‚\n",
    "\n",
    "- **Molchanov et al., *Pruning Convolutional Neural Networks for Resource Efficient Inference*, ICLR 2017**  \n",
    "  <https://arxiv.org/abs/1611.06440>  \n",
    "  - ç°ä»£å‰ªæå·¥ä½œä¸­å¸¸ç”¨çš„åŸºçº¿ä¹‹ä¸€ï¼Œæ˜ç¡®å°† â€œæŒ‰æƒé‡ç»å¯¹å€¼æ’åºåå‰ªæ‰æœ€å°éƒ¨åˆ†â€ ä½œä¸º magnitude-based pruning baselineã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cc4a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params before: 203530\n",
      "k=101632, total weights=203264\n",
      "Non-zero params after prune: 101898\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, in_dim=784, hidden=256, out_dim=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "def global_magnitude_prune(model: nn.Module, sparsity: float):\n",
    "    \"\"\"\n",
    "    å¯¹æ¨¡å‹åšä¸€æ¬¡ global magnitude å‰ªæï¼ˆåªå¯¹ weight å‚æ•°ï¼‰.\n",
    "\n",
    "    Args:\n",
    "        model: nn.Module\n",
    "        sparsity: è¦å‰ªæ‰çš„æ¯”ä¾‹ (0 ~ 1)ï¼Œä¾‹å¦‚ 0.5 è¡¨ç¤ºå‰ªæ‰ 50% æœ€å°æƒé‡\n",
    "    \"\"\"\n",
    "    # æ”¶é›†æ‰€æœ‰ weight\n",
    "    weight_tensors = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            weight_tensors.append(p.data.view(-1))\n",
    "\n",
    "    all_weights = torch.cat(weight_tensors)\n",
    "    k = int(sparsity * all_weights.numel())\n",
    "    print(f\"{k=}, total weights={all_weights.numel()}\")\n",
    "\n",
    "    # kth-value æ˜¯ä»å°åˆ°å¤§æ’åºåç¬¬ k ä¸ªå…ƒç´ \n",
    "    threshold = all_weights.abs().kthvalue(k).values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, p in model.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                mask = (p.data.abs() > threshold).float()\n",
    "                p.data.mul_(mask)\n",
    "\n",
    "\n",
    "# quick smoke test\n",
    "model = TinyMLP()\n",
    "print(\"Total params before:\", sum(p.numel() for p in model.parameters()))\n",
    "global_magnitude_prune(model, sparsity=0.5)\n",
    "print(\"Non-zero params after prune:\",\n",
    "      sum((p != 0).sum().item() for p in model.parameters() if p.dim() > 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3ec79",
   "metadata": {},
   "source": [
    "## 2.2 Taylor-Based Pruningï¼ˆä¸€é˜¶ / äºŒé˜¶è¿‘ä¼¼ï¼‰\n",
    "\n",
    "### 2.2.1  æ•°å­¦åŸç†\n",
    "\n",
    "æˆ‘ä»¬æƒ³ä¼°è®¡ï¼šå°†æŸä¸ªå‚æ•° $w_i$ ç½® 0 æ—¶ï¼ŒæŸå¤±ä¼šå˜åŒ–å¤šå°‘ã€‚\n",
    "\n",
    "è®° $L(w)$ ä¸ºå¹³å‡æŸå¤±ï¼Œè€ƒè™‘åªæ”¹åŠ¨ç¬¬ $i$ ç»´ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta L_i = L(w_1,\\dots,0,\\dots,w_d) - L(w)\n",
    "$$\n",
    "\n",
    "ç”¨ä¸€ç»´ Taylor å±•å¼€è¿‘ä¼¼ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta L_i \\approx g_i \\Delta w_i + \\frac{1}{2} h_{ii} (\\Delta w_i)^2\n",
    "$$\n",
    "\n",
    "å…¶ä¸­\n",
    "\n",
    "- $g_i = \\frac{\\partial L}{\\partial w_i}$\n",
    "- $h_{ii} = \\frac{\\partial^2 L}{\\partial w_i^2}$\n",
    "- å‰ªææ—¶ä»¤ $\\Delta w_i = -w_i$\n",
    "\n",
    "ä»£å…¥å¾—åˆ°ï¼š\n",
    "\n",
    "- ä¸€é˜¶è¿‘ä¼¼ï¼š\n",
    "  $$\n",
    "  \\Delta L_i^{(1)} \\approx - g_i w_i\n",
    "  $$\n",
    "- äºŒé˜¶è¿‘ä¼¼ï¼š\n",
    "  $$\n",
    "  \\Delta L_i^{(2)} \\approx - g_i w_i + \\frac{1}{2} h_{ii} w_i^2\n",
    "  $$\n",
    "\n",
    "è‹¥åœ¨ã€Œæ¥è¿‘æ”¶æ•›ã€çš„ç‚¹å‰ªæï¼Œä¸€èˆ¬æœ‰ $\\mathbb{E}[g_i] \\approx 0$ï¼Œæ‰€ä»¥å¸¸ç”¨ **äºŒé˜¶é¡¹**ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta L_i^{(2)} \\approx \\frac{1}{2} h_{ii} w_i^2\n",
    "$$\n",
    "\n",
    "äºæ˜¯å¯ä»¥å®šä¹‰ **é‡è¦æ€§åˆ†æ•°(importance scoreï¼Œsaliency score)**ï¼š\n",
    "\n",
    "$$\n",
    "I_i \\propto h_{ii} w_i^2\n",
    "$$\n",
    "\n",
    "ä¸€é˜¶ç‰ˆæœ¬ï¼ˆMolchanovï¼‰åˆ™ä½¿ç”¨\n",
    "\n",
    "$$\n",
    "I_i^{(1)} = \\big|\\mathbb{E}[g_i w_i]\\big|\n",
    "$$\n",
    "\n",
    "åœ¨ mini-batch ä¸Šåšå¹³å‡ã€‚\n",
    "è¯¥é‡è¦æ€§æŒ‡æ ‡å¯ä»¥æŒ‰ weightã€filterã€channel èšåˆï¼Œç”¨äºç»“æ„åŒ–å‰ªæã€‚\n",
    "\n",
    "\n",
    "åœ¨å®è·µä¸­ï¼š\n",
    "\n",
    "- ç²¾ç¡® Hessian å¾ˆè´µï¼Œå¸¸ç”¨ diagonal è¿‘ä¼¼æˆ– Fisher è¿‘ä¼¼ã€‚\n",
    "- ä¸€é˜¶ç‰ˆæœ¬ï¼ˆMolchanovï¼‰åªéœ€è¦æ¢¯åº¦å’Œæƒé‡ï¼Œå°±èƒ½å¯¹ filter / channel åšç»“æ„åŒ–å‰ªæã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2.2. å·¥ç¨‹è½åœ°ï¼šHessian / Fisher è¿‘ä¼¼ä¸å®é™…æµç¨‹\n",
    "\n",
    "ç›´æ¥æ±‚ Hessian éå¸¸æ˜‚è´µï¼Œå› æ­¤å®è·µä¸­å¸¸ç”¨è¿‘ä¼¼ã€‚\n",
    "\n",
    "#### Hessian diagonalï¼ˆOBD / OBSï¼‰\n",
    "\n",
    "- Optimal Brain Damageï¼ˆLeCun, 1990ï¼‰ä½¿ç”¨ Hessian å¯¹è§’é¡¹ $h_{ii}$ï¼Œæå‡ºäº† $\\Delta L_i \\approx \\tfrac{1}{2} h_{ii} w_i^2$ çš„å½¢å¼ã€‚  \n",
    "- Optimal Brain Surgeonï¼ˆHassibi & Stork, 1993ï¼‰è¿›ä¸€æ­¥è€ƒè™‘ Hessian é€†çŸ©é˜µï¼Œæ›´ç²¾ç¡®ä½†ä»£ä»·æ›´å¤§ã€‚\n",
    "\n",
    "#### Fisher diagonal è¿‘ä¼¼ï¼ˆæœ€å¸¸ç”¨æ›¿ä»£ Hessianï¼‰\n",
    "\n",
    "åœ¨å¾ˆå¤šå·¥ç¨‹ç³»ç»Ÿä¸­ï¼Œç”¨ **ç»éªŒ Fisher ä¿¡æ¯çŸ©é˜µ** çš„å¯¹è§’æ¥è¿‘ä¼¼ Hessianï¼š\n",
    "\n",
    "$$\n",
    "F_{ii} \\approx \\mathbb{E}\\bigg[\\Big(\\frac{\\partial L}{\\partial w_i}\\Big)^2\\bigg].\n",
    "$$\n",
    "\n",
    "äºæ˜¯äºŒé˜¶åˆ†æ•°å¯ä»¥å†™æˆ\n",
    "\n",
    "$$\n",
    "I_i^{(F)} = F_{ii} w_i^2.\n",
    "$$\n",
    "\n",
    "ä¼˜åŠ¿ï¼š\n",
    "\n",
    "- ä¸éœ€è¦äºŒé˜¶æ¢¯åº¦ï¼Œåªéœ€åœ¨æ ¡å‡†æ•°æ®ä¸Šç»Ÿè®¡æ¢¯åº¦å¹³æ–¹ï¼›  \n",
    "- ä¸ EWC ç­‰å·¥ä½œé‡Œâ€œå‚æ•°é‡è¦æ€§â€ä¼°è®¡æ–¹æ³•ä¸€è‡´ï¼Œå·¥ç¨‹ä¸Šéå¸¸å¸¸è§ã€‚\n",
    "\n",
    "\n",
    "### 2.2.3. å®é™…å¯ç”¨ç®—æ³•ï¼ˆé€‚åˆ CNN / Transformer å­æ¨¡å—ï¼‰\n",
    "\n",
    "ä¸€ä¸ªå¯ç›´æ¥è½åœ°çš„ Taylor/Fisher å‰ªææµç¨‹ï¼š\n",
    "\n",
    "1. è®­ç»ƒå¥½åŸºçº¿æ¨¡å‹ï¼Œå›ºå®šå‚æ•° $w^*$ï¼›  \n",
    "2. å‡†å¤‡ä¸€ä»½ä»£è¡¨çœŸå®åˆ†å¸ƒçš„æ ¡å‡†æ•°æ®é›†ï¼ˆå‡ ç™¾åˆ°å‡ åƒä¸ª batch å³å¯ï¼‰ï¼›  \n",
    "3. åœ¨æ ¡å‡†æ•°æ®ä¸Šå¤šæ¬¡å‰å‘ + åå‘ï¼Œç´¯è®¡  \n",
    "   - ä¸€é˜¶åˆ†æ•°ï¼š$\\lvert g_i w_i \\rvert$  \n",
    "   - æˆ–äºŒé˜¶ï¼ˆFisherï¼‰åˆ†æ•°ï¼š$g_i^2 w_i^2$ï¼›  \n",
    "4. å°†æ‰€æœ‰åˆ†æ•°å±•å¼€ï¼Œåš **global æ’åº**ï¼›  \n",
    "5. æŒ‰ç›®æ ‡ sparsityï¼ˆä¾‹å¦‚ 50%ï¼‰æ‰¾åˆ°é˜ˆå€¼ï¼Œæ„é€ å…¨å±€å‰ªæ maskï¼›  \n",
    "6. åº”ç”¨å‰ªæï¼ˆmask æ‰å¯¹åº”æƒé‡æˆ–æ•´æ¡é€šé“ï¼‰ï¼›  \n",
    "7. ç”¨å‰ªæåçš„ç»“æ„åšè‹¥å¹² epoch çš„ fine-tuneï¼Œæ¢å¤æ€§èƒ½ã€‚\n",
    "\n",
    "å¦‚æœè¦åš **ç»“æ„åŒ–å‰ªæï¼ˆchannel/filterï¼‰**ï¼Œå¯ä»¥æŠŠåŒä¸€é€šé“å†…æ‰€æœ‰æƒé‡çš„åˆ†æ•°æ±‚å’Œæˆ–æ±‚å¹³å‡ä½œä¸ºè¯¥é€šé“çš„ importanceï¼Œå†å¯¹é€šé“æ’åºã€‚\n",
    "\n",
    "### 2.2.4 æ–‡çŒ®\n",
    "\n",
    "- **LeCun et al., â€œOptimal Brain Damageâ€, NeurIPS 1989.**  \n",
    "  <https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf>\n",
    "\n",
    "- **Hassibi & Stork, â€œSecond Order Derivatives for Network Pruning: Optimal Brain Surgeonâ€, NeurIPS 1992.**  \n",
    "  <https://proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf>   \n",
    "\n",
    "- **Amari, â€œNatural Gradient Works Efficiently in Learningâ€, Neural Computation, 1998.**  \n",
    "  <https://www.semanticscholar.org/paper/Natural-Gradient-Works-Eciently-in-Learning-Amari/e1c2a2fd6a26947e5bbb8df47e30c1199ab1270d>\n",
    "\n",
    "- **Kirkpatrick et al., â€œOvercoming Catastrophic Forgetting in Neural Networksâ€, PNAS 2017ï¼ˆEWCï¼‰.**  \n",
    "  <https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114>\n",
    "\n",
    "- **Molchanov et al., â€œPruning Convolutional Neural Networks for Resource Efficient Inferenceâ€, ICLR 2017.**  \n",
    "  <https://arxiv.org/abs/1611.06440>\n",
    "\n",
    "### 2.2.5 Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f6a94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1336f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 20042, Non-zero after pruning: 10074\n",
      "before: {'loss': 2.305746308517456, 'acc': 0.1092}\n",
      "after pruning by taylor score : {'loss': 2.3057271259307863, 'acc': 0.109}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def collect_fisher_scores(model, dataloader, device=\"cpu\"):\n",
    "    \"\"\"åœ¨ä¸€ä»½æ ¡å‡†æ•°æ®ä¸Šç»Ÿè®¡ Fisher-based Taylor åˆ†æ•° g^2 * w^2.\"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    fisher_scores = {\n",
    "        name: torch.zeros_like(p)\n",
    "        for name, p in model.named_parameters()\n",
    "        if p.requires_grad and p.dim() > 1\n",
    "    }\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        model.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, p in model.named_parameters():\n",
    "                if name in fisher_scores and p.grad is not None:\n",
    "                    fisher_scores[name] += (p.grad ** 2) * (p.data ** 2)\n",
    "\n",
    "    return fisher_scores\n",
    "\n",
    "def collect_taylor_scores(model, dataloader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    taylor_scores = {name: torch.zeros_like(p.data) for name, p in model.named_parameters() if \"weight\" in name}\n",
    "    n_batches = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        model.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for name, p in model.named_parameters():\n",
    "                if \"weight\" in name and p.grad is not None:\n",
    "                    # è®¡ç®— |g * w| ä¸€é˜¶ Taylor æ•æ„Ÿåº¦åˆ†æ•°\n",
    "                    taylor_scores[name].add_((p.grad * p).abs())\n",
    "        n_batches += 1\n",
    "    for name in taylor_scores:\n",
    "        taylor_scores[name] /= max(1, n_batches)\n",
    "    return taylor_scores\n",
    "\n",
    "\n",
    "def prune_by_scores(model, score_dict, sparsity):\n",
    "    \"\"\"ç»™å®š importance score å­—å…¸ï¼Œåšä¸€æ¬¡ global å‰ªæ.\"\"\"\n",
    "    # æ‹‰å¹³æˆä¸€ä¸ªå‘é‡\n",
    "    all_scores = torch.cat([s.view(-1) for s in score_dict.values()])\n",
    "    k = int(all_scores.numel() * sparsity)\n",
    "    # é€‰å‡ºè¦å‰ªæ‰éƒ¨åˆ†çš„æœ€å¤§åˆ†æ•°ä½œä¸ºé˜ˆå€¼\n",
    "    threshold = all_scores.kthvalue(k).values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, p in model.named_parameters():\n",
    "            if name in score_dict:\n",
    "                mask = (score_dict[name] > threshold).float()\n",
    "                p.data.mul_(mask)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    tot_loss, tot_correct, tot = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            tot_loss += loss.item() * y.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            tot_correct += (preds == y).sum().item()\n",
    "            tot += y.size(0)\n",
    "    return {\"loss\": tot_loss / tot, \"acc\": tot_correct / tot}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "calib_ds_full = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "calib_ds = Subset(calib_ds_full, list(range(512)))  # use first 512 samples\n",
    "calib_loader = DataLoader(calib_ds, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "val_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "model = SmallCNN(num_classes=10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "orig_metrics = evaluate(model, val_loader, device)\n",
    "\n",
    "scores = collect_taylor_scores(model, calib_loader, device=device)\n",
    "prune_by_scores(model, scores, sparsity=0.5)\n",
    "pruned_metrics = evaluate(model, val_loader, device)\n",
    "\n",
    "# compare original model and pruned model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "nonzero_params = sum((p != 0).sum().item() for p in model.parameters() if p.dim() > 0)\n",
    "print(f\"Total params: {total_params}, Non-zero after pruning: {nonzero_params}\")\n",
    "\n",
    "# ignore accuracy number comparision the dataset is too small. the difference is just random noise.\n",
    "# But we can see with 50% weights pruned, the model size is halved. \n",
    "print(\"before:\", orig_metrics)\n",
    "print(\"after pruning by taylor score :\", pruned_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3628b778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 20042, Non-zero after pruning: 10074\n",
      "before: {'loss': 2.3042822174072266, 'acc': 0.1103}\n",
      "after pruning by fisher score : {'loss': 2.30426893119812, 'acc': 0.1102}\n"
     ]
    }
   ],
   "source": [
    "model = SmallCNN(num_classes=10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "orig_metrics = evaluate(model, val_loader, device)\n",
    "\n",
    "scores = collect_fisher_scores(model, calib_loader, device=device)\n",
    "prune_by_scores(model, scores, sparsity=0.5)\n",
    "pruned_metrics = evaluate(model, val_loader, device)\n",
    "\n",
    "# compare original model and pruned model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "nonzero_params = sum((p != 0).sum().item() for p in model.parameters() if p.dim() > 0)\n",
    "print(f\"Total params: {total_params}, Non-zero after pruning: {nonzero_params}\")\n",
    "\n",
    "# ignore accuracy number comparision the dataset is too small. the difference is just random noise.\n",
    "# But we can see with 50% weights pruned, the model size is halved. \n",
    "print(\"before:\", orig_metrics)\n",
    "print(\"after pruning by fisher score :\", pruned_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee607b4c",
   "metadata": {},
   "source": [
    "### 2.2.6 é™„ï¼šFisher Information Matrix\n",
    "#### - æ·±åº¦å­¦ä¹ ä¸­çš„æŸå¤±ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰\n",
    "\n",
    "åˆ†ç±»æ¨¡å‹ä¼˜åŒ–çš„ç›®æ ‡é€šå¸¸æ˜¯æœ€å¤§åŒ–æ­£ç¡®æ ‡ç­¾çš„æ¦‚ç‡ï¼š\n",
    "\n",
    "$ p(y \\mid x; \\theta) $\n",
    "\n",
    "è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆå³äº¤å‰ç†µï¼‰å®šä¹‰ä¸ºï¼š\n",
    "\n",
    "$ L(\\theta) = - \\mathbb{E}_{(x,y)\\sim \\text{data}} \\left[ \\log p(y \\mid x;\\theta) \\right] $\n",
    "\n",
    "å®é™…è®­ç»ƒä¸­ç”¨ mini-batch è¿‘ä¼¼ï¼š\n",
    "\n",
    "$ L(\\theta) \\approx - \\frac{1}{N} \\sum_{i=1}^{N} \\log p(y_i \\mid x_i; \\theta) $\n",
    "\n",
    "---\n",
    "\n",
    "#### - Fisher Information Matrix çš„å®šä¹‰\n",
    "\n",
    "Fisher ä¿¡æ¯çŸ©é˜µå®šä¹‰ä¸º log-likelihood æ¢¯åº¦çš„äºŒé˜¶çŸ©ï¼ˆå¤–ç§¯çš„æœŸæœ›ï¼‰ï¼š\n",
    "\n",
    "$ F(\\theta) = \\mathbb{E}_{x,y} \\left[ \\nabla_\\theta \\log p(y \\mid x; \\theta) \\, \\nabla_\\theta \\log p(y \\mid x; \\theta)^\\top \\right] $\n",
    "\n",
    "å¯¹è§’éƒ¨åˆ†\n",
    "$$F_{ii} = \\mathbb{E} \\left[\\left(\\frac{\\partial}{\\partial\\theta_i}\\log p(y\\mid x;\\theta)\\right)^2\\right]$$\n",
    "\n",
    "å®ƒè¡¡é‡ï¼š\n",
    "\n",
    "**å‚æ•°æ”¹å˜æ—¶ï¼Œæ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¼šå˜åŒ–å¤šå°‘ã€‚**\n",
    "\n",
    "å˜åŒ–è¶Šå¤§ï¼Œå‚æ•°è¶Šé‡è¦ï¼ŒFisher è¶Šå¤§ã€‚æ¢å¥è¯è¯´ï¼ŒFisher æ˜¯ log-likelihood æ¢¯åº¦çš„äºŒé˜¶çŸ© / åæ–¹å·®çŸ©\n",
    "\n",
    "---\n",
    "\n",
    "#### - ä¸ºä»€ä¹ˆ Fisher å¯ä»¥è¿‘ä¼¼ Hessianï¼Ÿ\n",
    "\n",
    "æ·±åº¦å­¦ä¹ æœ€å¸¸ç”¨çš„æŸå¤±æ˜¯è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼š\n",
    "\n",
    "$ L(\\theta) = - \\log p(y \\mid x;\\theta) $\n",
    "äºæ˜¯ï¼š\n",
    "$$\\frac{\\partial L}{\\partial\\theta_i} = -\\frac{\\partial}{\\partial \\theta_i} \\log p(y\\mid x;\\theta)$$\n",
    "\n",
    "å¯¹æ¯”ä¸Šé¢fisher å¯¹è§’éƒ¨åˆ†è®¡ç®—$\\left(\\frac{\\partial}{\\partial\\theta_i}\\log p(y\\mid x;\\theta)\\right)^2$\n",
    "\n",
    "$$F_{ii} = \\mathbb{E} \\left[\\left(\\frac{\\partial}{\\partial\\theta_i}\\log p(y\\mid x;\\theta)\\right)^2\\right]$$\n",
    "å°±å˜æˆï¼š\n",
    "$$F_{ii} = \\mathbb{E} \\left[\\left(\\frac{\\partial L}{\\partial\\theta_i}\\right)^2\\right]$$\n",
    "\n",
    "- Fisher diagonal å¯ä»¥ç›´æ¥ç”¨ loss å¯¹æƒé‡çš„æ¢¯åº¦å¹³æ–¹ä¼°è®¡\n",
    "- æ‰€ä»¥å‰ªæä»£ç é‡Œä¼šçœ‹åˆ° (grad ** 2) è€Œä¸æ˜¯ grad_log_probã€‚\n",
    "\n",
    "åœ¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ç†è®ºä¸­æœ‰ä¸€ä¸ªå…³é”®ç»“è®ºï¼š\n",
    "\n",
    "$ H(\\theta) = \\nabla_\\theta^2 L(\\theta) \\approx F(\\theta) $\n",
    "\n",
    "åŸå› å¦‚ä¸‹ï¼š\n",
    "\n",
    "- åœ¨æœ€ä¼˜ç‚¹ï¼Œæ¢¯åº¦æœŸæœ›ä¸º 0  \n",
    "- å¯¹ log-likelihood çš„äºŒé˜¶å¯¼åœ¨æ•°å­¦ä¸Šä¸æ¢¯åº¦å¤–ç§¯çš„æœŸæœ›ç­‰ä»·  \n",
    "- å› æ­¤ Hessian å’Œ Fisher åœ¨æ”¶æ•›åŒºåŸŸå…·æœ‰ç›¸åŒç»“æ„\n",
    "\n",
    "è¿™ç§°ä¸º **Hessianâ€“Fisher ç­‰ä»·æ€§**ã€‚\n",
    "\n",
    "å› æ­¤ï¼š\n",
    "\n",
    "$ h_{ii} \\approx F_{ii} $\n",
    "\n",
    "---\n",
    "\n",
    "#### - ä¸ºä»€ä¹ˆå‰ªæä¸­ä½¿ç”¨ Fisher diagonalï¼Ÿ\n",
    "\n",
    "Taylor äºŒé˜¶å±•å¼€ç»™å‡ºçš„æƒé‡é‡è¦æ€§ï¼š\n",
    "\n",
    "$ \\Delta L_i^{(2)} \\approx \\frac{1}{2} h_{ii} w_i^2 $\n",
    "\n",
    "è‹¥ Hessian diagonal éš¾è®¡ç®—ï¼Œå¯ç”¨ Fisher diagonal æ›¿ä»£ï¼š\n",
    "\n",
    "$ F_{ii} \\approx \\mathbb{E} \\left[ \\left( \\frac{\\partial L}{\\partial w_i} \\right)^2 \\right] $\n",
    "\n",
    "äºæ˜¯é‡è¦æ€§åˆ†æ•°å˜ä¸ºï¼š\n",
    "\n",
    "$ I_i = F_{ii} \\, w_i^2 $\n",
    "\n",
    "è¿™æ˜¯ç°ä»£å‰ªæï¼ˆå¦‚ Molchanov 2017ï¼‰ä¸è¿ç»­å­¦ä¹ ï¼ˆEWC 2017ï¼‰çš„ç»Ÿä¸€åŸºç¡€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### - å·¥ç¨‹ä¸Šå¦‚ä½•è®¡ç®— Fisher diagonal\n",
    "\n",
    "å¯¹æ ¡å‡†æ•°æ®æ‰§è¡Œå¤šæ¬¡ forward/backwardï¼Œå¹¶ç´¯ç§¯ï¼š\n",
    "\n",
    "$ F_{ii} \\approx \\frac{1}{N} \\sum_{n=1}^{N} g_{i,n}^2 $\n",
    "\n",
    "å…¶ä¸­ $ g_{i,n} $ ä¸ºç¬¬ n ä¸ª batch çš„æ¢¯åº¦ã€‚\n",
    "\n",
    "ç‰¹ç‚¹ï¼š\n",
    "\n",
    "- æ— éœ€äºŒé˜¶æ¢¯åº¦  \n",
    "- æ— éœ€ Hessian-vector product  \n",
    "- ä¸è®­ç»ƒæµç¨‹å…¼å®¹  \n",
    "- å¯åº”ç”¨äº CNN / Transformer / MLP\n",
    "\n",
    "---\n",
    "\n",
    "#### - å·¥ä¸šåº”ç”¨æ€»ç»“\n",
    "\n",
    "- **å‰ªæï¼ˆTaylor-based / Movement pruningï¼‰**ï¼šå‚æ•°æˆ–é€šé“é‡è¦æ€§  \n",
    "- **é‡åŒ–ï¼ˆå¦‚ SmoothQuantï¼‰**ï¼šæ¿€æ´»/æƒé‡é‡åŒ–æ•æ„Ÿåº¦  \n",
    "- **è¿ç»­å­¦ä¹ ï¼ˆEWCï¼‰**ï¼šå‚æ•°é‡è¦æ€§  \n",
    "- **è‡ªç„¶æ¢¯åº¦ä¼˜åŒ–**ï¼šFisher ä½œä¸ºå‚æ•°ç©ºé—´ä¸­çš„ Riemann åº¦é‡  \n",
    "\n",
    "Fisher diagonal æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„â€œè¿‘ä¼¼äºŒé˜¶ä¿¡æ¯â€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6aa655",
   "metadata": {},
   "source": [
    "## 2.3 â€” Global vs Layerwise & LAMPï¼ˆLayer-Adaptive Magnitude Pruningï¼‰\n",
    "\n",
    "### 2.3.1. é—®é¢˜ï¼šæ¯ä¸€å±‚å‰ªå¤šå°‘ï¼Ÿ\n",
    "\n",
    "Magnitude/Taylor ç»™äº†æ¯ä¸ªæƒé‡/é€šé“çš„åˆ†æ•°ï¼Œä½†æ²¡æœ‰å›ç­”ï¼š\n",
    "\n",
    "- æ¯ä¸€å±‚åº”è¯¥å‰ªå¤šå°‘ï¼Ÿ\n",
    "- å‡åŒ€å‰ªï¼Ÿæ·±å±‚å‰ªå¾—å¤šä¸€ç‚¹ï¼Ÿè¿˜æ˜¯æ‰‹è°ƒä¸€å †è¶…å‚ï¼Ÿ\n",
    "\n",
    "**LAMP**ï¼ˆLayer-Adaptive Magnitude-based Pruningï¼‰çš„ç›®æ ‡ï¼š\n",
    "\n",
    "> ç”¨ä¸€ä¸ªç®€å•çš„é‡æ ‡å®šï¼ˆrescalingï¼‰ï¼Œè®© global magnitude æ’åºè‡ªç„¶å¸¦å‡ºåˆç†çš„ã€Œå±‚è‡ªé€‚åº” sparsityã€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.2. LAMP æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "- æŠŠæŸä¸€å±‚çš„æƒé‡ $W$ å±•å¹³æˆä¸€ç»´ï¼Œå¹¶æŒ‰ç»å¯¹å€¼ä»å°åˆ°å¤§æ’åºï¼š\n",
    "$$|W[1]| \\le |W[2]| \\le \\dots \\le |W[n]|$$\n",
    "- å¯¹æ’åºä¹‹åçš„ç¬¬ $u$ ä¸ªå…ƒç´ ï¼Œå®šä¹‰\n",
    "$$\\text{score}(u; W) = \\frac{W[u]^2}{\\sum_{v \\ge u} W[v]^2}$$\n",
    "\n",
    "ä¹Ÿå°±æ˜¯**â€œè¿™ä¸ªæƒé‡çš„èƒ½é‡ / å‰©ä¸‹æ‰€æœ‰è¿˜æ²¡è¢«å‰ªæ‰çš„èƒ½é‡â€**\n",
    "\n",
    "ç›´è§‰ï¼š\n",
    "\n",
    "- æœ¬æ¥å°±ã€Œèƒ½é‡å¾ˆé›†ä¸­ã€å†—ä½™å°‘ã€çš„å±‚ï¼Œå¦‚æœå†å‰ªï¼Œä¼š hurt å¾—æ›´å¤šï¼Œå› æ­¤å…¶åˆ†æ•°è¢«æ”¾å¤§ã€‚\n",
    "- å†—ä½™å¾ˆå¤šçš„å±‚åˆ†æ•°è¢«å‹ç¼©ï¼Œå¯ä»¥å¤šå‰ªä¸€äº›ã€‚\n",
    "\n",
    "ç„¶ååšçš„äº‹å¾ˆç®€å•ï¼š\n",
    "\n",
    "- åœ¨æ¯ä¸€å±‚ç®—å‡ºè¿™ä¸ª scoreï¼›\n",
    "\n",
    "- æŠŠæ‰€æœ‰å±‚çš„ score æ‰”åœ¨ä¸€èµ·ï¼Œåšä¸€æ¬¡ global æ’åº + å…¨å±€é˜ˆå€¼ï¼›\n",
    "\n",
    "- é˜ˆå€¼ä»¥ä¸‹çš„éƒ½å‰ªæ‰ï¼Œå°±è‡ªåŠ¨ç»™å‡ºäº†æ¯å±‚è¯¥å‰ªå¤šå°‘ï¼ˆlayer-adaptiveï¼‰ã€‚\n",
    "\n",
    "ç»“æœï¼šç›´æ¥å¯¹ $s_i$ åš global æ’åºå‰ªæï¼Œå°±ä¼šè‡ªåŠ¨ç»™å‡ºã€Œå±‚è‡ªé€‚åº”ã€çš„é…é¢ã€‚\n",
    "\n",
    "### 2.3.3 ç®—æ³•\n",
    "#### 2.3.3.1 å•å±‚æƒ…å†µ\n",
    "å¯¹å•å±‚å…¨è¿æ¥ $y = W x$ï¼Œå¦‚æœæˆ‘ä»¬è¦åœ¨å›ºå®š sparsity $\\kappa$ ä¸‹æ‰¾ä¸€ä¸ª mask $M$ï¼ˆ0/1 çŸ©é˜µï¼‰å»å‰ªæï¼š\n",
    "\n",
    "å‰ªå®Œä¹‹åæ˜¯ $W_f = M \\odot W$ã€‚\n",
    "\n",
    "æƒ³è¦æœ€å°åŒ–æœ€åæƒ…å†µä¸‹çš„è¾“å‡º $â„“_2$ è¯¯å·®ï¼š\n",
    "$$\\min_{M, \\|M\\|_0 \\le \\kappa} \\ \\sup_{\\|x\\|_2 \\le 1} \\|W x - (M\\odot W) x\\|_2$$\n",
    "ç”¨è°±èŒƒæ•°çš„å®šä¹‰ + ä¸Šç•Œï¼Œå¯ä»¥æŠŠå®ƒ relax åˆ° Frobenius norm\n",
    "$$\\approx \\min_{M,\\|M\\|_0 \\le \\kappa} \\|W - M\\odot W\\|_F$$\n",
    "\n",
    "è¿™ä¸€æ­¥çš„ç»“è®ºï¼š\n",
    "åœ¨å•å±‚ä¸Šï¼Œè¦åœ¨ Frobenius å¤±çœŸæœ€å°çš„å‰æä¸‹é€‰ $\\kappa$ ä¸ªä¿ç•™çš„ weightï¼Œæ˜¾ç„¶å°±æ˜¯ã€Œä¿ç•™ $|W_{ij}|$ æœ€å¤§çš„é‚£å‡ ä¸ªã€â€”â€”ä¹Ÿå°±æ˜¯ layerwise magnitude pruning æœ¬èº«ã€‚https://arxiv.org/pdf/2010.07611\n",
    "\n",
    "æ‰€ä»¥ MPï¼ˆmagnitude pruningï¼‰å¯ä»¥è¢«ç†è§£æˆï¼š\n",
    "\n",
    "å¯¹æ¯ä¸€å±‚ï¼Œæ±‚è§£â€œåœ¨ç»™å®šå±‚å†… sparsity ä¸‹ï¼Œä½¿æƒé‡çš„ $â„“_2$ å¤±çœŸæœ€å°â€çš„æœ€ä¼˜æ–¹æ¡ˆ\n",
    "\n",
    "#### 2.3.3.2 æ•´ä¸ªç½‘ç»œï¼šæ¨¡å‹çº§è¾“å‡ºå¤±çœŸä¸Šç•Œ\n",
    "çœŸæ­£æƒ³è¦çš„æ˜¯ï¼šåœ¨ç»™å®šå…¨å±€ sparsity ä¸‹ï¼Œæœ€å°åŒ–æ¨¡å‹è¾“å‡ºçš„ $â„“_2$ å¤±çœŸï¼š\n",
    "$$\\min_{\\{M^{(i)}\\}} \\sup_{\\|x\\|_2\\le 1} \\big\\| f(x; W^{(1:d)}) - f(x; W_f^{(1:d)})\\big\\|_2$$\n",
    "å…¶ä¸­ $W^{(i)}$ æ˜¯ç¬¬ $i$ å±‚æƒé‡ï¼Œ$W_f^{(i)} = M^{(i)}\\odot W^{(i)}$ æ˜¯å‰ªæ‰åçš„æƒé‡.\n",
    "\n",
    "ç›´æ¥è§£è¿™ä¸ªå¤ªéš¾ï¼Œäºæ˜¯ä»–ä»¬æäº†ä¸€ä¸ªè´ªå¿ƒ + ä¸Šç•Œ:\n",
    "\n",
    "- å‡è®¾æ¯æ¬¡åªå‰ªæ‰ä¸€ä¸ªæƒé‡ï¼›\n",
    "\n",
    "- æ¨å‡ºä¸€ä¸ªæ¨¡å‹è¾“å‡ºè¯¯å·®çš„ä¸Šç•Œï¼ˆè¿™é‡Œç”¨äº†å¾ˆå¤šä¸ç­‰å¼å’Œå±‚é—´èŒƒæ•°çš„ä¹˜ç§¯ï¼‰ï¼š\n",
    "$$\\begin{array}{c} \\sup_{\\|x\\|_2\\le 1} \n",
    "\\big\\| f(x; W^{(1:d)}) - f(x; W^{(1:i-1)}, W_f^{(i)}, W^{(i+1:d)})\\big\\|_2\n",
    "\\\\\n",
    "\\le\n",
    "\\frac{\\|W^{(i)} - W_f^{(i)}\\|_F}{\\|W^{(i)}\\|_F} \\cdot \\prod_{j=1}^{d}\\|W^{(j)}\\|_F \\end{array}$$\n",
    "å³è¾¹é™¤äº† $|W^{(i)} - W_f^{(i)}|_F / |W^{(i)}|_F$ ä¹‹å¤–ï¼Œå…¶ä»–éƒ½æ˜¯å¸¸æ•°ï¼Œå®ƒä»¬å¯¹ã€Œå‰ªè°ã€è¿™ä»¶äº‹ä¸äº§ç”Ÿæ’åºå½±å“ã€‚\n",
    "\n",
    "äºæ˜¯é—®é¢˜å˜æˆï¼š\n",
    "\n",
    "æ¯å‰ªæ‰ä¸€ä¸ª weightï¼Œå¸Œæœ›è®©ã€Œæœ¬å±‚ç›¸å¯¹å¤±çœŸã€ $|W^{(i)} - W_f^{(i)}|_F / |W^{(i)}|_F$ çš„å¢é‡å°½é‡å°\n",
    "\n",
    "#### 2.3.3.3 LAMP score çš„å½¢å¼\n",
    "ç”±æ­¤æ¨å‡º LAMP score çš„å½¢å¼\n",
    "\n",
    "æŠŠä¸€å±‚çš„æƒé‡æŒ‰ä»å°åˆ°å¤§æ’åºåï¼Œæƒ³è±¡ä¸€ä¸ªä»å°åˆ°å¤§é€ä¸ªå‰ªæ‰çš„è¿‡ç¨‹ï¼š\n",
    "\n",
    "- å½“å‰åœ¨ä½ç½® $u$ ä¸Šçš„ weight æ˜¯ $W[u]$ï¼›\n",
    "\n",
    "- å‡è®¾ä¹‹å‰ $1,\\dots,u-1$ éƒ½å·²ç»è¢«å‰ªæ‰äº†ï¼›\n",
    "\n",
    "- å‰©ä½™çš„èƒ½é‡æ˜¯ $\\sum_{v\\ge u} W[v]^2$ï¼›\n",
    "\n",
    "- è¿™æ—¶å€™å¦‚æœå†å‰ªæ‰ $W[u]$ï¼Œå…¶å¸¦æ¥çš„æœ¬å±‚å¤±çœŸå¢é‡å’Œ $W[u]^2$ æˆæ­£æ¯”ï¼›\n",
    "\n",
    "- ä½†ã€Œç›¸å¯¹åç¨‹åº¦ã€å–å†³äºè¿™ä¸ªå¢é‡å å½“å‰å‰©ä½™é‡çš„æ¯”ä¾‹ã€‚\n",
    "\n",
    "æ‰€ä»¥å°±å¾—åˆ°ï¼š\n",
    "$$\\text{score}(u; W) = \\frac{W[u]^2}{\\sum_{v\\ge u} W[v]^2}$$\n",
    "è§£é‡Šï¼š\n",
    "\n",
    "- åˆ†å­ï¼šå½“å‰è¿™æ¡è¿æ¥çš„èƒ½é‡ï¼›\n",
    "\n",
    "- åˆ†æ¯ï¼šå‰ªæ‰å®ƒä¹‹å‰æœ¬å±‚å‰©ä½™è¿˜æ´»ç€çš„æ€»èƒ½é‡ï¼ˆæŠŠä¹‹å‰æ›´å°çš„éƒ½è§†ä¸ºå·²ç»å‰ªæ‰äº†ï¼‰ï¼›\n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "\n",
    "- å¦‚æœä½ å†å‰ªè¿™ä¸€æ¡ï¼Œä¼šæŠŠå‰©ä½™çš„èƒ½é‡åƒæ‰å¤šå°‘æ¯”ä¾‹ï¼Ÿ\n",
    "\n",
    "- é«˜åˆ†çš„æƒé‡ = ä¸€æ—¦å‰ªæ‰ï¼Œæœ¬å±‚ã€Œæœ€åä¸€ç‚¹è¡€ã€ä¼šæ‰å¾ˆå¤š â†’ æ›´è¯¥è¢«ä¿ç•™ï¼›\n",
    "- ä½åˆ†çš„æƒé‡ = æ‰€å¤„å±‚è¿˜å¾ˆå¯Œè£• / å†—ä½™å¤šï¼Œå¯ä»¥å¤šå‰ªã€‚\n",
    "\n",
    "ç„¶ååœ¨å…¨ç½‘ç»œä¸Šï¼Œæˆ‘ä»¬åªéœ€è¦ï¼š\n",
    "\n",
    "- å¯¹æ¯å±‚éƒ½ç®—è¿™ä¸ª scoreï¼›\n",
    "\n",
    "- ç”¨ä¸€ä¸ªå…¨å±€ threshold ç»Ÿä¸€å‰ªæï¼›\n",
    "\n",
    "- ç»“æœå°±æ˜¯ï¼šè‡ªç„¶å‡ºç°ã€ŒæŸäº›å±‚è¢«å¤šå‰ªï¼ŒæŸäº›å±‚è¢«å°‘å‰ªã€ï¼Œè€Œä¸éœ€è¦æ‰‹è°ƒå„å±‚ sparsityã€‚\n",
    "\n",
    "#### pytorch LAMP score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3afbd6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def lamp_LAMP_scores(w: torch.Tensor):\n",
    "    \"\"\"\n",
    "    æ ¹æ®åŸå§‹è®ºæ–‡çš„æ•°å­¦å®šä¹‰è®¡ç®— LAMP åˆ†æ•°ã€‚\n",
    "    Score(W_u) = W_u^2 / Sum(W_v^2 for all v >= u)\n",
    "    è¿”å›åŒå½¢çŠ¶çš„ score å¼ é‡ã€‚\n",
    "    \"\"\"\n",
    "    # 1. å±•å¹³å¼ é‡\n",
    "    flat = w.view(-1)\n",
    "\n",
    "    # 2. æŒ‰ç»å¯¹å€¼ä»å°åˆ°å¤§æ’åº\n",
    "    abs_w, idx = flat.abs().sort()\n",
    "\n",
    "    # 3. è®¡ç®—æ’åºåæƒé‡çš„å¹³æ–¹\n",
    "    sq = abs_w.pow(2)\n",
    "\n",
    "    # 4. è®¡ç®—å¹³æ–¹åçš„å‰ç¼€å’Œ (ä»å°åˆ°å¤§ç´¯åŠ )\n",
    "    cumsum = torch.cumsum(sq, dim=0)\n",
    "\n",
    "    # 5. è®¡ç®—å‰©ä½™èƒ½é‡ï¼ˆæ€»èƒ½é‡ - å‰ç¼€å’Œï¼‰\n",
    "    total_energy = sq.sum()\n",
    "    # remaining_energy_sum[i] ä»£è¡¨æ’åœ¨ç¬¬ i ä½ï¼ˆå«ï¼‰ä¹‹åæ‰€æœ‰æƒé‡çš„èƒ½é‡å’Œ\n",
    "    remaining_energy_sum = total_energy - cumsum + sq # åŠ ä¸Š sq[i] å› ä¸º cumsum[i] å‡å»äº†å®ƒ\n",
    "\n",
    "    # 6. è®¡ç®—åˆ†æ•°ï¼šå½“å‰æƒé‡çš„å¹³æ–¹ é™¤ä»¥ å‰©ä½™èƒ½é‡æ€»å’Œ\n",
    "    # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œæ·»åŠ ä¸€ä¸ªå°çš„ epsilon\n",
    "    epsilon = 1e-8\n",
    "    # åˆ†å­ï¼šå½“å‰æƒé‡å¹³æ–¹ (sq)ï¼Œåˆ†æ¯ï¼šå‰©ä½™èƒ½é‡å’Œ (remaining_energy_sum)\n",
    "    scores_sorted = sq / (remaining_energy_sum + epsilon)\n",
    "\n",
    "    # 7. è¿˜åŸåˆ°åŸå§‹å¼ é‡çš„å½¢çŠ¶å’Œä½ç½®\n",
    "    inv_idx = torch.empty_like(idx)\n",
    "    inv_idx[idx] = torch.arange(len(idx), device=w.device)\n",
    "    flat_scores = scores_sorted[inv_idx]\n",
    "    \n",
    "    return flat_scores.view_as(w)\n",
    "\n",
    "\n",
    "def compute_lamp_scores(model: nn.Module):\n",
    "    lamp_scores = {}\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            lamp_scores[name] = lamp_scores_for_tensor(p.data)\n",
    "    return lamp_scores\n",
    "\n",
    "\n",
    "# ä½¿ç”¨æ–¹å¼ï¼š\n",
    "# 1. scores = compute_lamp_scores(model)\n",
    "# 2. prune_by_scores(model, scores, sparsity=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28444ede",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3.4 æ–‡çŒ®\n",
    "Lee et al., *Layer-adaptive Sparsity for the Magnitude-based Pruning*, NeurIPS 2020.  \n",
    "<https://arxiv.org/abs/2010.07611>\n",
    "\n",
    "### 2.3.5 LAMP çš„å­¦æœ¯åœ°ä½ä¸å·¥ä¸šè½åœ°æƒ…å†µ\n",
    "åœ¨ **2021 å¹´ä¹‹åçš„å‰ªææ–‡çŒ®ä¸­**ï¼ŒLAMP ç»å¸¸è¢«å½“ä½œ  \n",
    "- ã€Œmagnitude-based global pruning ä¸‹çš„ layer-adaptive sparsity baselineã€ï¼Œ  \n",
    "- ç”¨æ¥å’Œä¿¡æ¯è®ºã€æ¦‚ç‡å›¾ã€é—¨æ§æœºåˆ¶ã€æ‹“æ‰‘æ–¹æ³•ç­‰æ›´å¤æ‚çš„å‰ªæç­–ç•¥åšå¯¹æ¯”\n",
    "\n",
    "å¯¹æ¯”å½“å‰ï¼ˆ2022â€“2025ï¼‰ä¸»æµå‹ç¼© / éƒ¨ç½²æ–¹å‘\n",
    "\n",
    "å¼•ç”¨æœ€è¿‘çš„æ¨¡å‹å‹ç¼©ç»¼è¿°ä¸å‰ªæè®ºæ–‡ï¼š\n",
    "\n",
    "- **A Survey on Model Compression for Deep Neural Networks (2023)**  \n",
    "  ğŸ”— https://arxiv.org/abs/2308.07610  \n",
    "\n",
    "- **Whatâ€™s Left? Pruning LLMs (2023)**  \n",
    "  ğŸ”— https://arxiv.org/abs/2302.10403  \n",
    "\n",
    "- **SparseGPTï¼ˆLLM å‰ªæä¸»æµæ–¹æ³•ï¼‰**  \n",
    "  ğŸ”— https://arxiv.org/abs/2301.00774  \n",
    "\n",
    "- **Wanda: Weights Are Not Equalï¼ˆLLM å‰ªæå¦ä¸€ä¸ªä¸»æµï¼‰**  \n",
    "  ğŸ”— https://arxiv.org/abs/2306.11695  \n",
    "\n",
    "è¿™äº›å·¥ä½œæœ‰å¦‚ä¸‹å…±è¯†ï¼š\n",
    "\n",
    "---\n",
    "\n",
    " âœ” Quantization æ˜¯å·¥ä¸šè½åœ°æ ¸å¿ƒï¼ˆINT8 / FP8 / BF16ï¼‰\n",
    "\n",
    "ç›¸æ¯”é‡åº¦å‰ªæï¼Œé‡åŒ–æ›´ç¨³å®šã€æ”¶ç›Šæ›´å¤§ï¼š\n",
    "\n",
    "- INT8ï¼šæ˜¾è‘—å‡å¸¦å®½  \n",
    "- FP8ï¼šNVIDIA Hopper æ¨åŠ¨æˆä¸ºæ–°æ¨ç†æ ‡å‡†  \n",
    "- BF16ï¼šè½¦ç«¯/äº‘ç«¯æœ€å¸¸è§çš„ mixed precision format  \n",
    "\n",
    "---\n",
    "\n",
    "âœ” çœŸè¦å‰ªæï¼Œå„æ¨¡å‹çš„ä¸»æµæ–¹æ³•å¦‚ä¸‹ï¼š\n",
    "\n",
    " **â‘  CNN / Visionï¼šStructured Pruning + Distillation**\n",
    "\n",
    "- Channel / Filter / Block å‰ªæ  \n",
    "- è¾…ä»¥è’¸é¦æ¥ä¿æŒç²¾åº¦  \n",
    "\n",
    "ç¤ºä¾‹ï¼š  \n",
    "- **Network Slimming**  \n",
    "  ğŸ”— https://arxiv.org/abs/1708.06519  \n",
    "\n",
    "- **DMCP**  \n",
    "  ğŸ”— https://arxiv.org/abs/2005.03354  \n",
    "\n",
    "---\n",
    "\n",
    "**â‘¡ Transformer / LLMï¼šMovement Pruning / SparseGPT / Wanda**\n",
    "\n",
    "Transformer å¯¹ unstructured magnitude pruningï¼ˆåŒ…æ‹¬å…¨å±€ magnitude + LAMP æ‰“åˆ†ï¼‰éå¸¸è„†å¼±ï¼Œå› æ­¤ä¸šç•Œè½¬å‘æ›´ç¨³å®šçš„å‰ªææ–¹å¼ï¼š\n",
    "\n",
    "- **Movement Pruningï¼ˆGoogle, 2020ï¼‰**  \n",
    "  ğŸ”— https://arxiv.org/abs/2005.07683  \n",
    "\n",
    "- **SparseGPTï¼ˆ2023ï¼‰**  \n",
    "  ğŸ”— https://arxiv.org/abs/2301.00774  \n",
    "\n",
    "- **Wandaï¼ˆ2023ï¼‰**  \n",
    "  ğŸ”— https://arxiv.org/abs/2306.11695  \n",
    "\n",
    "è¿™äº›æ–¹æ³•åœ¨ 70â€“90% sparsity ä¸‹çš„ç¨³å®šæ€§è¿œä¼˜äº magnitude-based å‰ªæã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**â‘¢ N:M ç¨€ç–ï¼ˆ2:4ï¼‰æ˜¯çœŸæ­£èƒ½åŠ é€Ÿçš„ç¨€ç–æ¨¡å¼**\n",
    "\n",
    "- æ”¯æŒ NVIDIA Ampere/Hopper çš„ sparse tensor core  \n",
    "- æ˜¯å”¯ä¸€çœŸæ­£èƒ½å¸¦æ¥ååæå‡çš„ç¨€ç–æ–¹å¼  \n",
    "- å¹¿æ³›ç”¨äºå·¥ä¸šæ¨ç†å¼•æ“  \n",
    "- ä¹Ÿå¯ä»¥ç»“åˆ LAMP åš **per-layer group-level sparsity allocation**\n",
    "\n",
    "ç›¸å…³è®ºæ–‡ï¼š  \n",
    "**Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch**  \n",
    "ğŸ”— https://arxiv.org/abs/2111.10988  \n",
    "\n",
    "---\n",
    "ç»¼åˆç»“è®ºï¼ˆå­¦æœ¯ vs å·¥ä¸šï¼‰\n",
    "\n",
    "\n",
    " ğŸ­ å·¥ä¸šè½åœ°è§’åº¦  \n",
    "LAMP **ä¸æ˜¯å·¥ä¸šç•Œçš„æœ€ç»ˆå‰ªææ–¹æ¡ˆ**ï¼Œè€Œæ˜¯ï¼š\n",
    "\n",
    "- ä¸€ç§ **layer-wise sparsity allocator**  \n",
    "- å¸¸ç”¨äºåˆ†æå“ªäº›å±‚å†—ä½™å¤§ã€å“ªäº›å±‚ä¸èƒ½ç¢°  \n",
    "- å¯èƒ½ä¸ N:Mã€structured pruningã€QATã€distillation ç­‰ç»“åˆä½¿ç”¨  \n",
    "\n",
    "çœŸæ­£ä¸Šçº¿çš„ pipeline ä¸€èˆ¬æ˜¯ï¼š\n",
    "\n",
    "1. **Structured pruningï¼ˆchannel/head/blockï¼‰**  \n",
    "2. **N:M ç¨€ç–ï¼ˆ2:4ï¼‰**  \n",
    "3. **é‡åŒ–ï¼ˆINT8, FP8, BF16ï¼‰**  \n",
    "4. **è’¸é¦æ¢å¤ç²¾åº¦**  \n",
    "5. é’ˆå¯¹ç¡¬ä»¶çš„ **è‡ªå®šä¹‰ kernel / compiler pass**\n",
    "\n",
    "è€Œä¸æ˜¯ï¼š\n",
    "\n",
    "> ã€Œä»…é  LAMP åš 90% unstructured pruning åç›´æ¥éƒ¨ç½²ã€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**ä¸€å¥è¯æ€»ç»“ï¼š**\n",
    "\n",
    "> **LAMP = å¼ºå¤§çš„å±‚è‡ªé€‚åº” sparsity åˆ†é…å™¨ï¼ˆacademic SOTA baselineï¼‰ï¼Œä½†ä¸æ˜¯å·¥ä¸šç«¯æœ€ç»ˆçš„é«˜æ€§èƒ½å‰ªææ–¹æ¡ˆã€‚**  \n",
    "> å·¥ä¸šç•Œæ›´ä¾èµ– structured/N:M sparsity + quantization + hardware-specific kernelsã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972044f",
   "metadata": {},
   "source": [
    "## 2.4 Structured Pruningï¼ˆé€šé“ / Filter å‰ªæï¼‰\n",
    "### 2.4.1. éç»“æ„åŒ– vs ç»“æ„åŒ–\n",
    "\n",
    "- **éç»“æ„åŒ–å‰ªæ**ï¼šå•ä¸ªæƒé‡å˜ 0ï¼Œå‚æ•°çŸ©é˜µå˜ç¨€ç–ï¼Œä½†å½¢çŠ¶ä¸å˜ã€‚\n",
    "- **ç»“æ„åŒ–å‰ªæ**ï¼šç›´æ¥åˆ é™¤æ•´ä¸ªé€šé“ / filter / head / blockï¼Œæ¨¡å‹å½¢çŠ¶å‘ç”Ÿæ”¹å˜ï¼š\n",
    "  - å·ç§¯å±‚ï¼šåˆ é™¤è¾“å‡ºé€šé“ï¼ˆæˆ–è¾“å…¥é€šé“ï¼‰ã€‚\n",
    "  - å…¨è¿æ¥å±‚ï¼šåˆ é™¤ä¸€æ•´åˆ— / ä¸€æ•´è¡Œã€‚\n",
    "  - Transformerï¼šåˆ é™¤æ•´ä¸ª attention head / FFN hidden blockã€‚\n",
    "\n",
    "ç»“æ„åŒ–å‰ªæçš„ä¼˜ç‚¹ï¼š\n",
    "\n",
    "- FLOPs çœŸæ­£ä¸‹é™ï¼Œå®¹æ˜“è¢« TensorRT / ONNX Runtime / TVM ç­‰ç¼–è¯‘å™¨åˆ©ç”¨ã€‚\n",
    "- ç¡¬ä»¶å‹å¥½ï¼ˆä¸éœ€è¦ç‰¹æ®Šç¨€ç– kernel ä¹Ÿèƒ½åŠ é€Ÿï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4.2 é€šé“ / Filter å‰ªæçš„æ•°å­¦è§†è§’\n",
    "\n",
    "ä»¥ä¸€ä¸ªå·ç§¯å±‚ä¸ºä¾‹ï¼Œå¿½ç•¥ biasï¼š\n",
    "\n",
    "- è¾“å…¥ï¼š$X\\in\\mathbb{R}^{N\\times C_{\\text{in}}\\times H\\times W}$\n",
    "- å·ç§¯æ ¸ï¼š$W\\in\\mathbb{R}^{C_{\\text{out}}\\times C_{\\text{in}}\\times k_h\\times k_w}$\n",
    "- è¾“å‡ºï¼š$Y\\in\\mathbb{R}^{N\\times C_{\\text{out}}\\times H'\\times W'}$\n",
    "\n",
    "åˆ é™¤è¾“å‡ºé€šé“ $c$ï¼š\n",
    "\n",
    "- ç­‰ä»·äºå°† $W_{c,:,:,:}$ ç½®é›¶ï¼Œå¹¶åœ¨å®ç°ä¸ŠçœŸæ­£åˆ é™¤è¯¥é€šé“ã€‚\n",
    "- å…¶å½±å“å¯ä»¥ç”¨æŸç§ã€Œé€šé“é‡è¦æ€§åˆ†æ•°ã€æ¥è¡¡é‡ï¼Œä¾‹å¦‚ï¼š\n",
    "  $$\n",
    "  I_c = \\|W_c\\|_2 \\quad\\text{æˆ–}\\quad I_c = |\\gamma_c|\n",
    "  $$\n",
    "  å…¶ä¸­ $\\gamma_c$ æ˜¯è¯¥é€šé“å¯¹åº”çš„ BatchNorm ç¼©æ”¾ç³»æ•°ï¼ˆNetwork Slimmingï¼‰ã€‚\n",
    "\n",
    "### 2.4.3 python ä¾‹å­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5057ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "def structured_channel_prune(conv_layer: nn.Conv2d, amount: float = 0.3):\n",
    "    \"\"\"\n",
    "    å¯¹å•ä¸ª Conv2d å±‚è¿›è¡Œç»“æ„åŒ–å‰ªæï¼Œåœ¨ dim=0 (è¾“å‡ºé€šé“) ç»´åº¦ä¸Šåˆ æ‰ L2 èŒƒæ•°æœ€å°çš„ amount æ¯”ä¾‹é€šé“.\n",
    "    \"\"\"\n",
    "    prune.ln_structured(conv_layer, name='weight', amount=amount, n=2, dim=0)\n",
    "    # å°† mask åº”ç”¨ä¸ºæ°¸ä¹…ï¼ˆåˆ é™¤ pruning ç›¸å…³ bufferï¼‰\n",
    "    prune.remove(conv_layer, 'weight')\n",
    "\n",
    "\n",
    "def structured_prune_model(model: nn.Module, amount: float = 0.3):\n",
    "    \"\"\"å¯¹æ¨¡å‹ä¸­æ‰€æœ‰ Conv2d å±‚è¿›è¡Œç»Ÿä¸€ç»“æ„åŒ–å‰ªæ.\"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            structured_channel_prune(m, amount=amount)\n",
    "\n",
    "\n",
    "# ä½¿ç”¨æ–¹å¼ï¼š\n",
    "# cnn_model = ...\n",
    "# structured_prune_model(cnn_model, amount=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe255a7",
   "metadata": {},
   "source": [
    "> æ›´é«˜çº§ç”¨æ³•ï¼š\n",
    ">\n",
    "> - ç”¨ BN çš„ $\\gamma_c$ æˆ– Taylor åˆ†æ•°æ¥å¯¹æ¯ä¸ªé€šé“æ‰“åˆ†ï¼Œå†³å®šåˆ å“ªäº›é€šé“ï¼Œè€Œä¸æ˜¯å¯¹æ¯å±‚ç”¨å›ºå®š `amount`ã€‚\n",
    "> - å¯¹ Transformerï¼Œå¯ä»¥å¯¹ attention head æˆ– FFN hidden ç»´åº¦åšç±»ä¼¼çš„ã€Œblock-levelã€å‰ªæã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4.4 æ–‡çŒ®\n",
    "\n",
    "- Liu et al., **Learning Efficient Convolutional Networks through Network Slimming**, ICCV 2017.  \n",
    "  <https://arxiv.org/abs/1708.06519>\n",
    "- Molchanov et al. ç³»åˆ—ï¼ˆTaylor-based structured pruningï¼‰ï¼š  \n",
    "  <https://arxiv.org/abs/1611.06440>  \n",
    "  <https://arxiv.org/abs/1906.10771>\n",
    "\n",
    "PyTorch å®˜æ–¹å‰ªææ–‡æ¡£ï¼š  \n",
    "<https://pytorch.org/docs/stable/nn.utils.prune.html>\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4.5 å·¥ä¸šè½åœ°å¤‡æ³¨\n",
    "\n",
    "- å¯¹ CNN backbone åšç»“æ„åŒ–å‰ªæå¯ä»¥ç›´æ¥å¸¦æ¥ GFLOPs çš„ä¸‹é™ï¼Œåœ¨ TensorRT / ONNX / TVM ä¸Šå¸¸å¸¸èƒ½çœ‹åˆ° 1.5â€“3Ã— çš„ latency æ”¹å–„ï¼ˆè§† sparsity å’Œ kernel å®ç°è€Œå®šï¼‰ã€‚\n",
    "- å¯¹ Transformerï¼Œå¯ä»¥ï¼š\n",
    "  - å‰ªæ‰éƒ¨åˆ† attention headsï¼›\n",
    "  - å‡å° FFN hidden sizeï¼›\n",
    "  - åˆ é™¤æŸäº›å±‚ï¼ˆlayer dropping / layer dropping with distillationï¼‰ã€‚\n",
    "- å¯¹è½¦ç«¯éƒ¨ç½²æ¥è¯´ï¼Œç»“æ„åŒ–å‰ªææ˜¯æ¯”éç»“æ„åŒ–æ›´ç›´æ¥ã€å¯æ§çš„é™ FLOPs æ‰‹æ®µã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1dd55",
   "metadata": {},
   "source": [
    "## 2.5 N:M / åŠç»“æ„åŒ–ç¨€ç–ï¼ˆä»¥ 2:4 Sparsity ä¸ºä¾‹ï¼‰\n",
    "\n",
    "### 2.5.1 çº¦æŸå½¢å¼\n",
    "\n",
    "ä»¥ NVIDIA Ampere çš„ **2:4 sparsity** ä¸ºä¾‹ï¼š\n",
    "\n",
    "- åœ¨æ¯ 4 ä¸ªè¿ç»­æƒé‡ï¼ˆé€šå¸¸æ²¿æŸä¸ªçŸ©é˜µç»´åº¦ï¼‰ä¸­ï¼Œ**æœ€å¤š 2 ä¸ªéé›¶**ã€‚\n",
    "- ç­‰ä»·æè¿°ï¼šåœ¨æ¯ä¸ªé•¿åº¦ä¸º 4 çš„ block ä¸­ï¼Œåªå…è®¸ 2 ä¸ªä½ç½®æ˜¯éé›¶ã€‚\n",
    "\n",
    "\n",
    "è®°æƒé‡çŸ©é˜µ $W\\in\\mathbb{R}^{M\\times N}$ï¼Œåœ¨æ¯ä¸€è¡Œçš„ block $b\\in\\mathbb{R}^4$ ä¸Šçº¦æŸï¼š\n",
    "\n",
    "$$\n",
    "\\|b\\|_0 \\le 2\n",
    "$$\n",
    "\n",
    "æ•´ä¸ªå‰ªæé—®é¢˜å¯ä»¥å†™æˆï¼š\n",
    "\n",
    "$$\n",
    "\\min_{W} L(W) \\quad \\text{s.t.}\\; \\forall b,\\; \\|b\\|_0 \\le 2\n",
    "$$\n",
    "\n",
    "#### 2.5.1.1 block  æ˜¯ä» ğ‘Š é‡Œâ€œåˆ‡ç‰‡â€çš„ä¸€ä¸ª 4 å…ƒç»„\n",
    "è®¾$W\\in\\mathbb{R}^{M\\times N}$é€‰å®šä¸€ä¸ªæ–¹å‘ï¼ˆæœ€å¸¸è§ï¼šæ²¿åˆ—æ–¹å‘ï¼Œä¹Ÿå°±æ˜¯å›ºå®šè¡Œã€å–è¿ç»­ 4 ä¸ªåˆ—å…ƒç´ ï¼‰ï¼š\n",
    "- å–ç¬¬ ğ‘– è¡Œï¼Œä»ç¬¬ 4ğ‘˜ åˆ—å¼€å§‹çš„è¿ç»­ 4 ä¸ªå…ƒç´ ï¼š\n",
    "$$b_{i,k} = [W_{i,4k},\\; W_{i,4k+1},\\; W_{i,4k+2},\\; W_{i,4k+3}] \\in \\mathbb{R}^4$$\n",
    "- ä½ ä¹Ÿå¯ä»¥æ²¿è¡Œæ–¹å‘åˆ‡ï¼Œæˆ–è€…åœ¨å·ç§¯é‡Œæ²¿ç‰¹å®šå±•å¹³åçš„ç»´åº¦åˆ‡ã€‚å…³é”®æ˜¯ï¼šâ€œè¿ç»­çš„ 4 ä¸ªâ€è¦è·Ÿåº•å±‚åº“/ç¡¬ä»¶çš„æ ¼å¼çº¦å®šä¸€è‡´ï¼Œå¦åˆ™å°±ç®—ä½ æ»¡è¶³ 2:4ï¼Œä¹Ÿä¸ä¼šè§¦å‘ç¡¬ä»¶åŠ é€Ÿã€‚[nvidia developer](https://developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt)\n",
    "\n",
    "#### $\\|b\\|_0$ èŒƒæ•°\n",
    "$\\ell_0$â€œèŒƒæ•°â€å…¶å®ä¸æ˜¯ä¸¥æ ¼æ„ä¹‰çš„èŒƒæ•°ï¼Œå®ƒå®šä¹‰ä¸ºï¼š\n",
    "$$\\|b\\|_0 = \\#\\{j \\in \\{1,2,3,4\\} \\mid b_j \\neq 0\\}$$\n",
    "ä¹Ÿå°±æ˜¯ï¼šb é‡Œæœ‰å‡ ä¸ªå…ƒç´ ä¸ä¸º 0ã€‚\n",
    "\n",
    "#### ä¸ºä»€ä¹ˆè¿™å« â€œ2:4 sparsityâ€ï¼šå®ƒæ˜¯ä¸€ç§â€œåŠç»“æ„åŒ–ï¼ˆsemi-structuredï¼‰â€ç¨€ç–\n",
    "- â€œéç»“æ„åŒ–ç¨€ç–â€ï¼šå“ªé‡Œéƒ½å¯ä»¥ä¸º 0ï¼Œmask å¾ˆä¹±ï¼Œç¡¬ä»¶å¾ˆéš¾é«˜æ•ˆè·³è¿‡ã€‚\n",
    "- 2:4 åŠç»“æ„åŒ–ç¨€ç–â€ï¼šä½ åªèƒ½åœ¨æ¯ä¸ª 4 å…ƒç»„é‡ŒæŠŠ 2 ä¸ªå˜æˆ 0ï¼ˆæˆ–ä¿ç•™ 2 ä¸ªéé›¶ï¼‰ï¼Œpattern è¢«é™åˆ¶ï¼Œç¡¬ä»¶èƒ½ç”¨å›ºå®šæ ¼å¼æŠŠå®ƒå‹ç¼©å¹¶ç”¨ä¸“é—¨ kernel åŠ é€Ÿ. å¾ˆå¤šå®ç°è¿˜ä¼šæ›´è¿›ä¸€æ­¥ï¼Œè¦æ±‚ï¼š\n",
    "$\\|b\\|_0 = 2$ ä¸¥æ ¼ 50% ç¨€ç–ï¼Œè¿™æ ·å‹ç¼©æ ¼å¼æ›´å›ºå®šã€å…ƒæ•°æ®æ›´å¥½ç®¡ç†ã€‚Ampere Sparse Tensor Cores çš„è·¯çº¿åŸºæœ¬æ˜¯å›´ç»•è¿™ç§â€œå›ºå®š 2-of-4â€æ ¼å¼åšçš„\n",
    "---\n",
    "\n",
    "### 2.5.2 ç®€å•çš„ 2:4 å‰ªæç®—æ³•ï¼ˆç¦»çº¿ï¼‰\n",
    "\n",
    "å…¸å‹å·¥ç¨‹åšæ³•ï¼š\n",
    "\n",
    "1. å¯¹æ¯ä¸ª block çš„ 4 ä¸ªæƒé‡è®¡ç®—ç»å¯¹å€¼ã€‚\n",
    "2. åœ¨ block å†…ä¿ç•™ top-2 è¾ƒå¤§çš„ï¼Œå…¶ä»–è®¾ä¸º 0ã€‚\n",
    "3. åœ¨å…¨æ¨¡å‹æˆ–æŸäº›å±‚ä¸Šç»Ÿä¸€æ‰§è¡Œï¼Œå† fine-tune è‹¥å¹² epochã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5.3. PyTorch ç¤ºä¾‹ï¼šå¯¹ Linear å±‚åš 2:4 blockwise å‰ªæ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d97b04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_2_4_blockwise(weight: torch.Tensor, dim: int = 1):\n",
    "    \"\"\"\n",
    "    å¯¹ç»™å®šæƒé‡å¼ é‡åš 2:4 å‰ªæï¼ˆç®€åŒ–ç‰ˆï¼‰.\n",
    "\n",
    "    å‡è®¾ dim=1 (åˆ—æ–¹å‘)ï¼ŒN å¿…é¡»æ˜¯ 4 çš„å€æ•°ã€‚\n",
    "    åœ¨æ¯ (M, 4) block ä¸­ä¿ç•™ç»å¯¹å€¼æœ€å¤§çš„ 2 ä¸ªå…ƒç´ .\n",
    "    \"\"\"\n",
    "    w = weight.data\n",
    "    if dim == 0:\n",
    "        w = w.t()  # äº¤æ¢æˆ (in, out) æ–¹ä¾¿å¤„ç†\n",
    "\n",
    "    M, N = w.shape\n",
    "    assert N % 4 == 0, \"For this simple example, N must be multiple of 4.\"\n",
    "\n",
    "    w_view = w.view(M, N // 4, 4)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        abs_block = w_view.abs()\n",
    "        # åœ¨ block å†…é€‰ top-2\n",
    "        top2_idx = abs_block.topk(k=2, dim=-1).indices\n",
    "        mask = torch.zeros_like(w_view, dtype=torch.bool)\n",
    "        mask.scatter_(-1, top2_idx, True)\n",
    "        w_view *= mask\n",
    "\n",
    "    # å†™å›\n",
    "    if dim == 0:\n",
    "        weight.data.copy_(w.t())\n",
    "    else:\n",
    "        weight.data.copy_(w)\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ï¼š\n",
    "# linear = nn.Linear(1024, 1024, bias=False)  # è¿™é‡Œ out_features å¿…é¡» %4==0\n",
    "# prune_2_4_blockwise(linear.weight, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eff01b",
   "metadata": {},
   "source": [
    "> æ³¨æ„ï¼š\n",
    ">\n",
    "> - çœŸå®ç³»ç»Ÿä¸­ï¼Œ2:4 æ¨¡å¼ä¼šä¸ä¸“é—¨çš„ Sparse Tensor Core kernel é…åˆä½¿ç”¨ã€‚\n",
    "> - block çš„åˆ’åˆ†æ–¹å¼ï¼ˆæŒ‰è¡Œ/åˆ—ã€æ²¿å“ªä¸ªç»´åº¦ï¼‰è¦å’Œç¡¬ä»¶ / åº•å±‚åº“çš„çº¦å®šä¸€è‡´ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 4. æ¨èé˜…è¯»\n",
    "\n",
    "- NVIDIA, **Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture**  \n",
    "  <https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-the-nvidia-ampere-architecture/>\n",
    "- PyTorch Blog, **Accelerating Inference with Sparsity in PyTorch**ï¼ˆ2:4 æ”¯æŒï¼‰  \n",
    "  <https://pytorch.org/blog/accelerating-inference-with-sparsity/>\n",
    "\n",
    "  - ç›´æ¥æŠŠæƒé‡æŸäº›ä½ç½®è®¾ä¸º 0ï¼Œå¼ é‡è¿˜æ˜¯ denseï¼Œé»˜è®¤ GEMM kernel ä»ç„¶ä¼šç…§ç®—ï¼Œæ‰€ä»¥ä¸ä¼šå˜å¿«ã€‚\n",
    "\n",
    "  - è¦å˜å¿«ï¼Œä½ å¾—æŠŠæƒé‡è½¬æ¢åˆ° semi-structured sparse çš„å‹ç¼©æ ¼å¼ï¼Œå¹¶èµ°ä¸“é—¨ kernelï¼ˆPyTorch å†…éƒ¨/åç«¯ä¼šåšç›¸åº” dispatchï¼‰\n",
    "---\n",
    "\n",
    "### 5. å·¥ä¸šè½åœ°å¤‡æ³¨\n",
    "\n",
    "- åœ¨ A100 / H100 ä¸Šï¼Œ2:4 ç¨€ç–çŸ©é˜µä¹˜å¯ä»¥è·å¾—æœ€é«˜ 2Ã— çš„ç†è®ºåŠ é€Ÿï¼Œå®é™…ç«¯åˆ°ç«¯åŠ é€Ÿè§†ç½‘ç»œç»“æ„å’Œ I/O å¼€é”€ï¼Œå¤§çº¦åœ¨ 1.3â€“1.8Ã—ã€‚\n",
    "- å¦‚æœæœªæ¥è½¦ç«¯ SoC æä¾›ç±»ä¼¼ N:M ç¨€ç–çŸ©é˜µæŒ‡ä»¤ï¼Œä½ åœ¨è¿™é‡Œç§¯ç´¯çš„ã€Œblock-constrained å‰ªæ + fine-tuneã€ç»éªŒå¯ä»¥ç›´æ¥è¿ç§»è¿‡å»ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33426e68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37e65a6e",
   "metadata": {},
   "source": [
    "## 2.6 Movement Pruning & åŠ¨æ€ç¨€ç–ï¼ˆFine-tuning åœºæ™¯ï¼‰\n",
    "\n",
    "### 2.6.1 èƒŒæ™¯ï¼šä»ã€Œå¤§å°ã€åˆ°ã€Œç§»åŠ¨è¶‹åŠ¿ã€\n",
    "\n",
    "åœ¨ä¸‹æ¸¸ä»»åŠ¡ fine-tune åœºæ™¯ï¼ˆä¾‹å¦‚ä»å¤§é¢„è®­ç»ƒæ¨¡å‹åˆ°ç‰¹å®šä»»åŠ¡ï¼‰ï¼š\n",
    "\n",
    "- æŸäº›æƒé‡æœ€ç»ˆæ•°å€¼å°ï¼Œä½†åœ¨ fine-tune è¿‡ç¨‹ä¸­ä¸€ç›´åœ¨å¾€ã€Œæœ‰ç”¨ã€æ–¹å‘ç§»åŠ¨ã€‚\n",
    "- æŸäº›æƒé‡æœ€ç»ˆæ•°å€¼å¤§ï¼Œä½†å…¶å®åœ¨ fine-tune è¿‡ç¨‹ä¸­è¢«å¾€ 0 æ‹‰ã€‚\n",
    "\n",
    "**Movement Pruning** çš„æ ¸å¿ƒï¼š\n",
    "\n",
    "> ä¸ä»…çœ‹æƒé‡çš„å¤§å°ï¼Œè¿˜çœ‹åœ¨ fine-tune è¿‡ç¨‹ä¸­çš„ã€Œç§»åŠ¨æ–¹å‘ / ç§»åŠ¨å¼ºåº¦ã€ï¼Œå¯¹ mask çš„åˆ†æ•°åšã€Œéšè®­ç»ƒæ›´æ–°ã€ã€‚\n",
    "\n",
    "å‚è€ƒï¼šSanh et al., *Movement Pruning: Adaptive Sparsity by Fine-Tuning for NLP*, NeurIPS 2020.  \n",
    "<https://arxiv.org/abs/2005.07683>\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6.2. æ¦‚å¿µæ¡†æ¶ï¼ˆç®€åŒ–ï¼‰\n",
    "\n",
    "ä¸ºæ¯ä¸ªæƒé‡å¼•å…¥ä¸€ä¸ªã€Œè¯„åˆ†å˜é‡ã€ $s_i$ï¼Œmask å®šä¹‰ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "m_i = \\mathbf{1}(s_i > 0)\n",
    "$$\n",
    "\n",
    "ä¼˜åŒ–ç›®æ ‡ï¼š\n",
    "\n",
    "$$\n",
    "\\min_{w,s} L\\big(w\\odot m(s)\\big) + \\lambda\\|s\\|_1\n",
    "$$\n",
    "\n",
    "- ç”¨ STEï¼ˆStraight-Through Estimatorï¼‰æ¥è®­ç»ƒ $m(s)$ã€‚\n",
    "- åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ$s_i$ ä¼šæ ¹æ®æ¢¯åº¦ã€Œç§»åŠ¨ã€ï¼š\n",
    "  - å¯¹ä»»åŠ¡é‡è¦çš„æƒé‡ï¼Œå…¶å¯¹åº”çš„ $s_i$ ä¼šè¢«æ¨å‘æ­£å€¼ï¼ˆä¿ç•™ï¼‰ï¼›\n",
    "  - ä¸é‡è¦çš„æƒé‡ï¼Œå…¶ $s_i$ ä¼šè¢«æ¨å‘è´Ÿå€¼ï¼ˆå‰ªæ‰ï¼‰ã€‚\n",
    "\n",
    "Movement Pruning çš„åˆ†æ•°æœ¬è´¨ä¸Šæ˜¯ï¼š\n",
    "\n",
    "> åœ¨ fine-tune çš„æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œæƒé‡è¢«æ¨å‘ã€Œä¿ç•™ã€è¿˜æ˜¯ã€Œå‰ªæ‰ã€æ–¹å‘çš„ **ç´¯ç§¯è¶‹åŠ¿**ã€‚\n",
    "Movement Pruning = ğŸ‘‰ æŠŠâ€œå‰ª or ä¸å‰ªâ€è¿™ä»¶äº‹ï¼Œä¹Ÿå˜æˆä¸€ä¸ªå¯å­¦ä¹ çš„è¿ç»­ä¼˜åŒ–å˜é‡\n",
    "\n",
    "\n",
    "1ï¸âƒ£ ä¼ ç»Ÿ magnitude pruning çš„å‡è®¾\n",
    "$|w_i|$ å° â‡’ ä¸é‡è¦ â‡’ å¯ä»¥å‰ª\n",
    "\n",
    "éšå«å‡è®¾ï¼š\n",
    "\n",
    "æƒé‡çš„â€œå½“å‰æ•°å€¼å¤§å°â€ â‰ˆ â€œå®ƒå¯¹ä»»åŠ¡çš„é‡è¦æ€§â€\n",
    "\n",
    "âš ï¸ åœ¨ fine-tuning åœºæ™¯ï¼Œè¿™ä¸ªå‡è®¾ç»å¸¸æ˜¯é”™çš„ï¼š\n",
    "\n",
    "é¢„è®­ç»ƒæƒé‡å¾ˆå¤§\n",
    "â†’ fine-tune æ—¶è¢«æŒç»­å¾€ 0 æ‹‰ï¼ˆè¯´æ˜ï¼šä¸é€‚åˆæ–°ä»»åŠ¡ï¼‰\n",
    "\n",
    "æŸäº›æƒé‡å½“å‰å¾ˆå°\n",
    "â†’ ä½†æ¢¯åº¦ä¸€ç›´åœ¨æŠŠå®ƒå¾€åŒä¸€ä¸ªæ–¹å‘æ¨ï¼ˆè¯´æ˜ï¼šæ­£åœ¨å˜é‡è¦ï¼‰\n",
    "\n",
    "ğŸ‘‰ é‡è¦çš„ä¸æ˜¯â€œç°åœ¨å¤šå¤§â€ï¼Œè€Œæ˜¯â€œè®­ç»ƒåœ¨æŠŠå®ƒå¾€å“ªé‡Œæ¨â€\n",
    "\n",
    "#### 2.6.2.1 $s_i$ æ„ä¹‰\n",
    "è¯„åˆ†å˜é‡ $s_i$ ä¸æ˜¯æƒé‡æœ¬èº«ï¼Œè€Œæ˜¯ï¼š\n",
    "ğŸ‘‰ã€Œè¿™ä¸ªæƒé‡ æ˜¯å¦å€¼å¾—å­˜åœ¨ çš„å¯å­¦ä¹ è¯æ®ç´¯ç§¯å™¨ã€\n",
    "- $w_i$: è¿™ä¸ªæƒé‡â€œå–ä»€ä¹ˆå€¼â€\n",
    "- $s_i$: è¿™ä¸ªæƒé‡â€œè¦ä¸è¦è¢«ä¿ç•™â€\n",
    "\n",
    "Movement Pruning =\n",
    "ğŸ‘‰ æŠŠâ€œå‰ª or ä¸å‰ªâ€è¿™ä»¶äº‹ï¼Œä¹Ÿå˜æˆä¸€ä¸ªå¯å­¦ä¹ çš„è¿ç»­ä¼˜åŒ–å˜é‡\n",
    "\n",
    "#### 2.6.2.2 $s_i$ ä¸ºä»€ä¹ˆ$|w_i|$åœ¨ fine-tune é‡Œä¸å¤Ÿ\n",
    "1ï¸âƒ£ ä¼ ç»Ÿ magnitude pruning çš„å‡è®¾:\n",
    "$|w_i|$ å° â‡’ ä¸é‡è¦ â‡’ å¯ä»¥å‰ª\n",
    "éšå«å‡è®¾ï¼š\n",
    "\n",
    "æƒé‡çš„â€œå½“å‰æ•°å€¼å¤§å°â€ â‰ˆ â€œå®ƒå¯¹ä»»åŠ¡çš„é‡è¦æ€§â€\n",
    "âš ï¸ åœ¨ fine-tuning åœºæ™¯ï¼Œè¿™ä¸ªå‡è®¾ç»å¸¸æ˜¯é”™çš„ï¼š\n",
    "\n",
    "é¢„è®­ç»ƒæƒé‡å¾ˆå¤§\n",
    "â†’ fine-tune æ—¶è¢«æŒç»­å¾€ 0 æ‹‰ï¼ˆè¯´æ˜ï¼šä¸é€‚åˆæ–°ä»»åŠ¡ï¼‰\n",
    "\n",
    "æŸäº›æƒé‡å½“å‰å¾ˆå°\n",
    "â†’ ä½†æ¢¯åº¦ä¸€ç›´åœ¨æŠŠå®ƒå¾€åŒä¸€ä¸ªæ–¹å‘æ¨ï¼ˆè¯´æ˜ï¼šæ­£åœ¨å˜é‡è¦ï¼‰\n",
    "\n",
    "ğŸ‘‰ é‡è¦çš„ä¸æ˜¯â€œç°åœ¨å¤šå¤§â€ï¼Œè€Œæ˜¯â€œè®­ç»ƒåœ¨æŠŠå®ƒå¾€å“ªé‡Œæ¨â€\n",
    "\n",
    "#### 2.6.2.3 æƒé‡åœ¨è®­ç»ƒä¸­ä¸‰ä¸ªæ–¹é¢çš„è¡¡é‡\n",
    "| å˜é‡    | å«ä¹‰       | è§’è‰²             |\n",
    "| ----- | -------- | -------------- |\n",
    "| $w_i$ | æƒé‡å€¼      | æ•°å€¼å¤§å° / è¡¨è¾¾èƒ½åŠ›    |\n",
    "| $s_i$ | **è¯„åˆ†å˜é‡** | æ˜¯å¦å€¼å¾—ä¿ç•™         |\n",
    "| $m_i$ | mask     | çœŸæ­£å‚ä¸è®¡ç®—çš„ 0/1 å¼€å…³ |\n",
    "\n",
    "å…³ç³»æ˜¯ï¼š\n",
    "$$m_i = \\mathbf{1}(s_i > 0)$$\n",
    "$$\\text{effective weight} = w_i \\cdot m_i$$\n",
    "\n",
    "- ä¸ºä»€ä¹ˆè¦æœ‰ $s_i$, ä¸èƒ½ç›´æ¥å­¦$m_i$?\n",
    "  - å› ä¸ºï¼š$m_i \\in \\{0,1\\}$, **ç¦»æ•£ã€ä¸å¯å¯¼**\n",
    "  - è®­ç»ƒæ˜¯é æ¢¯åº¦çš„ â†’ æ²¡æ³•ç›´æ¥ä¼˜åŒ–\n",
    "\n",
    "#### 2.6.2.4 ä¼˜åŒ–ç›®æ ‡\n",
    "$$\\min_{w,s} \\; L\\big(w \\odot m(s)\\big) + \\lambda \\|s\\|_1$$\n",
    "\n",
    "\n",
    "- ä»»åŠ¡Loss $L(w \\odot m(s))$\n",
    "forward æ—¶ï¼š\n",
    "  - å¦‚æœ $s_i > 0$ è¿™ä¸ªæƒé‡å‚ä¸è®¡ç®—\n",
    "  - å¦‚æœ $s_i \\le 0$ è¿™ä¸ªæƒé‡è¢«å±è”½\n",
    "ğŸ“Œ æ³¨æ„ï¼šloss å¹¶ä¸ç›´æ¥çœ‹åˆ° $s_i ä½†ä¼šé€šè¿‡ mask é—´æ¥å½±å“ $s_i$\n",
    "\n",
    "- ç¨€ç–åŒ–ï¼š$\\lambda \\|s\\|_1$\n",
    "  - $\\|s\\|_1$ è¶Šå°\n",
    "  - è¶Šå¤š$s_i \\to 0$\n",
    "  - è¶Šå¤š mask è¢«å…³æ‰\n",
    "ğŸ‘‰ å’Œ LASSO éå¸¸åƒï¼Œä½†å¯¹è±¡ä¸æ˜¯ $w$ è€Œæ˜¯â€œæ˜¯å¦ä¿ç•™â€  \n",
    "\tâ€‹\n",
    "#### 2.6.2.5 æ¢¯åº¦å¦‚ä½•â€œæ¨åŠ¨ $s_i$?\n",
    "- mask ä¸å¯å¯¼ï¼Œäºæ˜¯ç”¨ï¼šStraight-Through Estimator (STE)ã€‚ç›´è§‰å°±æ˜¯ï¼š\n",
    "\n",
    "  - forwardï¼šğŸ‘‰ hard thresholdï¼ˆ0 / 1ï¼‰\n",
    "\n",
    "  - backwardï¼šå‡è£… $m_i \\approx s_i$\n",
    "  - è®©æ¢¯åº¦ç›´æ¥æµåˆ° $s_i$\n",
    "\n",
    "- æ¢¯åº¦å¦‚ä½•å½±å“ $s_i$\n",
    "å‡è®¾æŸæ¬¡åå‘ä¼ æ’­ä¸­ï¼š\n",
    "\n",
    "æ¢¯åº¦å¸Œæœ› å¢å¤§ $w_i$ ï¼ˆæœ‰åŠ©äºé™ä½ lossï¼‰ï¼Œå½“å‰ $w_i > 0$ ï¼›é‚£ä¹ˆæ¢¯åº¦ä¼šæ¨åŠ¨$s_i$â†‘ï¼ˆè®© mask æ›´ç¨³åœ°ä¸º 1ï¼‰ï¼› \n",
    "åä¹‹ï¼Œå¦‚æœæ¢¯åº¦æ–¹å‘ä¸ $w_i$ å½“å‰ç¬¦å·ç›¸åï¼Œæˆ–è€…è¯¥æƒé‡å¯¹ loss æ²¡å¸®åŠ© ğŸ‘‰ $s_i$ä¼šè¢«æ¨å‘è´Ÿå€¼ â†’ mask å…³é—­\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6.3. PyTorch ç¤ºä¾‹ï¼šç®€åŒ–çš„ movement-like åˆ†æ•°\n",
    "\n",
    "ä¸‹é¢æ˜¯ä¸€ä¸ªã€Œç©å…·ç‰ˆã€çš„ movement-style åˆ†æ•°æ”¶é›†å™¨ï¼šç”¨ç´¯è®¡çš„ $-\\text{sign}(w)\\cdot g$ ä½œä¸º movement è¿‘ä¼¼ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "478f9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_movement_scores(model, dataloader, device=\"cuda\", steps=100):\n",
    "    \"\"\"\n",
    "    ç®€åŒ–ç‰ˆ movement score:\n",
    "    å¯¹æ¯ä¸ª weight ç»Ÿè®¡ sum (-grad * sign(weight)) / steps.\n",
    "    åˆ†æ•°è¶Šå¤§ï¼Œè¯´æ˜åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¶Šè¢«å¾€å½“å‰ç¬¦å·æ–¹å‘æ¨åŠ¨ï¼Œè¶Šé‡è¦ã€‚\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    movement_scores = {\n",
    "        name: torch.zeros_like(p.data)\n",
    "        for name, p in model.named_parameters() if \"weight\" in name\n",
    "    }\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    it = iter(dataloader)\n",
    "    for step in range(steps):\n",
    "        try:\n",
    "            x, y = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(dataloader)\n",
    "            x, y = next(it)\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, p in model.named_parameters():\n",
    "                if \"weight\" in name and p.grad is not None:\n",
    "                    movement_scores[name].add_(-p.grad * p.data.sign())\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "    for name in movement_scores:\n",
    "        movement_scores[name] /= max(1, steps)\n",
    "\n",
    "    return movement_scores\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ï¼š\n",
    "# scores = collect_movement_scores(model, train_loader, steps=200)\n",
    "# prune_by_scores(model, scores, sparsity=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae69d1",
   "metadata": {},
   "source": [
    "`movement_scores[name].add_(-p.grad * p.data.sign())`, æ•°å­¦çš„è§£è¯»æ˜¯ï¼š\n",
    "å› ä¸ºæƒé‡æ›´æ–°çš„æ¢¯åº¦ä¸‹é™\n",
    "- SGD æ›´æ–°ï¼š$w \\leftarrow w - \\eta g$\n",
    "- Movement score ç”¨çš„æ˜¯ï¼š$\\text{score} \\;\\sim\\; - g \\cdot \\mathrm{sign}(w)$\n",
    "\n",
    "\n",
    "$$\\Delta s_i \\;\\propto\\; - g_i \\cdot \\text{sign}(w_i)$$\n",
    "\n",
    "$$s_i \\approx \\sum_t - g_i^{(t)} \\cdot \\mathrm{sign}(w_i^{(t)})$$\n",
    "è¿™ç­‰ä»·äºï¼š\n",
    "\n",
    "ç»Ÿè®¡æ•´ä¸ª fine-tune è¿‡ç¨‹ä¸­ï¼š\n",
    "æ¢¯åº¦æœ‰å¤šå°‘æ¬¡åœ¨â€œæ”¯æŒå®ƒç»§ç»­å­˜åœ¨â€\n",
    "\n",
    "- å¶å°”æ”¯æŒ â†’ åˆ†æ•°å°\n",
    "\n",
    "- é•¿æœŸæ”¯æŒ â†’ åˆ†æ•°å¤§\n",
    "\n",
    "- é•¿æœŸåå¯¹ â†’ åˆ†æ•°è´Ÿ â†’ å‰ªæ‰\n",
    "\n",
    "| æƒ…å†µ      | $g_i$        | $\\text{sign}(w_i)$ | $-g_i \\cdot \\text{sign}(w_i)$ | è§£è¯»       |\n",
    "| ------- | ------------ | ------------------ | ----------------------------- | -------- |\n",
    "| æ¢¯åº¦æƒ³å¢å¤§æƒé‡ | ä¸ sign(w) ç›¸å | +                  | æ­£                             | **æ”¯æŒä¿ç•™** |\n",
    "| æ¢¯åº¦æƒ³å‡å°æƒé‡ | ä¸ sign(w) åŒå· | +                  | è´Ÿ                             | **æ”¯æŒå‰ªæ‰** |\n",
    "\n",
    "\n",
    "ğŸ‘‰ ç´¯è®¡çš„æ˜¯ï¼š\n",
    "â€œè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¢¯åº¦æœ‰å¤šå¤§ç¨‹åº¦åœ¨æ”¯æŒè¿™ä¸ªæƒé‡ç»§ç»­å­˜åœ¨â€\n",
    "\n",
    "è¿™å°±æ˜¯ä¸ºä»€ä¹ˆï¼šMovement score â‰ˆ â€œä¿ç•™è¯æ®çš„æ—¶é—´ç§¯åˆ†â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a384c",
   "metadata": {},
   "source": [
    "> è¯´æ˜ï¼š\n",
    ">\n",
    "> - è¿™ä¸æ˜¯åŸè®ºæ–‡çš„ç²¾ç¡®å®ç°ï¼Œåªæ˜¯æœ‰ç±»ä¼¼ã€Œmovement æ„å›¾ã€çš„ç®€åŒ–ç‰ˆç¤ºä¾‹ï¼Œæ–¹ä¾¿åœ¨å°æ¨¡å‹ä¸Šç©ã€‚\n",
    "> - çœŸæ­£çš„ Movement Pruning ä½¿ç”¨æ˜¾å¼çš„ mask å‚æ•°åŒ–ã€æ¢¯åº¦è£å‰ªç­‰ï¼Œæ›´é€‚åˆç”¨ç°æˆå®ç°ï¼ˆå¦‚ HuggingFace çš„å®ç°ï¼‰ã€‚\n",
    "\n",
    "Movement Pruning çš„ $s_i$ æ˜¯ï¼š\n",
    "\n",
    "â€œè®­ç»ƒåŠ¨åŠ›å­¦è§†è§’çš„ importanceâ€\n",
    "\n",
    "| æ–¹æ³•           | å›ç­”çš„é—®é¢˜          |\n",
    "| ------------ | -------------- |\n",
    "| Magnitude    | ç°åœ¨å€¼å¤§ä¸å¤§         |\n",
    "| Hessian      | è¢«æ‰°åŠ¨ä¼šä¸ä¼šç‚¸        |\n",
    "| Jacobian     | è¾“å…¥å™ªå£°ä¼šä¸ä¼šæ”¾å¤§      |\n",
    "| **Movement** | **è®­ç»ƒæ˜¯å¦æŒç»­æƒ³ä¿ç•™å®ƒ** |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### 2.6.4. æ¨èé˜…è¯»\n",
    "\n",
    "- Sanh et al., **Movement Pruning: Adaptive Sparsity by Fine-Tuning for NLP**, NeurIPS 2020.  \n",
    "  <https://arxiv.org/abs/2005.07683>\n",
    "- HuggingFace å®ç°ï¼ˆæ”¯æŒ BERT ç­‰ï¼‰ï¼š  \n",
    "  <https://github.com/huggingface/transformers/tree/main/examples/research_projects/movement-pruning>\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6.5. å·¥ä¸šè½åœ°å¤‡æ³¨\n",
    "\n",
    "- å¯¹å¤§å‹ Transformerï¼ˆå°¤å…¶ NLP / å¤šæ¨¡æ€ï¼‰åœ¨è½¦ç«¯åš domain-specific fine-tune æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ Movement Pruningï¼š\n",
    "  - ä¸éœ€è¦ä»å¤´è®­ç»ƒ sparse æ¨¡å‹ã€‚\n",
    "  - ç›´æ¥åœ¨ä¸‹æ¸¸ä»»åŠ¡ fine-tune é˜¶æ®µæ”¶æ•›åˆ°ä¸€ä¸ªå¸¦ç¨€ç–çš„æƒé‡é…ç½®ã€‚\n",
    "- å¯¹æ¯”çº¯ magnitudeï¼šMovement Pruning æ›´ã€Œä»»åŠ¡ awareã€ï¼Œæ›´é€‚åˆä¸‹æ¸¸å¾®è°ƒåœºæ™¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8aaf70",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
