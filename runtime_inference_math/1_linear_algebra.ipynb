{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70de591",
   "metadata": {},
   "source": [
    "# ç¬¬ 1 ç« ï¼šçº¿æ€§ä»£æ•°ï¼ˆLinear Algebra for Runtime Inferenceï¼‰\n",
    "\n",
    "## 1.1 çº¿æ€§ä»£æ•°åœ¨æ¨ç†åŠ é€Ÿä¸­çš„è§’è‰²\n",
    "\n",
    "åœ¨æ·±åº¦å­¦ä¹ æ¨ç†ä¸­ï¼Œç»å¤§éƒ¨åˆ†â€œé‡è®¡ç®—â€éƒ½å¯ä»¥æŠ½è±¡ä¸ºï¼š\n",
    "\n",
    "- å‘é‡å†…ç§¯ï¼ˆdot productï¼‰\n",
    "- çŸ©é˜µâ€“å‘é‡ä¹˜æ³•ï¼ˆGEMVï¼‰\n",
    "- çŸ©é˜µâ€“çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰\n",
    "- å·ç§¯ç»å˜æ¢åå˜æˆçš„ GEMMï¼ˆim2col / Winogradï¼‰\n",
    "\n",
    "ä¾‹å¦‚ï¼š\n",
    "\n",
    "- å…¨è¿æ¥å±‚ï¼ˆFully Connectedï¼‰ï¼š  \n",
    "  $y = W x + b$\n",
    "- Transformer MLPï¼š  \n",
    "  $Y = \\sigma(X W_1 + b_1) W_2 + b_2$\n",
    "- è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰ï¼š  \n",
    "  $Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V$  \n",
    "  $A = \\text{softmax}(QK^\\top / \\sqrt{d_k})$  \n",
    "  $O = A V$\n",
    "\n",
    "**ã€å·¥ç¨‹è§†è§’ã€‘**  \n",
    "æ¨ç†åŠ é€Ÿåº“ï¼ˆcuBLASã€CUTLASSã€MKLã€QNNPACKã€TensorRT ç­‰ï¼‰çš„æ ¸å¿ƒä»»åŠ¡ï¼Œå°±æ˜¯ï¼š\n",
    "\n",
    "> è®©è¿™äº›çŸ©é˜µ/å‘é‡è¿ç®—åœ¨ç‰¹å®šç¡¬ä»¶ä¸Šå°½å¯èƒ½é«˜æ•ˆåœ°æ‰§è¡Œã€‚\n",
    "\n",
    "ç†è§£çº¿æ€§ä»£æ•° â†’ ç†è§£ï¼š\n",
    "\n",
    "- ä¸ºä»€ä¹ˆæ‰€æœ‰äººéƒ½åœ¨è¯´ GEMM\n",
    "- ä¸ºä»€ä¹ˆ low-rank / SVD / LoRA èƒ½å¤Ÿâ€œç™½å«–â€åŠ é€Ÿ\n",
    "- ä¸ºä»€ä¹ˆ Conv2d ç»å¸¸è¢«å˜æˆ GEMM\n",
    "- ä¸ºä»€ä¹ˆ Attention çš„ä¼˜åŒ–é›†ä¸­åœ¨ QKáµ€ / AV ä¸Š\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 ä»å†…ç§¯åˆ° GEMMï¼šæ¨ç†çš„åŸºæœ¬ç®—å­\n",
    "\n",
    "### 1.2.1 å†…ç§¯ï¼ˆdot productï¼‰\n",
    "\n",
    "ç»™å®šä¸¤ä¸ªå‘é‡ $a,b \\in \\mathbb{R}^d$ï¼Œå®ƒä»¬çš„å†…ç§¯ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\langle a, b \\rangle = \\sum_{i=1}^d a_i b_i\n",
    "$$\n",
    "\n",
    "\n",
    "è¿™æ˜¯æœ€å°çš„â€œä¹˜â€“åŠ â€å•å…ƒã€‚\n",
    "\n",
    "**ã€ç¡¬ä»¶è§†è§’ã€‘**  \n",
    "ç°ä»£ CPU/GPU/NPU éƒ½æœ‰ **FMA (Fused Multiply-Add)** æŒ‡ä»¤ï¼š\n",
    "\n",
    "$$\n",
    "\\text{FMA}(a,b,c) = a \\times b + c\n",
    "$$\n",
    "\n",
    "\n",
    "dot product å¯ä»¥å†™æˆï¼š\n",
    "\n",
    "```text\n",
    "acc = 0\n",
    "for i in range(d):\n",
    "    acc = fma(a[i], b[i], acc)\n",
    "```\n",
    "\n",
    "- FMA æ˜¯çŸ©é˜µä¹˜æ³•çš„åº•å±‚åŸè¯­ï¼ˆprimitiveï¼‰\n",
    "- è¶Šèƒ½è®©ç¡¬ä»¶é•¿æ—¶é—´â€œåˆ· FMAâ€ï¼Œæ¨ç†ååè¶Šé«˜\n",
    "- æ•°å­¦ä¸Šçš„æ„ä¹‰ï¼š\n",
    "$\\text{round}\\big(\\text{round}(a\\times b) + c \\big)\n",
    "\\neq\n",
    "\\text{round}(a\\times b + c)$\n",
    "FMA ä½¿ç”¨å³è¾¹çš„ï¼ˆæ›´ç²¾ç¡®ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2.2 çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰\n",
    "\n",
    "GEMM çš„ä¸€èˆ¬å½¢å¼ï¼š\n",
    "\n",
    "$$\n",
    "C = \\alpha A B + \\beta C\n",
    "$$\n",
    "\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $A \\in \\mathbb{R}^{M \\times K}$\n",
    "- $B \\in \\mathbb{R}^{K \\times N}$\n",
    "- $C \\in \\mathbb{R}^{M \\times N}$\n",
    "- $\\alpha, \\beta$ ä¸ºæ ‡é‡ï¼ˆå¸¸è§æƒ…å†µï¼š$\\alpha=1, \\beta=0$ï¼‰\n",
    "\n",
    "å±•å¼€å•ä¸ªå…ƒç´ ï¼š\n",
    "\n",
    "$$\n",
    "c_{ij} = \\alpha \\sum_{k=1}^K a_{ik} b_{kj} + \\beta c_{ij}^{\\text{(old)}}\n",
    "$$\n",
    "\n",
    "\n",
    "è¿™å°±æ˜¯ï¼š**å¤šæ¬¡ FMA çš„äºŒç»´ç‰ˆæœ¬**ã€‚\n",
    "\n",
    "**ã€å·¥ç¨‹è§†è§’ã€‘**  \n",
    "- å…¨è¿æ¥å±‚ = å¤šä¸ª GEMM\n",
    "- Attention = 2 ä¸ªå¤§ GEMMï¼ˆQKáµ€ å’Œ AVï¼‰+ è‹¥å¹²å°ç®—å­\n",
    "- Conv2d ç»è¿‡å˜æ¢å = GEMM\n",
    "- æ¨ç†åŠ é€Ÿçš„å¤§å¤´ï¼Œå°±æ˜¯æŠŠä¸åŒç®—å­ç»Ÿä¸€æ˜ å°„åˆ°é«˜æ•ˆ GEMM å†…æ ¸ä¸Š\n",
    "\n",
    "\n",
    "å°ç»“\n",
    "| ç½‘ç»œç»„ä»¶            | æ•°å­¦å½¢å¼     | æœ€ç»ˆå˜æˆ          |\n",
    "| --------------- | -------- | ------------- |\n",
    "| å…¨è¿æ¥å±‚ FC         | Wx + b   | GEMM          |\n",
    "| å·ç§¯ conv2d       | K * X    | im2col â†’ GEMM |\n",
    "| Attention QKáµ€   | QKáµ€      | GEMM          |\n",
    "| Attention AV    | AV       | GEMM          |\n",
    "| Transformer MLP | XWâ‚, XWâ‚‚ | GEMM          |\n",
    "| Embedding       | æŸ¥è¡¨       | é€‰è¡Œï¼ˆç±»ä¼¼ GEMMï¼‰   \n",
    "\n",
    "é™„ï¼š ä¸ºä»€ä¹ˆè¯´â€œEmbedding ç±»ä¼¼ GEMMâ€ï¼Ÿ\n",
    "\n",
    "è™½ç„¶ æ•°å­¦ä¸Š Embedding æ˜¯é€‰è¡Œï¼Œ\n",
    "ä½†å®ƒå¯ä»¥çœ‹æˆä¸€ä¸ªéå¸¸ç¨€ç–çš„çŸ©é˜µä¹˜æ³•ã€‚\n",
    "ç»™ä¸€ä¸ª embedding matrixï¼ˆè¯å‘é‡çŸ©é˜µï¼‰\n",
    "$E \\in \\mathbb{R}^{V \\times D}$\n",
    "- V = vocabulary sizeï¼ˆå¦‚ 50kï¼‰\n",
    "\n",
    "- D = hidden dimensionï¼ˆå¦‚ 1024ï¼‰\n",
    "\n",
    "æ¯ä¸€è¡Œ $ğ¸_i$ å°±æ˜¯ä¸€ä¸ª token çš„å‘é‡ã€‚å½“ä¸€ä¸ª token çš„ ID = i æ—¶ï¼Œembedding åšçš„äº‹æƒ…æ˜¯ï¼š$\\text{Embedding}(i) = E_{i}$\n",
    "æ²¡æœ‰ä¹˜æ³•ï¼Œæ²¡æœ‰çŸ©é˜µè¿ç®—ï¼Œæ‰€ä»¥ Embedding çš„ FLOPs ~ 0ï¼ˆå‡ ä¹ 0ï¼‰ã€‚\n",
    "çœŸæ­£çš„æˆæœ¬æ˜¯ï¼š\n",
    "\n",
    "- å†…å­˜è®¿é—®ï¼ˆmemory bandwidthï¼‰\n",
    "\n",
    "- éšæœºè®¿é—®ï¼ˆrandom accessï¼‰\n",
    "\n",
    "- cache missï¼ˆembedding layer å¾ˆå®¹æ˜“ miss cacheï¼‰\n",
    "\n",
    "å‡è®¾ token id = 3ï¼Œ è¿™æ—¶å¯ä»¥æ„é€ ä¸€ä¸ª size V çš„ one-hot å‘é‡ï¼š$x = [0,0,0,1,0,0,\\dots]^T \\in \\mathbb{R}^V$\n",
    "\n",
    "Embedding çš„è¾“å‡ºå°±æ˜¯ï¼š$y = x^T E \\in \\mathbb{R}^D$\n",
    "\n",
    "å±•å¼€æ¥çœ‹ï¼š$y = \\sum_{i=1}^V x_i E_i$\n",
    "\n",
    "ä½†å› ä¸ºåªæœ‰ä¸€ä¸ª $X_i = 1$ï¼Œå…¶ä½™å…¨æ˜¯ 0ï¼š $y = E_3$\n",
    "\n",
    "è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ç‰¹æ®Šçš„ GEMMï¼š$Y = X E$\n",
    "å…¶ä¸­ X æ˜¯ batch ä¸ª one-hot å‘é‡ã€‚\n",
    "\n",
    "åªæ˜¯ï¼š\n",
    "\n",
    "X ä¸­ç»å¤§éƒ¨åˆ†å€¼æ˜¯ 0ï¼Œ X åªæœ‰ä¸€ä¸ª 1ï¼Œ æ‰€ä»¥çŸ©é˜µä¹˜æ³•é€€åŒ–æˆâ€œé€‰è¡Œâ€.\n",
    "\n",
    "è¿™ä¹ˆç†è§£çš„å¥½å¤„ï¼š\n",
    "Embedding ä»æ•°å­¦ä¸Šå¯ä»¥è§†ä¸º GEMM çš„ç‰¹ä¾‹ï¼ˆç¨€ç– GEMMï¼‰\n",
    "\n",
    "è¿™è®©æˆ‘ä»¬å¯ä»¥ï¼š\n",
    "\n",
    "- ä½¿ç”¨çŸ©é˜µè§‚ç‚¹åˆ†æå®ƒ\n",
    "\n",
    "- ä½¿ç”¨ GEMM çš„ layoutã€tiling ç†è§£å…¶ä¼˜åŒ–\n",
    "\n",
    "- æŠŠ embedding ä¹Ÿå½“æˆä¸€ä¸ªâ€œçŸ©é˜µç®—å­â€ï¼Œå¯ä»¥èåˆã€é‡åŒ–ã€cacheå‹å¥½åŒ–\n",
    "\n",
    "æ€»ç»“ä¸€ä¸‹ï¼ŒEmbedding çš„ç“¶é¢ˆä¸æ˜¯ç®—åŠ›ï¼ˆä¸æ˜¯ FMAï¼‰ï¼Œè€Œæ˜¯ï¼š\n",
    "\n",
    "- å†…å­˜å¸¦å®½ï¼ˆbandwidthï¼‰\n",
    "\n",
    "- éšæœºè®¿é—®ï¼ˆrandom access patternsï¼‰\n",
    "\n",
    "- cache line miss\n",
    "\n",
    "å› æ­¤ï¼š\n",
    "\n",
    "å¸¸è§çœ‹åˆ°ä¼˜åŒ–ç­–ç•¥æ˜¯ï¼š\n",
    "\n",
    "- æŠŠ embedding matrix rearrangeï¼ˆè¡Œå‹ç¼©ï¼‰\n",
    "\n",
    "- æŠŠå¸¸ç”¨ token æå‰æ”¾åœ¨ cache-friendly åŒºåŸŸ\n",
    "\n",
    "- æ›´å° embeddingï¼ˆé‡åŒ–ã€å‡å°‘ç»´åº¦ï¼‰\n",
    "\n",
    "- batching lookups\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2.3 å¼ é‡è¿ç®—ï¼šçº¿æ€§ä»£æ•°çš„å¤šç»´ç‰ˆæœ¬\n",
    "\n",
    "å¼ é‡ï¼ˆtensorï¼‰æœ¬è´¨å°±æ˜¯å¤šç»´æ•°ç»„ï¼Œå¸¸è§æ˜¯ rank-3 / rank-4ï¼š\n",
    "\n",
    "- rank-3ï¼š$B \\times T \\times D$ï¼ˆbatch Ã— seq_len Ã— hidden_dimï¼‰\n",
    "- rank-4ï¼š$B \\times C \\times H \\times W$ï¼ˆç”¨äºå·ç§¯ï¼‰\n",
    "\n",
    "å¼ é‡è¿ç®—ï¼ˆeinsumã€batched matmulï¼‰æœ€ç»ˆéƒ½ä¼šè¢«åˆ†è§£æˆï¼š\n",
    "\n",
    "- ä¸€å † reshape / transpose\n",
    "- è‹¥å¹²çŸ©é˜µä¹˜æ³•ï¼ˆbatched GEMMï¼‰\n",
    "- å† reshape å›æƒ³è¦çš„å½¢çŠ¶\n",
    "\n",
    "> æ‰€ä»¥â€œå­¦çº¿æ€§ä»£æ•°â€ä¸ä»…æ˜¯å­¦çŸ©é˜µï¼Œè¿˜è¦å­¦ä¼š**å¦‚ä½•æŠŠé«˜ç»´å¼ é‡ reshape æˆçŸ©é˜µ**ä»¥ä¾¿ä½¿ç”¨é«˜æ•ˆ GEMMã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9240db3",
   "metadata": {},
   "source": [
    "## 1.3 ä½ç§©è¿‘ä¼¼ï¼ˆLow-Rank Approximationï¼‰ä¸ SVD\n",
    "\n",
    "### 1.3.1 é—®é¢˜èƒŒæ™¯\n",
    "\n",
    "ç»™å®šä¸€ä¸ªå¤§çš„æƒé‡çŸ©é˜µ $W \\in \\mathbb{R}^{m \\times n}$ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "- Transformer MLP çš„æƒé‡ï¼ˆå¦‚ 4096 Ã— 11008ï¼‰\n",
    "- Attention ä¸­çš„æŠ•å½±çŸ©é˜µï¼ˆå¦‚ 4096 Ã— 4096ï¼‰\n",
    "\n",
    "åœ¨å¾ˆå¤šå®é™…æ¨¡å‹ä¸­ï¼Œ$W$ çš„â€œæœ‰æ•ˆç§©ï¼ˆeffective rankï¼‰â€è¿œå°äº $\\min(m,n)$ï¼š\n",
    "- ä¹Ÿå°±æ˜¯è¯´ï¼Œâ€œä¿¡æ¯â€é›†ä¸­åœ¨å°‘æ•°å‡ ä¸ªæ–¹å‘ä¸Šã€‚\n",
    "\n",
    "è¿™æ—¶æˆ‘ä»¬å¸Œæœ›ç”¨ä¸€ä¸ªç§©ä¸º $k \\ll \\min(m,n)$ çš„çŸ©é˜µ $W_k$ æ¥è¿‘ä¼¼ $W$ï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\text{rank}(X) \\le k} \\ \\|W - X\\|_F\n",
    "$$\n",
    "\n",
    "\n",
    "è¿™ä¸ªé—®é¢˜çš„è§£æè§£ç”± **å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰** ç»™å‡ºã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.2 å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰çš„å½¢å¼\n",
    "\n",
    "$$\n",
    "W = U \\Sigma V^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ï¼šåˆ—å‘é‡ä¸ºå·¦å¥‡å¼‚å‘é‡\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ï¼šåˆ—å‘é‡ä¸ºå³å¥‡å¼‚å‘é‡\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ï¼šå¯¹è§’çº¿ä¸ºå¥‡å¼‚å€¼ $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$ï¼Œå…¶ä½™ä¸º 0\n",
    "\n",
    "æˆªæ–­åˆ°å‰ $k$ ä¸ªå¥‡å¼‚å€¼ï¼š\n",
    "\n",
    "$$\n",
    "W_k = U_k \\Sigma_k V_k^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "- $U_k \\in \\mathbb{R}^{m \\times k}$\n",
    "- $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$\n",
    "- $V_k \\in \\mathbb{R}^{n \\times k}$\n",
    "\n",
    "è¿™æ˜¯ä¸€ä¸ªç§©ä¸º $k$ çš„çŸ©é˜µã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.3 æœ€ä¼˜ä½ç§©è¿‘ä¼¼å®šç†ï¼ˆEckartâ€“Youngâ€“Mirskyï¼‰\n",
    "\n",
    "**å®šç†ï¼š**  \n",
    "åœ¨ Frobenius èŒƒæ•°ä¸‹ï¼Œç§©ä¸è¶…è¿‡ $k$ çš„çŸ©é˜µä¸­ç¦» $W$ æœ€è¿‘çš„æ˜¯ $W_k$ï¼š\n",
    "\n",
    "$$\n",
    "W_k = \\arg\\min_{\\text{rank}(X) \\le k} \\|W - X\\|_F\n",
    "$$\n",
    "\n",
    "\n",
    "è€Œè¯¯å·®å¤§å°åˆšå¥½ç­‰äºè¢«æˆªæ‰çš„å¥‡å¼‚å€¼çš„å¹³æ–¹å’Œï¼š\n",
    "\n",
    "$$\n",
    "\\|W - W_k\\|_F^2 = \\sum_{i=k+1}^r \\sigma_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "**ã€ç›´è§‚è§£é‡Šã€‘**  \n",
    "å¥‡å¼‚å€¼è¶Šå¤§ï¼Œå¯¹çŸ©é˜µâ€œèƒ½é‡â€çš„è´¡çŒ®è¶Šå¤§ï¼›æˆªæ–­åï¼Œä¸¢æ‰çš„èƒ½é‡æ°å¥½æ˜¯å¯¹åº”å¥‡å¼‚å€¼å¹³æ–¹çš„æ€»å’Œã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.4 ç”¨ä¸¤ä¸ªå°çŸ©é˜µå®ç°ä½ç§©è¿‘ä¼¼\n",
    "\n",
    "åœ¨å·¥ç¨‹ä¸­ï¼Œä¸ºäº†å‡å°‘ FLOPsï¼Œé€šå¸¸æŠŠ $W_k$ æ‹†æˆä¸¤ä¸ªæ›´å°çš„çŸ©é˜µç›¸ä¹˜ï¼š\n",
    "\n",
    "$$\n",
    "W_k = A B, \\quad A \\in \\mathbb{R}^{m \\times k}, \\ B \\in \\mathbb{R}^{k \\times n}\n",
    "$$\n",
    "\n",
    "\n",
    "ä¸€ç§å¸¸è§æ„é€ æ˜¯ï¼š\n",
    "\n",
    "$$\n",
    "A = U_k \\Sigma_k^{1/2}, \\quad B = \\Sigma_k^{1/2} V_k^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "äºæ˜¯ï¼š\n",
    "\n",
    "$$\n",
    "W x \\approx W_k x = A (B x)\n",
    "$$\n",
    "\n",
    "\n",
    "- åŸå§‹è®¡ç®—ï¼š$W x$ æ˜¯ä¸€ä¸ª $m \\times n$ çš„çŸ©é˜µâ€“å‘é‡ä¹˜æ³•ï¼Œå¤æ‚åº¦ $O(mn)$\n",
    "- åˆ†è§£åï¼š\n",
    "  - å…ˆç®— $z = B x$ï¼ˆç»´åº¦ $k$ï¼‰\n",
    "  - å†ç®— $A z$\n",
    "  - æ€»å¤æ‚åº¦ $O(kn + mk)$\n",
    "\n",
    "å¦‚æœ $k \\ll \\min(m,n)$ï¼Œæ€»ä½“ FLOPs å¤§å¹…å‡å°‘ã€‚\n",
    "\n",
    "**ã€å·¥ç¨‹è§†è§’ã€‘**  \n",
    "- è¿™å°±æ˜¯å¾ˆå¤š **SVD-based compression / low-rank factorization / LoRA** å†…æ ¸çš„æ•°å­¦åŸºç¡€ã€‚\n",
    "- åœ¨æ¨¡å‹éƒ¨ç½²æ—¶ï¼Œä½ ä¼šçœ‹åˆ°æŸäº›å±‚è¢«æ›¿æ¢æˆä¸¤å±‚å°çš„ Linearï¼š`Linear(d_in â†’ k)` + `Linear(k â†’ d_out)`ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.5 æ–‡å­—å›¾ç¤ºï¼ˆçŸ©é˜µç»“æ„çš„å¯è§†åŒ–ï¼‰\n",
    "\n",
    "åŸå§‹çŸ©é˜µ $W$ï¼š\n",
    "\n",
    "```text\n",
    "W (mÃ—n)\n",
    "+------------------------+\n",
    "| â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ  |\n",
    "| â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ  |\n",
    "| â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ  |\n",
    "| â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ  |\n",
    "|         Â·Â·Â·            |\n",
    "+------------------------+\n",
    "```\n",
    "\n",
    "ä½ç§©åˆ†è§£åï¼š\n",
    "\n",
    "```text\n",
    "A (mÃ—k)          B (kÃ—n)\n",
    "+------+       +------------------+\n",
    "| â–ˆ â–ˆ |       | â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ   |\n",
    "| â–ˆ â–ˆ |   x   | â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ   |\n",
    "| â–ˆ â–ˆ |       |        Â·Â·Â·        |\n",
    "| â–ˆ â–ˆ |       +------------------+\n",
    "+------+\n",
    "```\n",
    "\n",
    "æ¨ç†æ—¶ï¼š\n",
    "\n",
    "1. å…ˆè®¡ç®— $z = B x$ï¼ˆç»´åº¦ kï¼‰  \n",
    "2. å†è®¡ç®— $y = A z$\n",
    "\n",
    "å½“ k æ¯” m,n å°å¾ˆå¤šæ—¶ï¼Œè¿™æ˜¯æ˜¾è‘—çš„åŠ é€Ÿã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.6 åœ¨æ¨ç†åŠ é€Ÿä¸­çš„å…¸å‹ç”¨æ³•\n",
    "\n",
    "1. **MLP æƒé‡ä½ç§©åˆ†è§£**  \n",
    "   - åŸå§‹ï¼š4096Ã—11008 çš„å·¨çŸ©é˜µ  \n",
    "   - åˆ†è§£ï¼š4096Ã—k å’Œ kÃ—11008ï¼Œk=1024 æˆ–æ›´å°  \n",
    "2. **LoRAï¼ˆLow-Rank Adaptationï¼‰**  \n",
    "   - æŠŠæ›´æ–°é‡ $\\Delta W$ å‚æ•°åŒ–ä¸º $A B$ï¼Œrank é€šå¸¸å¾ˆå°ï¼ˆå¦‚ 8ã€16ï¼‰  \n",
    "   - å¯¹éƒ¨ç½²æ¥è¯´ï¼Œæ¨ç†æ—¶å¤šäº†ä¸¤å±‚å°çŸ©é˜µä¹˜æ³•\n",
    "3. **KV Cache å‹ç¼© / æŠ•å½±**  \n",
    "   - ç”¨ä½ç§©æ˜ å°„é™ä½ KV ç»´åº¦ï¼Œä»è€Œå‡å°‘å†…å­˜å’Œå¸¦å®½\n",
    "4. **ç»“æ„åŒ–å‰ªæçš„æ›¿ä»£æ–¹æ¡ˆ**  \n",
    "   - ç›¸æ¯”ç›´æ¥ç¡¬å‰ªé€šé“ï¼Œä½ç§©åˆ†è§£æ˜¯æ›´å¹³æ»‘çš„ç»´åº¦å‰Šå‡æ–¹å¼\n",
    "\n",
    "> åœ¨è¿™ç±»åšè¾¹ç¼˜æ¨ç†çš„åœºæ™¯ä¸­ï¼š  \n",
    "> - ä½ ä¼šå¸Œæœ›åœ¨ **ä¸å¤§å¹…é™ç²¾åº¦** çš„å‰æä¸‹ï¼Œ**æ˜¾è‘—å‡å°‘ MLP å’ŒæŠ•å½±å±‚çš„ FLOPs å’Œå‚æ•°é‡**ï¼Œä½ç§©åˆ†è§£æ˜¯æœ€ä¸»æµçš„æ•°å­¦å·¥å…·ä¹‹ä¸€ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc690d1b",
   "metadata": {},
   "source": [
    "## 1.4 çŸ©é˜µèŒƒæ•°ä¸è¯¯å·®åº¦é‡\n",
    "\n",
    "åœ¨åšä»»ä½•å‹ç¼©ï¼ˆä½ç§©ã€å‰ªæã€é‡åŒ–ï¼‰æ—¶ï¼Œä½ éƒ½éœ€è¦ä¸€ä¸ªâ€œåº¦é‡æ ‡å‡†â€æ¥è¡¡é‡ï¼š\n",
    "\n",
    "- å‹ç¼©å‰åçš„æƒé‡å·®å¼‚æœ‰å¤šå¤§ï¼Ÿ\n",
    "- è¿™ä¼šå¸¦æ¥å¤šå¤§çš„è¾“å‡ºè¯¯å·®ï¼Ÿ\n",
    "\n",
    "çŸ©é˜µèŒƒæ•°æä¾›äº†è¿™äº›åº¦é‡å·¥å…·ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.1 Frobenius èŒƒæ•°ï¼ˆæ•´ä½“èƒ½é‡ï¼‰\n",
    "\n",
    "$$\n",
    "\\|W\\|_F = \\sqrt{\\sum_{i,j} w_{ij}^2}\n",
    "$$\n",
    "\n",
    "\n",
    "å¯¹åº”äºæŠŠçŸ©é˜µå½“æˆä¸€ä¸ªé•¿å‘é‡åçš„ $L_2$ èŒƒæ•°ã€‚\n",
    "\n",
    "- å®ƒè¡¡é‡æ•´ä½“â€œèƒ½é‡â€ï¼ˆenergyï¼‰\n",
    "- åœ¨ SVD ä¸­æœ‰éå¸¸æ¼‚äº®çš„æ€§è´¨ï¼š\n",
    "\n",
    "$$\n",
    "\\|W\\|_F^2 = \\sum_{i=1}^r \\sigma_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "è€Œå¯¹ä½ç§©è¿‘ä¼¼ï¼š\n",
    "\n",
    "$$\n",
    "\\|W - W_k\\|_F^2 = \\sum_{i=k+1}^r \\sigma_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "**ã€å·¥ç¨‹åº”ç”¨ã€‘**  \n",
    "- å¯ä»¥é€šè¿‡å¥‡å¼‚å€¼å¿«é€Ÿä¼°è®¡å‹ç¼©è¯¯å·®çš„ä¸Šç•Œ\n",
    "- å¯ä»¥æ¯”è¾ƒä¸åŒ $k$ å€¼ä¸‹çš„è¯¯å·®ï¼Œåš bitâ€“accuracy tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.2 è°±èŒƒæ•°ï¼ˆoperator norm / 2-èŒƒæ•°ï¼‰\n",
    "\n",
    "$$\n",
    "\\|W\\|_2 = \\sigma_{\\max}(W)\n",
    "$$\n",
    "\n",
    "\n",
    "- ç­‰äºæœ€å¤§çš„å¥‡å¼‚å€¼\n",
    "- åæ˜ ï¼š$W$ ä½œä¸ºçº¿æ€§ç®—å­èƒ½æŠŠå‘é‡æ”¾å¤§çš„æœ€å¤§æ¯”ä¾‹\n",
    "\n",
    "å…·ä½“æ¥è¯´ï¼š\n",
    "\n",
    "$$\n",
    "\\|W\\|_2 = \\max_{\\|x\\|_2 = 1} \\|W x\\|_2\n",
    "$$\n",
    "\n",
    "\n",
    "**ã€å·¥ç¨‹æ„ä¹‰ã€‘**  \n",
    "- å¦‚æœ $\\|W\\|_2$ éå¸¸å¤§ï¼Œåˆ™å¯¹è¾“å…¥çš„å°æ‰°åŠ¨éå¸¸æ•æ„Ÿ  \n",
    "**è§£é‡Š**ï¼šå½“çŸ©é˜µçš„è°±èŒƒæ•°ï¼ˆspectral normï¼‰ï¼Œå³ $|W|_2$ éå¸¸å¤§æ—¶ï¼Œå®ƒæ„å‘³ç€è¿™ä¸ªçŸ©é˜µä½œä¸ºä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œèƒ½å¤Ÿå°†è¾“å…¥å‘é‡â€œæ”¾å¤§â€çš„æœ€å¤§æ¯”ä¾‹éå¸¸å¤§ã€‚\n",
    "\n",
    "è°±èŒƒæ•°çš„å®šä¹‰æ˜¯ï¼š\n",
    "\n",
    "$$\n",
    "\\|W\\|_2 = \\max_{\\|x\\|_2 = 1} \\|W x\\|_2\n",
    "$$\n",
    "\n",
    "è¿™æ„å‘³ç€ï¼Œå¯¹äºä»»ä½•éé›¶å‘é‡ $x$ï¼Œ\n",
    "\n",
    "$$|W x|_2 \\le |W|_2 |x|_2$$\n",
    "\n",
    "ç°åœ¨ï¼Œè€ƒè™‘ä¸€ä¸ªè¾“å…¥å‘é‡ $x_0$ å—åˆ°ä¸€ä¸ªå°æ‰°åŠ¨ $\\Delta x$ã€‚é‚£ä¹ˆï¼Œè¾“å‡ºçš„æ”¹å˜å°†æ˜¯ $W(x_0 + \\Delta x) - W x_0 = W \\Delta x$ã€‚\n",
    "\n",
    "è¾“å‡ºæ‰°åŠ¨çš„èŒƒæ•°æ˜¯ $|W \\Delta x|_2$ã€‚æ ¹æ®ä¸Šé¢çš„ä¸ç­‰å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š\n",
    "\n",
    "$$|W \\Delta x|_2 \\le |W|_2 |\\Delta x|_2$$\n",
    "\n",
    "å¦‚æœ $|W|_2$ éå¸¸å¤§ï¼Œå³ä½¿è¾“å…¥çš„æ‰°åŠ¨ $|\\Delta x|_2$ éå¸¸å°ï¼Œç»è¿‡çŸ©é˜µ $W$ å˜æ¢åï¼Œè¾“å‡ºçš„æ‰°åŠ¨ $|W \\Delta x|_2$ ä¹Ÿä¼šè¢«æ”¾å¤§å¾ˆå¤šå€ã€‚è¿™æ„å‘³ç€åŸå§‹è¾“å…¥ä¸­å¾®å°çš„è¯¯å·®æˆ–å™ªå£°ï¼Œåœ¨ç»è¿‡è¿™ä¸ªçŸ©é˜µå±‚ä¹‹åï¼Œå¯èƒ½ä¼šè¢«æ˜¾è‘—æ”¾å¤§ï¼Œå¯¼è‡´è¾“å‡ºç»“æœçš„å‰§çƒˆå˜åŒ–ã€‚\n",
    "\n",
    "æ€»ç»“æ¥è¯´ï¼š\n",
    "\n",
    "å¤§ $|W|_2$ æ„å‘³ç€å¼ºæ”¾å¤§æ•ˆåº”ï¼šçŸ©é˜µ $W$ æœ‰èƒ½åŠ›å°†æŸäº›æ–¹å‘ä¸Šçš„è¾“å…¥å‘é‡æ”¾å¤§å¾ˆå¤šå€ã€‚\n",
    "è¯¯å·®ä¼ æ’­ï¼šå½“è¾“å…¥å­˜åœ¨å°æ‰°åŠ¨æ—¶ï¼ˆæ¯”å¦‚é‡åŒ–è¯¯å·®ã€æµ®ç‚¹æ•°è¯¯å·®æˆ–ä¼ æ„Ÿå™¨å™ªå£°ï¼‰ï¼Œè¿™ä¸ªæ‰°åŠ¨ä¼šè¢« $|W|_2$ çš„å¤§å°æ‰€æ”¾å¤§ï¼Œä»è€Œå¯¼è‡´æœ€ç»ˆè¾“å‡ºçš„è¯¯å·®ä¹Ÿå¾ˆå¤§ã€‚\n",
    "åœ¨æ¨ç†ç³»ç»Ÿä¸­ï¼Œå°¤å…¶æ˜¯åœ¨é‡åŒ–æˆ–ä½ç²¾åº¦è®¡ç®—æ—¶ï¼Œå¦‚æœæ¨¡å‹çš„æŸä¸ªæƒé‡çŸ©é˜µçš„è°±èŒƒæ•°è¿‡å¤§ï¼Œé‚£ä¹ˆè¯¥å±‚å°†å¯¹é‡åŒ–å™ªå£°ã€æ•°å€¼èˆå…¥è¯¯å·®éå¸¸æ•æ„Ÿï¼Œå®¹æ˜“å¯¼è‡´ç²¾åº¦å¤§å¹…ä¸‹é™æˆ–è¾“å‡ºä¸ç¨³å®šã€‚å› æ­¤ï¼Œåœ¨æ¨¡å‹è®¾è®¡æˆ–ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæœ‰æ—¶ä¼šé€šè¿‡æ­£åˆ™åŒ–ç­‰æ–¹å¼æ¥é™åˆ¶æƒé‡çš„è°±èŒƒæ•°ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ•°å€¼ç¨³å®šæ€§ã€‚\n",
    "- åœ¨é‡åŒ–/å‰ªææ—¶ï¼Œè¿™ç§å±‚æ›´å®¹æ˜“å‡ºç°æ•°å€¼ä¸ç¨³å®š / è¾“å‡ºæ¥å›æŠ–åŠ¨  \n",
    "- æœ‰äº›é²æ£’æ€§åˆ†æä¼šç”¨è°±èŒƒæ•°åš Lipschitz å¸¸æ•°çš„ä¸Šç•Œ\n",
    "\n",
    "**è§£é‡Š**ï¼š\n",
    "\n",
    "*ä»€ä¹ˆæ˜¯ Lipschitz å¸¸æ•°*ï¼š åœ¨æ•°å­¦ä¸­ï¼Œä¸€ä¸ªå‡½æ•° $f: X \\to Y$ æ˜¯ Lipschitz è¿ç»­çš„ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªå¸¸æ•° $K \\ge 0$ï¼Œä½¿å¾—å¯¹äº $X$ åŸŸä¸­çš„ä»»æ„ä¸¤ä¸ªç‚¹ $x_1$ å’Œ $x_2$ï¼Œéƒ½æœ‰ï¼š\n",
    "\n",
    "$$\\|f(x_1) - f(x_2)\\| \\le K \\|x_1 - x_2\\|$$\n",
    "è¿™ä¸ªå¸¸æ•° $K$ å°±è¢«ç§°ä¸ºè¯¥å‡½æ•°çš„ Lipschitz å¸¸æ•°ã€‚è¿™é‡Œçš„ $\\|\\cdot\\|$ è¡¨ç¤ºæŸç§èŒƒæ•°ï¼ˆä¾‹å¦‚å‘é‡çš„æ¬§å‡ é‡Œå¾—èŒƒæ•°æˆ–çŸ©é˜µçš„è°±èŒƒæ•°ï¼‰ã€‚\n",
    "\n",
    "*ç›´è§‚ç†è§£*ï¼š Lipschitz å¸¸æ•° $K$ ç»™å‡ºäº†å‡½æ•° â€œæœ€é™¡å³­â€ çš„ç¨‹åº¦ã€‚å®ƒé™åˆ¶äº†å‡½æ•°å€¼å˜åŒ–çš„é€Ÿç‡ï¼š\n",
    "\n",
    "å¦‚æœ $K$å¾ˆå°ï¼Œå‡½æ•°æ˜¯â€œå¹³ç¼“â€çš„ï¼Œè¾“å‡ºçš„å˜åŒ–ä¸ä¼šæ¯”è¾“å…¥çš„å¾®å°å˜åŒ–å¤§å¤ªå¤šã€‚å®ƒåƒä¸€ä¸ªæ–œç‡æœ‰é™çš„å‡½æ•°ã€‚\n",
    "å¦‚æœ $K$å¾ˆå¤§ï¼Œå‡½æ•°å¯èƒ½æ˜¯â€œé™¡å³­â€çš„ï¼Œè¾“å‡ºçš„å˜åŒ–å¯èƒ½æ¯”è¾“å…¥çš„å¾®å°å˜åŒ–å¤§å¾ˆå¤šã€‚è¿™æ„å‘³ç€å‡½æ•°å¯¹è¾“å…¥çš„å°æ‰°åŠ¨éå¸¸æ•æ„Ÿã€‚\n",
    "\n",
    "*æ•°å€¼ç¨³å®šæ€§ä¸è¯¯å·®ä¼ æ’­*ï¼šå¦‚æœä¸€ä¸ªçŸ©é˜µï¼ˆå¯ä»¥çœ‹ä½œä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼‰çš„è°±èŒƒæ•°å¾ˆå¤§ï¼Œé‚£ä¹ˆå®ƒä¼šå°†è¾“å…¥çš„å¾®å°è¯¯å·®æ”¾å¤§å¾ˆå¤šå€ã€‚ä¸€ä¸ªçº¿æ€§å‡½æ•° $f(x) = Wx$ çš„ Lipschitz å¸¸æ•°å°±æ˜¯å®ƒçš„è°±èŒƒæ•° $|W|_2$ã€‚\n",
    "å¯¹äºéçº¿æ€§å‡½æ•°ï¼ˆå¦‚æ¿€æ´»å‡½æ•°ã€æ•´ä¸ªç¥ç»ç½‘ç»œï¼‰ï¼Œå…¶ Lipschitz å¸¸æ•°é™åˆ¶äº†è¾“å…¥å™ªå£°æˆ–é‡åŒ–è¯¯å·®åœ¨ç½‘ç»œä¸­ä¼ æ’­æ—¶çš„æœ€å¤§æ”¾å¤§å€æ•°ã€‚\n",
    "ä½ Lipschitz å¸¸æ•° çš„æ¨¡å‹é€šå¸¸å…·æœ‰æ›´å¥½çš„ æ•°å€¼ç¨³å®šæ€§å’Œå¯¹æŠ—æ ·æœ¬çš„é²æ£’æ€§ã€‚\n",
    "\n",
    "*æ¨¡å‹å‹ç¼©ä¸é‡åŒ–*ï¼šåœ¨é‡åŒ–æˆ–å‰ªæè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šå¯¹æ¨¡å‹å‚æ•°å¼•å…¥å¾®å°çš„æ‰°åŠ¨ï¼ˆé‡åŒ–è¯¯å·®ï¼‰ã€‚å¦‚æœæŸä¸€å±‚æˆ–æ•´ä¸ªæ¨¡å‹çš„ Lipschitz å¸¸æ•°è¿‡å¤§ï¼Œè¿™äº›å¾®å°æ‰°åŠ¨å°±å¯èƒ½è¢«æ˜¾è‘—æ”¾å¤§ï¼Œå¯¼è‡´æ¨¡å‹ç²¾åº¦æ€¥å‰§ä¸‹é™ã€‚\n",
    "å› æ­¤ï¼Œåœ¨è¿›è¡Œæ¨¡å‹å‹ç¼©æ—¶ï¼Œæœ‰æ—¶ä¼šå…³æ³¨ä¿æŒæˆ–é™åˆ¶æ¨¡å‹çš„ Lipschitz å¸¸æ•°ï¼Œä»¥ç¡®ä¿å‹ç¼©åçš„æ¨¡å‹ä»ç„¶ç¨³å®šä¸”æ€§èƒ½è‰¯å¥½ã€‚åœ¨å®è·µä¸­ï¼Œç›´æ¥è®¡ç®—å¤æ‚æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç²¾ç¡® Lipschitz å¸¸æ•°é€šå¸¸éå¸¸å›°éš¾ï¼ˆNP-hardï¼‰ã€‚\n",
    "ä½†å¯ä»¥é€šè¿‡ä¸€äº›æ–¹æ³•æ¥ ä¼°è®¡å…¶ä¸Šç•Œ æˆ– æ­£åˆ™åŒ– æ¥é—´æ¥é™åˆ¶å®ƒï¼Œä¾‹å¦‚ï¼šè°±èŒƒæ•°æ­£åˆ™åŒ– (Spectral Normalization)ï¼šé™åˆ¶ç¥ç»ç½‘ç»œä¸­æƒé‡çŸ©é˜µçš„è°±èŒƒæ•°ï¼Œä»è€Œé™åˆ¶çº¿æ€§å±‚çš„ Lipschitz å¸¸æ•°ã€‚ æŸäº›æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUï¼‰æ˜¯ 1-Lipschitz çš„ï¼Œæœ‰åŠ©äºæ§åˆ¶ä¼ æ’­çš„æ”¾å¤§ã€‚Batch Normalization æˆ– Layer Normalization ä¹Ÿæœ‰åŠ©äºç¨³å®šæ¿€æ´»çš„åˆ†å¸ƒï¼Œé—´æ¥æ§åˆ¶å‡½æ•°çš„è¡Œä¸ºã€‚æ€»è€Œè¨€ä¹‹ï¼ŒLipschitz å¸¸æ•°æ˜¯è¡¡é‡å‡½æ•°å¹³æ»‘åº¦å’Œå¯¹è¾“å…¥æ‰°åŠ¨æ•æ„Ÿç¨‹åº¦çš„ä¸€ä¸ªé‡è¦æŒ‡æ ‡ã€‚\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.3 $L_1$ å’Œ $L_0$ èŒƒæ•°ï¼ˆç¨€ç–æ€§ï¼‰\n",
    "\n",
    "- å…ƒç´ çº§ $L_1$ï¼š\n",
    "\n",
    "$$\n",
    "\\|W\\|_1 = \\sum_{i,j} |w_{ij}|\n",
    "$$\n",
    "\n",
    "\n",
    "- â€œ$L_0$â€ï¼ˆéé›¶ä¸ªæ•°ï¼‰ï¼š\n",
    "\n",
    "$$\n",
    "\\|W\\|_0 = \\#\\{(i,j) : w_{ij} \\ne 0\\}\n",
    "$$\n",
    "\n",
    "\n",
    "**ã€å·¥ç¨‹è§†è§’ã€‘**  \n",
    "\n",
    "- $L_0$ ç›´æ¥åº¦é‡â€œæœ‰å¤šå°‘å…ƒç´ è¢«ä¿ç•™/å‰ªæ‰â€ï¼Œä½†å¯¹åº”çš„ä¼˜åŒ–é—®é¢˜æ˜¯ NP-hard\n",
    "- é€šå¸¸ç”¨ $L_1$ ä½œä¸ºå‡¸è¿‘ä¼¼ï¼š\n",
    "\n",
    "$$\n",
    "L(W) + \\lambda \\|W\\|_1\n",
    "$$\n",
    "\n",
    "\n",
    "  è®­ç»ƒåå¾ˆå¤šå…ƒç´ è‡ªç„¶è¶‹è¿‘äº 0ï¼Œæ–¹ä¾¿åšåå¤„ç†å‰ªæã€‚\n",
    "\n",
    "**ã€å…·ä½“è§£é‡Šã€‘**\n",
    "1. $L_0$ èŒƒæ•°ï¼šç†æƒ³ä½†éš¾ä¼˜åŒ–\n",
    "å®šä¹‰ï¼š$\\|W\\|_0$ ä¸¥æ ¼æ¥è¯´ä¸æ˜¯ä¸€ä¸ªèŒƒæ•°ï¼Œå®ƒè¡¨ç¤ºçŸ©é˜µ $W$ ä¸­ éé›¶å…ƒç´ çš„ä¸ªæ•°ã€‚\n",
    "ç›®æ ‡ï¼šåœ¨å‰ªæä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–éé›¶å…ƒç´ çš„æ•°é‡ï¼Œä»è€Œè®©æ¨¡å‹å˜å¾—ç¨€ç–ï¼Œè¿™æ ·å¯ä»¥å‡å°‘å­˜å‚¨å’Œè®¡ç®—ã€‚æ‰€ä»¥ä¼˜åŒ–ç›®æ ‡å¯èƒ½æ˜¯è¿™æ ·çš„ï¼š\n",
    "$$\n",
    "\\min_{\\widehat{W}} \\; L(\\widehat{W}) \\;+\\; \\lambda \\,\\|\\widehat{W}\\|_{0}\n",
    "$$\n",
    "\n",
    "é—®é¢˜ï¼š$L_0$ èŒƒæ•°æ˜¯éå‡¸ã€ä¸è¿ç»­çš„ï¼Œè¿™æ„å‘³ç€å®ƒæ²¡æœ‰è‰¯å¥½çš„æ•°å­¦æ€§è´¨ï¼Œå¯¼è‡´ä¼˜åŒ–é—®é¢˜æ˜¯ NP-hard çš„ï¼ˆå³è®¡ç®—ä¸Šéå¸¸å›°éš¾ï¼Œæ²¡æœ‰é«˜æ•ˆçš„å…¨å±€æœ€ä¼˜è§£ç®—æ³•ï¼‰ã€‚ä½ æ— æ³•ç›´æ¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•æ¥æœ€å°åŒ–å®ƒï¼Œå› ä¸ºå®ƒçš„æ¢¯åº¦å‡ ä¹å¤„å¤„ä¸ºé›¶æˆ–ä¸å­˜åœ¨ã€‚\n",
    "\n",
    "2. $L_1$ èŒƒæ•°ï¼šå‡¸ä¸”ä¿ƒè¿›ç¨€ç–æ€§\n",
    "å®šä¹‰ï¼š$\\|W\\|1$ è¡¨ç¤ºçŸ©é˜µ $W$ ä¸­æ‰€æœ‰å…ƒç´ çš„ ç»å¯¹å€¼ä¹‹å’Œï¼š\n",
    "$$\n",
    "\\|W\\|_1 = \\sum_{i,j} |w_{ij}|\n",
    "$$\n",
    "\n",
    "æ€§è´¨ï¼š\n",
    "\n",
    "å‡¸æ€§ï¼š$L_1$ èŒƒæ•°æ˜¯ä¸€ä¸ª å‡¸å‡½æ•°ã€‚è¿™æ„å‘³ç€å½“æˆ‘ä»¬å°†å®ƒä½œä¸ºæ­£åˆ™é¡¹åŠ å…¥æŸå¤±å‡½æ•°æ—¶ï¼Œæ•´ä¸ªä¼˜åŒ–é—®é¢˜ï¼ˆå¦‚æœæ˜¯å‡¸æŸå¤±å‡½æ•°ï¼‰å°†ä»ç„¶æ˜¯å‡¸çš„ï¼Œæˆ–è€…è‡³å°‘æ˜¯æ›´å®¹æ˜“ä¼˜åŒ–çš„ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™åŠå…¶å˜ç§ï¼ˆä¾‹å¦‚æ¬¡æ¢¯åº¦ä¸‹é™ï¼Œå› ä¸º $|x|$ åœ¨ $x=0$ å¤„ä¸å¯å¯¼ï¼‰æ¥æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£æˆ–è¿‘ä¼¼æœ€ä¼˜è§£ã€‚\n",
    "\n",
    "ä¿ƒè¿›ç¨€ç–æ€§ï¼šè™½ç„¶ $L_1$ èŒƒæ•°ä¸ç›´æ¥ç»Ÿè®¡éé›¶å…ƒç´ çš„ä¸ªæ•°ï¼Œä½†å®ƒæœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„æ€§è´¨ï¼šåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œä¸ºäº†æœ€å°åŒ– $\\sum |w_{ij}|$ï¼Œä¼˜åŒ–å™¨å€¾å‘äºå°†è®¸å¤šæƒé‡æ¨å‘ ç²¾ç¡®çš„é›¶ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªéå¸¸å°ä½†éé›¶çš„å€¼ã€‚è¿™æ˜¯å› ä¸º $|x|$ åœ¨ $x=0$ é™„è¿‘æœ‰ä¸€ä¸ªâ€œå°–ç‚¹â€ï¼Œè¿™ä½¿å¾—ä¼˜åŒ–å™¨åœ¨æ¥è¿‘é›¶æ—¶å—åˆ°æ›´å¤§çš„æƒ©ç½šï¼Œä»è€Œä¿ƒä½¿æƒé‡è·¨è¿‡é›¶ç‚¹ã€‚\n",
    "\n",
    "3. ä¸ºä»€ä¹ˆæ˜¯â€œå‡¸è¿‘ä¼¼â€ï¼Ÿ\n",
    "è¿‘ä¼¼ $L_0$ è¡Œä¸ºï¼šè™½ç„¶ $L_1$ ä¸ç­‰äº $L_0$ï¼Œä½†å®ƒèƒ½æœ‰æ•ˆåœ°è¯±å¯¼ç¨€ç–æ€§ï¼Œä»è€Œåœ¨å®è·µä¸­è¿‘ä¼¼è¾¾åˆ°æˆ‘ä»¬ç”¨ $L_0$ è¿½æ±‚çš„æ•ˆæœï¼ˆå³å¾—åˆ°ä¸€ä¸ªæœ‰å¾ˆå¤šé›¶çš„æ¨¡å‹ï¼‰ã€‚\n",
    "å¯ä¼˜åŒ–æ€§ï¼š $L_1$ æ˜¯å‡¸çš„ï¼Œè¿™ä½¿å¾—ä¼˜åŒ–é—®é¢˜å˜å¾—å¯è§£ã€‚å®ƒæä¾›äº†ä¸€ä¸ªæ•°å­¦ä¸Šæ›´å®¹æ˜“å¤„ç†çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨åˆç†çš„æ—¶é—´å†…æ‰¾åˆ°ä¸€ä¸ªå¥½çš„è§£ã€‚æ‰€ä»¥ï¼Œå½“çœ‹åˆ°å‰ªæçš„ä¼˜åŒ–ç›®æ ‡ä¸­åŒ…å« $L_1$ æ­£åˆ™é¡¹æ—¶ï¼Œæ¯”å¦‚ï¼š\n",
    "$$\n",
    "\\min_{\\widehat{W}} \\; L(\\widehat{W}) + \\lambda \\|\\widehat{W}\\|_1\n",
    "$$\n",
    "å®ƒçš„æ„æ€æ˜¯ï¼šæˆ‘ä»¬å¸Œæœ›åœ¨ä¿æŒæ¨¡å‹ç²¾åº¦çš„åŒæ—¶ï¼ˆæœ€å°åŒ– $L(\\widehat{W})$ï¼‰ï¼Œä¹Ÿèƒ½è®©æ¨¡å‹çš„æƒé‡å°½å¯èƒ½å°ï¼Œå¹¶ä¸”å€¾å‘äºé›¶ï¼ˆæœ€å°åŒ– $\\lambda |\\widehat{W}|_1$ï¼‰ï¼Œä»è€Œè¾¾åˆ°ç¨€ç–åŒ–çš„ç›®çš„ã€‚è¿™ç§æ–¹æ³•æ˜¯å®è·µä¸­å®ç°æ¨¡å‹ç¨€ç–åŒ–ï¼ˆå°¤å…¶æ˜¯éç»“æ„åŒ–å‰ªæï¼‰éå¸¸å¸¸è§ä¸”æœ‰æ•ˆçš„æ–¹å¼ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.4 åœ¨å·¥ç¨‹ä¸­å¦‚ä½•å®é™…ä½¿ç”¨è¿™äº›èŒƒæ•°ï¼Ÿ\n",
    "\n",
    "1. **åˆ¤æ–­æŸå±‚æ˜¯å¦é€‚åˆä½ç§©åˆ†è§£**  \n",
    "   - è®¡ç®—å¥‡å¼‚å€¼ $\\{\\sigma_i\\}$ï¼Œçœ‹å‰ k ä¸ªæ˜¯å¦å·²ç»å åˆ°æ•´ä½“èƒ½é‡çš„ç»å¤§éƒ¨åˆ†ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^k \\sigma_i^2}{\\sum_{i=1}^r \\sigma_i^2} \\approx 0.95 \\text{ æˆ–æ›´é«˜}\n",
    "$$\n",
    "\n",
    "\n",
    "2. **è®¾è®¡å‰ªæç­–ç•¥**  \n",
    "   - è‹¥æŸå±‚æƒé‡çš„ $L_1$ åˆ†å¸ƒæä¸å‡åŒ€ï¼ˆå¤§é‡æ¥è¿‘ 0 çš„å…ƒç´ ï¼‰ï¼Œè¯´æ˜å­˜åœ¨è‡ªç„¶ç¨€ç–æ€§ â†’ å¯ä»¥å®‰å…¨å‰ªæ\n",
    "   - å¯ä»¥å¯¹æ¯ä¸€è¡Œ/æ¯ä¸€åˆ—è®¡ç®— $L_1$ æˆ– $L_2$ normï¼Œä½œä¸ºâ€œé€šé“é‡è¦æ€§â€çš„æŒ‡æ ‡\n",
    "\n",
    "3. **é‡åŒ–æ•æ„Ÿæ€§åˆ†æ**  \n",
    "   - å¯¹äºè°±èŒƒæ•°è¾ƒå¤§çš„å±‚ï¼Œå¯ä»¥è€ƒè™‘ï¼š\n",
    "     - ä½¿ç”¨æ›´é«˜ bit çš„é‡åŒ–ï¼ˆä¾‹å¦‚ 8-bit è€Œä¸æ˜¯ 4-bitï¼‰\n",
    "     - æˆ–åšæ›´ç»†è‡´çš„ per-channel scaling æ¥å‡å°é‡åŒ–è¯¯å·®\n",
    "\n",
    "> æ€»ç»“ï¼šèŒƒæ•°å¸®åŠ©ä½ ä»â€œæ‹è„‘è¢‹é€‰å±‚/é€‰ rank/é€‰ bitâ€  \n",
    "> å˜æˆâ€œæœ‰æŒ‡æ ‡ã€æœ‰ä¾æ®çš„å·¥ç¨‹å†³ç­–â€ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba75d04",
   "metadata": {},
   "source": [
    "### 1.4.5 ä¾‹å­\n",
    "ä¸‰ç±»èŒƒæ•°çš„å·¥ç¨‹å†³ç­–æ•´åˆä¾‹å­ã€‚\n",
    "å‡è®¾æœ‰ä¸€ä¸ª Transformer FFN å±‚ ğ‘Š\n",
    "è®¡ç®—ç»“æœå¦‚ä¸‹\n",
    "\n",
    "| æŒ‡æ ‡           | æ•°å€¼           | è§£é‡Š                |\n",
    "| ------------ | ------------ | ----------------- |\n",
    "| å¥‡å¼‚å€¼èƒ½é‡ E(128) | 96.3%        | å¾ˆé€‚åˆä½ç§©åˆ†è§£ï¼ˆrank=128ï¼‰ |\n",
    "| å¹³å‡ L1 norm   | å¤§é‡è¡Œåœ¨ 0.01 ä»¥ä¸‹ | æœ‰ç¨€ç–æ€§ï¼Œå¯åšå‰ªæ         |\n",
    "| æœ€å¤§å¥‡å¼‚å€¼ï¼ˆè°±èŒƒæ•°ï¼‰   | 780          | é‡åŒ–æ•æ„Ÿï¼Œä¸è¦ç”¨å¤ªä½ bit    |\n",
    "\n",
    "æœ€ç»ˆå·¥ç¨‹å†³ç­–å¯èƒ½æ˜¯\n",
    "ğŸ”¹ ç¬¬ä¸€æ­¥ï¼šä½ç§©åˆ†è§£\n",
    "\n",
    "æŠŠ 4096Ã—4096 é™åˆ° rank=128\n",
    "â†’ å‚æ•°ä» 16M â†’ 1M\n",
    "â†’ FLOPs ä» 33M â†’ 2M\n",
    "\n",
    "ğŸ”¹ ç¬¬äºŒæ­¥ï¼šç»“æ„åŒ–å‰ªæ\n",
    "\n",
    "ç”±äº L1 åˆ†å¸ƒç¨€ç– â†’ å»æ‰ 20% æ— ç”¨é€šé“\n",
    "â†’ é€šé“æ•°ä» 4096 â†’ 3276\n",
    "\n",
    "ğŸ”¹ ç¬¬ä¸‰æ­¥ï¼šé‡åŒ–\n",
    "\n",
    "å› ä¸ºè°±èŒƒæ•°å¤§ â†’ ä¸èƒ½ç”¨ INT4\n",
    "âœ” é€‰ FP16 æˆ– INT8-per-channel\n",
    "\n",
    "æœ€ç»ˆæ•ˆæœï¼š\n",
    "\n",
    "æ¨¡å‹å¤§å°å‡å°‘çº¦ 16Ã— Ã— 1.25Ã— = 20Ã—\n",
    "\n",
    "FLOPs é™åˆ°åŸæ¥çš„ 1/10~1/15\n",
    "\n",
    "å»¶è¿Ÿæ˜¾è‘—é™ä½ï¼Œæ²¡æœ‰æ˜æ˜¾ç²¾åº¦æŸå¤±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "900b032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Analyzing layer: conv1\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (64, 3, 7, 7)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 64\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 58\n",
      "  k (capped by 0.50*min_dim) = 32\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (2.676, 2.822, 2.921)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 0.4857\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer1.0.conv1\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (64, 64, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 64\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 62\n",
      "  k (capped by 0.50*min_dim) = 32\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (25.84, 26.56, 26.97)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.882\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer1.0.conv2\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (64, 64, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 64\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 62\n",
      "  k (capped by 0.50*min_dim) = 32\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (25.75, 26.32, 26.91)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.857\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer1.1.conv1\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (64, 64, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 64\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 62\n",
      "  k (capped by 0.50*min_dim) = 32\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (26.29, 26.63, 27.24)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.851\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer1.1.conv2\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (64, 64, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 64\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 62\n",
      "  k (capped by 0.50*min_dim) = 32\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (26.04, 26.47, 27.04)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.878\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer2.0.conv1\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (128, 64, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 128\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 121\n",
      "  k (capped by 0.50*min_dim) = 64\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (18.44, 18.77, 19.15)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.441\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer2.0.conv2\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (128, 128, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 128\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 123\n",
      "  k (capped by 0.50*min_dim) = 64\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (37.12, 37.74, 38.36)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.868\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer2.0.downsample.0\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (128, 64, 1, 1)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 64\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 57\n",
      "  k (capped by 0.50*min_dim) = 32\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (5.658, 6.017, 6.47)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 2.392\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer2.1.conv1\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (128, 128, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 128\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 123\n",
      "  k (capped by 0.50*min_dim) = 64\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (37.33, 37.85, 38.44)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.861\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer2.1.conv2\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (128, 128, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 128\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 123\n",
      "  k (capped by 0.50*min_dim) = 64\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (37.32, 37.65, 38.46)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.861\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer3.0.conv1\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (256, 128, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 256\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 241\n",
      "  k (capped by 0.50*min_dim) = 128\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (26.31, 26.74, 27.15)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.452\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer3.0.conv2\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (256, 256, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 256\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 246\n",
      "  k (capped by 0.50*min_dim) = 128\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (53.08, 53.6, 54.17)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.881\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer3.0.downsample.0\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (256, 128, 1, 1)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 128\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 112\n",
      "  k (capped by 0.50*min_dim) = 64\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (8.266, 8.598, 9.125)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 2.392\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer3.1.conv1\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (256, 256, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 256\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 246\n",
      "  k (capped by 0.50*min_dim) = 128\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (53.08, 53.55, 54.17)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.876\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer3.1.conv2\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (256, 256, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 256\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 246\n",
      "  k (capped by 0.50*min_dim) = 128\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (52.9, 53.63, 54.21)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.871\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer4.0.conv1\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (512, 256, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 512\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 482\n",
      "  k (capped by 0.50*min_dim) = 256\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (37.58, 37.85, 38.24)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.46\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer4.0.conv2\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (512, 512, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 512\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 491\n",
      "  k (capped by 0.50*min_dim) = 256\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (75.47, 76, 76.56)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.88\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer4.0.downsample.0\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (512, 256, 1, 1)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 256\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 225\n",
      "  k (capped by 0.50*min_dim) = 128\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (11.96, 12.32, 12.73)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 2.387\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer4.1.conv1\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (512, 512, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 512\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 491\n",
      "  k (capped by 0.50*min_dim) = 256\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (75.57, 76.12, 76.63)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.887\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: layer4.1.conv2\n",
      "============================================================\n",
      "Layer: Conv2d, weight shape = (512, 512, 3, 3)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 512\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 491\n",
      "  k (capped by 0.50*min_dim) = 256\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (75.38, 75.92, 76.5)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.873\n",
      "  Recommended quant bits = 4-bit\n",
      "\n",
      ">>> Analyzing layer: fc\n",
      "============================================================\n",
      "Layer: Linear, weight shape = (1000, 512)\n",
      "--- Low-rank analysis ---\n",
      "  min(out_dim, in_dim)  = 512\n",
      "  target energy         = 0.980\n",
      "  k (reach target)      = 448\n",
      "  k (capped by 0.50*min_dim) = 256\n",
      "  Compression ratio     â‰ˆ 2.00x (in FLOPs/params)\n",
      "\n",
      "--- L1 sparsity / pruning analysis ---\n",
      "  L1 per-out quantiles  (10%,25%,50%) = (10.91, 11.09, 11.32)\n",
      "  Recommended prune ratio (by channels) â‰ˆ 0.0%\n",
      "\n",
      "--- Quantization sensitivity ---\n",
      "  Spectral norm ||W||_2 = 1.375\n",
      "  Recommended quant bits = 4-bit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def get_weight_2d(layer: nn.Module):\n",
    "    \"\"\"\n",
    "    flatten the weight of a layer into 2D matrix\n",
    "    \"\"\"\n",
    "    W = layer.weight.detach().float().clone()\n",
    "\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        # [out_features, in_features]\n",
    "        W2d = W  # already 2D\n",
    "    elif isinstance(layer, nn.Conv2d):\n",
    "        # [out_channels, in_channels, kh, kw] -> [out_channels, in_channels*kh*kw]\n",
    "        out_c, in_c, kh, kw = W.shape\n",
    "        W2d = W.view(out_c, in_c * kh * kw)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported layer type: {type(layer)}\")\n",
    "\n",
    "    return W2d\n",
    "\n",
    "\n",
    "def analyze_layer(layer: nn.Module,\n",
    "                  energy_target: float = 0.98,\n",
    "                  max_rank_ratio: float = 0.5,\n",
    "                  max_prune_ratio: float = 0.5):\n",
    "    \"\"\"\n",
    "    for a given layer,\n",
    "    - SVD analysis â†’ recommend low-rank\n",
    "    - L1 distribution analysis â†’ recommend pruning ratio (by output channels)\n",
    "    - Spectral norm analysis â†’ recommend quantization bits\n",
    "    \"\"\"\n",
    "    W2d = get_weight_2d(layer)                  # [out_dim, in_dim]\n",
    "    out_dim, in_dim = W2d.shape\n",
    "\n",
    "    # ===== 1. SVD & rank recommendation =====\n",
    "    # only need singular values, and skip U and V\n",
    "    # note: for very large matrices, consider using randomized\n",
    "    sigma = torch.linalg.svdvals(W2d)           # [min(out_dim, in_dim)]\n",
    "    sigma2 = sigma ** 2\n",
    "    total_energy = sigma2.sum()\n",
    "    cum_energy = torch.cumsum(sigma2, dim=0) / total_energy\n",
    "\n",
    "    # Find the smallest k such that cum_energy[k-1] >= energy_target\n",
    "    k_candidate = int((cum_energy >= energy_target).nonzero(as_tuple=True)[0][0].item() + 1)\n",
    "    \n",
    "    # Do not exceed max_rank_ratio * min_dim to prevent keeping too many components\n",
    "    max_rank = int(min(out_dim, in_dim) * max_rank_ratio)\n",
    "    k_recommended = min(k_candidate, max_rank)\n",
    "\n",
    "    # ===== 2. L1 distribution & pruning ratio recommendation (by output channels) =====\n",
    "    # compute L1 norm for each output channel\n",
    "    l1_per_out = W2d.abs().sum(dim=1)          # [out_dim]\n",
    "    l1_np = l1_per_out.cpu().numpy()\n",
    "    # compute several percentiles to sense sparsity\n",
    "    q10, q25, q50 = np.percentile(l1_np, [10, 25, 50])\n",
    "\n",
    "    # If the gap between the low percentile and the median is large,\n",
    "    # it indicates that the tail channels are not important and can be pruned.\n",
    "    # Simple heuristic: check if the 25% percentile is significantly smaller than the 50% percentile.\n",
    "    prune_ratio = 0.0\n",
    "    if q25 < 0.3 * q50:\n",
    "        # very sparse, can prune up to 30%\n",
    "        prune_ratio = min(0.3, max_prune_ratio)\n",
    "    elif q25 < 0.5 * q50:\n",
    "        # slightly sparse, can prune up to 15%\n",
    "        prune_ratio = min(0.15, max_prune_ratio)\n",
    "    else:\n",
    "        prune_ratio = 0.0  # ä¸å»ºè®®å‰ª\n",
    "\n",
    "    # ===== Spectral norm & quantization bits recommendation =====\n",
    "    spectral_norm = sigma.max().item()\n",
    "\n",
    "    # Very rough heuristic:\n",
    "    if spectral_norm < 5:\n",
    "        quant_bits = 4 \n",
    "    elif spectral_norm < 20:\n",
    "        quant_bits = 8 \n",
    "    else:\n",
    "        # Recommended minimally 8 bits (or even 16 bits) with per-channel scaling\n",
    "        quant_bits = 8  \n",
    "\n",
    "    # print results\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Layer: {layer.__class__.__name__}, weight shape = {tuple(layer.weight.shape)}\")\n",
    "    print(\"--- Low-rank analysis ---\")\n",
    "    print(f\"  min(out_dim, in_dim)  = {min(out_dim, in_dim)}\")\n",
    "    print(f\"  target energy         = {energy_target:.3f}\")\n",
    "    print(f\"  k (reach target)      = {k_candidate}\")\n",
    "    print(f\"  k (capped by {max_rank_ratio:.2f}*min_dim) = {k_recommended}\")\n",
    "    print(f\"  Compression ratio     â‰ˆ {min(out_dim, in_dim) / k_recommended:.2f}x (in FLOPs/params)\")\n",
    "\n",
    "    print(\"\\n--- L1 sparsity / pruning analysis ---\")\n",
    "    print(f\"  L1 per-out quantiles  (10%,25%,50%) = ({q10:.4g}, {q25:.4g}, {q50:.4g})\")\n",
    "    print(f\"  Recommended prune ratio (by channels) â‰ˆ {prune_ratio*100:.1f}%\")\n",
    "\n",
    "    print(\"\\n--- Quantization sensitivity ---\")\n",
    "    print(f\"  Spectral norm ||W||_2 = {spectral_norm:.4g}\")\n",
    "    print(f\"  Recommended quant bits = {quant_bits}-bit\")\n",
    "\n",
    "    return {\n",
    "        \"sigma\": sigma,\n",
    "        \"cum_energy\": cum_energy,\n",
    "        \"recommended_rank\": k_recommended,\n",
    "        \"recommended_prune_ratio\": prune_ratio,\n",
    "        \"spectral_norm\": spectral_norm,\n",
    "        \"recommended_quant_bits\": quant_bits,\n",
    "    }\n",
    "\n",
    "\n",
    "try:\n",
    "    from torchvision.models import resnet18\n",
    "    # load no pre-trained weights\n",
    "    model = resnet18(weights=None)  \n",
    "except Exception as e:\n",
    "    raise e\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Traverse all sub-modules\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        print(f\"\\n>>> Analyzing layer: {name}\")\n",
    "        analyze_layer(module, energy_target=0.98, max_rank_ratio=0.5, max_prune_ratio=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94b322",
   "metadata": {},
   "source": [
    "## 1.5 å·ç§¯çš„çº¿æ€§ä»£æ•°è¡¨è¾¾ï¼šConv = MatMul\n",
    "\n",
    "å·ç§¯å±‚ï¼ˆConv2dï¼‰æ˜¯ CNN å’Œå¾ˆå¤šè§†è§‰æ¨¡å‹çš„æ ¸å¿ƒã€‚  \n",
    "ä»æ•°å­¦ä¸Šï¼Œå®ƒæ˜¯ä¸€ä¸ªå¸¦æœ‰ç»“æ„çš„çº¿æ€§ç®—å­ï¼š\n",
    "\n",
    "$$\n",
    "y = K * x\n",
    "$$\n",
    "\n",
    "\n",
    "- $x$ï¼šè¾“å…¥ç‰¹å¾å›¾ï¼ˆå¦‚ $\\mathbb{R}^{C_{\\text{in}} \\times H \\times W}$ï¼‰  \n",
    "- $K$ï¼šå·ç§¯æ ¸ï¼ˆå¦‚ $\\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times k_h \\times k_w}$ï¼‰\n",
    "\n",
    "é€šè¿‡é€‚å½“å±•å¼€å’Œé‡æ’ï¼Œå¯ä»¥æŠŠå®ƒæ”¹å†™ä¸º**çŸ©é˜µä¹˜æ³•**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5.1 im2colï¼šæŠŠå±€éƒ¨ patch å±•æˆåˆ—å‘é‡\n",
    "\n",
    "ä»¥å•é€šé“ã€kernel=2Ã—2ã€stride=1 ä¸ºä¾‹ï¼š\n",
    "\n",
    "è¾“å…¥ï¼š\n",
    "\n",
    "```text\n",
    "1 2 3\n",
    "4 5 6\n",
    "7 8 9\n",
    "```\n",
    "\n",
    "å·ç§¯æ ¸ï¼š\n",
    "\n",
    "```text\n",
    "k11 k12\n",
    "k21 k22\n",
    "```\n",
    "\n",
    "æ‰€æœ‰ 2Ã—2 patchï¼š\n",
    "\n",
    "1. ä¸Šå·¦ï¼š$\\{1,2,4,5\\}$\n",
    "2. ä¸Šå³ï¼š$\\{2,3,5,6\\}$\n",
    "3. ä¸‹å·¦ï¼š$\\{4,5,7,8\\}$\n",
    "4. ä¸‹å³ï¼š$\\{5,6,8,9\\}$\n",
    "\n",
    "æˆ‘ä»¬æŠŠæ¯ä¸ª patch æŒ‰å›ºå®šé¡ºåºå±•å¼€æˆåˆ—å‘é‡ï¼ˆä¹Ÿå°±æ˜¯ä»å·¦ä¸Šå¼€å§‹ï¼Œçª—å£å¤§å° 2Ã—2ï¼Œæ°´å¹³/å‚ç›´å„æ»‘åŠ¨ä¸€æ­¥ï¼Œæšä¸¾æ‰€æœ‰å¯èƒ½çš„ 2Ã—2 patchï¼‰ï¼š\n",
    "\n",
    "$$\n",
    "x_1 = [1,2,4,5]^\\top, \\quad\n",
    "x_2 = [2,3,5,6]^\\top, \\quad\n",
    "x_3 = [4,5,7,8]^\\top, \\quad\n",
    "x_4 = [5,6,8,9]^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "ç„¶åç»„æˆä¸€ä¸ªçŸ©é˜µ $X_{\\text{col}} = \\{x_1, x_2, x_3, x_4\\}$ï¼š\n",
    "\n",
    "$$\n",
    "X_{\\text{col}} =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 4 & 5 \\\\\n",
    "2 & 3 & 5 & 6 \\\\\n",
    "4 & 5 & 7 & 8 \\\\\n",
    "5 & 6 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{4 \\times 4}\n",
    "$$\n",
    "\n",
    "\n",
    "å·ç§¯æ ¸ä¹Ÿå±•å¼€æˆå‘é‡ï¼š\n",
    "\n",
    "$$\n",
    "w = [k_{11}, k_{12}, k_{21}, k_{22}]\n",
    "$$\n",
    "\n",
    "\n",
    "åˆ™æ¯ä¸ªè¾“å‡ºä½ç½®ï¼š\n",
    "\n",
    "$$\n",
    "y_i = w x_i\n",
    "$$\n",
    "\n",
    "\n",
    "æ‰€æœ‰ä½ç½®ä¸€èµ·å†™æˆçŸ©é˜µå½¢å¼ï¼š\n",
    "\n",
    "$$\n",
    "y^\\top = w X_{\\text{col}}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5.2 å¤šé€šé“ / å¤šå·ç§¯æ ¸çš„æƒ…å†µ\n",
    "\n",
    "å¯¹äºä¸€èˆ¬æƒ…å†µï¼š\n",
    "\n",
    "- è¾“å…¥ï¼š$x \\in \\mathbb{R}^{C_{\\text{in}} \\times H \\times W}$\n",
    "- å·ç§¯æ ¸ï¼š$K \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times k_h \\times k_w}$\n",
    "\n",
    "é€šè¿‡ im2colï¼š\n",
    "\n",
    "- ç”ŸæˆçŸ©é˜µ $X_{\\text{col}} \\in \\mathbb{R}^{(C_{\\text{in}} k_h k_w) \\times L}$  \n",
    "  å…¶ä¸­ L æ˜¯æ»‘çª—æ•°é‡ï¼ˆè¾“å‡ºç©ºé—´å°ºå¯¸çš„ä¹˜ç§¯ï¼‰\n",
    "- å·ç§¯æ ¸å‹å¹³æˆçŸ©é˜µ $W \\in \\mathbb{R}^{C_{\\text{out}} \\times (C_{\\text{in}} k_h k_w)}$\n",
    "\n",
    "åˆ™å·ç§¯å¯ä»¥å†™æˆï¼š\n",
    "\n",
    "$$\n",
    "Y = W X_{\\text{col}}\n",
    "$$\n",
    "\n",
    "\n",
    "- $Y \\in \\mathbb{R}^{C_{\\text{out}} \\times L}$ï¼Œä¹‹å reshape å›è¾“å‡ºå°ºå¯¸\n",
    "\n",
    "**ã€æ ¸å¿ƒç»“è®ºã€‘**  \n",
    "> Conv2d ç»è¿‡ im2col å˜æ¢åï¼Œæœ¬è´¨å°±æ˜¯ä¸€ä¸ª GEMMã€‚\n",
    "\n",
    "---\n",
    "### 1.5.3 å¤šæ ¸å¤šé€šé“ä¾‹å­\n",
    "è®¾ï¼š\n",
    "- è¾“å…¥ç‰¹å¾å›¾ï¼š$x \\in \\mathbb{R}^{C_{\\text{in}} \\times H \\times W},\\quad\n",
    "C_{\\text{in}} = 2,\\ H=W=3$\n",
    "- å·ç§¯æ ¸ï¼š $K \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times k_h \\times k_w},\\quad\n",
    "C_{\\text{out}} = 2,\\ k_h=k_w=2$\n",
    "- stride = 1, padding = 0\n",
    "- è¾“å‡ºç©ºé—´å°ºå¯¸ï¼š$H_{\\text{out}} = W_{\\text{out}} = 3-2+1 = 2$\n",
    "æ‰€ä»¥ä¸€å…± $L = H_{\\text{out}} \\cdot W_{\\text{out}} = 4$ ä¸ªè¾“å‡ºä½ç½®ã€‚\n",
    "---\n",
    "1. è¾“å…¥æ•°æ®ï¼ˆ2 ä¸ªé€šé“ï¼‰\n",
    "- ç¬¬ 1 ä¸ªé€šé“ï¼ˆc=0ï¼‰ï¼š\n",
    "$\\begin{array}{c} x^{(0)} =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix} \\end{array}$\n",
    "\n",
    "- ç¬¬ 2 ä¸ªé€šé“ï¼ˆc=1ï¼‰ï¼š\n",
    "$\\begin{array}{c} x^{(1)} =\n",
    "\\begin{bmatrix}\n",
    "10 & 11 & 12 \\\\\n",
    "13 & 14 & 15 \\\\\n",
    "16 & 17 & 18\n",
    "\\end{bmatrix} \\end{array}$\n",
    "---\n",
    "2. å·ç§¯æ ¸ï¼ˆ2 ä¸ªè¾“å‡ºé€šé“ = 2 ä¸ª filterï¼‰\n",
    "æ¯ä¸ªå·ç§¯æ ¸å¯¹ä¸¤ä¸ªè¾“å…¥é€šé“éƒ½æœ‰è‡ªå·±çš„ 2Ã—2 kernelï¼š\n",
    "- ç¬¬ 1 ä¸ªè¾“å‡ºé€šé“ï¼ˆfilter 0ï¼‰ï¼š\n",
    "$\\begin{array}{c} K^{(0,0)} =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "K^{(0,1)} =\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix} \\end{array}$\n",
    "\n",
    "- ç¬¬ 2 ä¸ªè¾“å‡ºé€šé“ï¼ˆfilter 1ï¼‰ï¼š\n",
    "$\\begin{array}{c} K^{(1,0)} =\n",
    "\\begin{bmatrix}\n",
    "c_{11} & c_{12} \\\\\n",
    "c_{21} & c_{22}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "K^{(1,1)} =\n",
    "\\begin{bmatrix}\n",
    "d_{11} & d_{12} \\\\\n",
    "d_{21} & d_{22}\n",
    "\\end{bmatrix} \\end{array}$\n",
    "\n",
    "è¿™é‡Œ $(0,0)$  è¡¨ç¤ºï¼šè¾“å‡ºé€šé“ 0ï¼Œå¯¹è¾“å…¥é€šé“ 0 çš„ kernelï¼Œ $(0,1)$ è¡¨ç¤ºï¼šè¾“å‡ºé€šé“ 0ï¼Œå¯¹è¾“å…¥é€šé“ 1 çš„ kernelï¼Œä¾æ­¤ç±»æ¨ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "3. ç”¨ im2col å±•å¼€å¤šé€šé“ patch â†’ åˆ—å‘é‡\n",
    "è¾“å‡ºç©ºé—´æ˜¯ 2Ã—2ï¼Œæ‰€ä»¥æœ‰ 4 ä¸ªæ»‘çª—ä½ç½®ï¼š\n",
    "\n",
    "- å·¦ä¸Šï¼šè¦†ç›–è¾“å…¥åæ ‡ (0:2, 0:2)\n",
    "\n",
    "- å³ä¸Šï¼šè¦†ç›– (0:2, 1:3)\n",
    "\n",
    "- å·¦ä¸‹ï¼šè¦†ç›– (1:3, 0:2)\n",
    "\n",
    "- å³ä¸‹ï¼šè¦†ç›– (1:3, 1:3)\n",
    "\n",
    "å¯¹æ¯ä¸ªä½ç½®ï¼Œæˆ‘ä»¬è¦ä» ä¸¤ä¸ªé€šé“ å„å–ä¸€ä¸ª 2Ã—2 patchï¼Œç„¶åå…¨éƒ¨æ‘Šå¹³æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªé•¿åº¦ä¸º $C_{\\text{in}} \\cdot k_h \\cdot k_w = 2 \\cdot 2 \\cdot 2 = 8$ çš„åˆ—å‘é‡ã€‚\n",
    "\n",
    "ğŸ”¹ ä½ç½® 1ï¼šå·¦ä¸Š patchï¼ˆtop-leftï¼‰\n",
    "- é€šé“ 0 çš„ 2Ã—2 patchï¼š\n",
    "$\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "4 & 5\n",
    "\\end{bmatrix}$\n",
    "- é€šé“ 1 çš„ 2Ã—2 patchï¼š\n",
    "$\\begin{bmatrix}\n",
    "10 & 11 \\\\\n",
    "13 & 14\n",
    "\\end{bmatrix}$\n",
    "\n",
    "æŒ‰å›ºå®šé¡ºåºï¼ˆæ¯”å¦‚ï¼šé€šé“é¡ºåºä¼˜å…ˆï¼Œæ¯ä¸ª patch æŒ‰è¡Œå±•å¹³ï¼‰æ‘Šå¹³å¹¶æ‹¼æ¥ï¼š\n",
    "$\\begin{array}{c} x_{\\text{col},1} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 2 \\\\ 4 \\\\ 5 \\\\ 10 \\\\ 11 \\\\ 13 \\\\ 14\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{8} \\end{array}$\n",
    "\n",
    "ğŸ”¹ ä½ç½® 2ï¼šå³ä¸Š patchï¼ˆtop-rightï¼‰\n",
    "- é€šé“ 0ï¼š$\\begin{bmatrix} 2 & 3 \\\\ 5 & 6 \\end{bmatrix}$\n",
    "- é€šé“ 1ï¼š$\\begin{bmatrix} 11 & 12 \\\\ 14 & 15 \\end{bmatrix}$\n",
    "å±•å¼€ï¼š\n",
    "$\\begin{array}{c} x_{\\text{col},2} =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\ 3 \\\\ 5 \\\\ 6 \\\\ 11 \\\\ 12 \\\\ 14 \\\\ 15\n",
    "\\end{bmatrix} \\end{array}$\n",
    "\n",
    "ğŸ”¹ ä½ç½® 3ï¼šå·¦ä¸‹ patchï¼ˆbottom-leftï¼‰\n",
    "- é€šé“ 0ï¼š$\\begin{bmatrix} 4 & 5 \\\\ 7 & 8 \\end{bmatrix}$\n",
    "- é€šé“ 1ï¼š$\\begin{bmatrix} 13 & 14 \\\\ 16 & 17 \\end{bmatrix}$\n",
    "å±•å¼€ï¼š\n",
    "$\\begin{array}{c} x_{\\text{col},3} =\n",
    "\\begin{bmatrix}\n",
    "4 \\\\ 5 \\\\ 7 \\\\ 8 \\\\ 13 \\\\ 14 \\\\ 16 \\\\ 17\n",
    "\\end{bmatrix} \\end{array}$\n",
    "\n",
    "ğŸ”¹ä½ç½® 4ï¼šå³ä¸‹ patchï¼ˆbottom-rightï¼‰\n",
    "- é€šé“ 0ï¼š$\\begin{bmatrix} 5 & 6 \\\\ 8 & 9 \\end{bmatrix}$\n",
    "- é€šé“ 1ï¼š$\\begin{bmatrix} 14 & 15 \\\\ 17 & 18 \\end{bmatrix}$\n",
    "å±•å¼€ï¼š\n",
    "$\\begin{array}{c} x_{\\text{col},4} =\n",
    "\\begin{bmatrix}\n",
    "5 \\\\ 6 \\\\ 8 \\\\ 9 \\\\ 14 \\\\ 15 \\\\ 17 \\\\ 18\n",
    "\\end{bmatrix} \\end{array}$\n",
    "\n",
    "ğŸ”¹ ç»„åˆæˆ $X_{\\text{col}}$ æŠŠ 4 ä¸ªåˆ—å‘é‡æ‹¼åœ¨ä¸€èµ·ï¼š\n",
    "$\\begin{array}{c} X_{\\text{col}} =\n",
    "\\begin{bmatrix}\n",
    "1  & 2  & 4  & 5  \\\\\n",
    "2  & 3  & 5  & 6  \\\\\n",
    "4  & 5  & 7  & 8  \\\\\n",
    "5  & 6  & 8  & 9  \\\\\n",
    "10 & 11 & 13 & 14 \\\\\n",
    "11 & 12 & 14 & 15 \\\\\n",
    "13 & 14 & 16 & 17 \\\\\n",
    "14 & 15 & 17 & 18\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{8 \\times 4} \\end{array}$\n",
    "\n",
    "---\n",
    "4ã€‚ å·ç§¯æ ¸å±•å¹³æˆçŸ©é˜µ Wï¼ˆå¤šå·ç§¯æ ¸ â†’ å¤šè¡Œï¼‰\n",
    "æ¯ä¸ªè¾“å‡ºé€šé“çš„ filter æ˜¯ ä¸¤ä¸ª 2Ã—2 çš„ kernelï¼ˆå¯¹åº” 2 ä¸ªè¾“å…¥é€šé“ï¼‰ï¼Œæˆ‘ä»¬ä¹ŸæŠŠå®ƒä»¬å±•å¹³å¹¶æ‹¼æ¥ï¼š\n",
    "- å¯¹è¾“å‡ºé€šé“ 0ï¼ˆfilter 0ï¼‰ï¼š\n",
    "$\\begin{array}{c} w^{(0)} =\n",
    "\\begin{bmatrix}\n",
    "a_{11} \\\\ a_{12} \\\\ a_{21} \\\\ a_{22} \\\\\n",
    "b_{11} \\\\ b_{12} \\\\ b_{21} \\\\ b_{22}\n",
    "\\end{bmatrix}^\\top \\end{array}$\tâ€‹\n",
    "- å¯¹è¾“å‡ºé€šé“ 1ï¼ˆfilter 1ï¼‰ï¼š\n",
    "$\\begin{array}{c} w^{(1)} =\n",
    "\\begin{bmatrix}\n",
    "c_{11} \\\\ c_{12} \\\\ c_{21} \\\\ c_{22} \\\\\n",
    "d_{11} \\\\ d_{12} \\\\ d_{21} \\\\ d_{22}\n",
    "\\end{bmatrix}^\\top \\end{array}$\n",
    "\n",
    "- è¿™æ ·ç»„åˆæˆçŸ©é˜µ$W$ï¼š\n",
    "$\\begin{array}{c} W =\n",
    "\\begin{bmatrix}\n",
    "w^{(0)} \\\\\n",
    "w^{(1)}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{2 \\times 8} \\end{array}$\n",
    "---\n",
    "\n",
    "5. ç»Ÿä¸€æˆä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼šY = W X_col\n",
    "ç°åœ¨å·ç§¯è¿ç®—å¯ä»¥å†™æˆï¼š$Y = W X_{\\text{col}}$\n",
    "\n",
    "- $W \\in \\mathbb{R}^{C_{\\text{out}} \\times (C_{\\text{in}} k_h k_w)} = \\mathbb{R}^{2 \\times 8}$\n",
    "- $X_{\\text{col}} \\in \\mathbb{R}^{(C_{\\text{in}} k_h k_w) \\times L} = \\mathbb{R}^{8 \\times 4}$\n",
    "- $Y \\in \\mathbb{R}^{C_{\\text{out}} \\times L} = \\mathbb{R}^{2 \\times 4}$\n",
    "å…¶ä¸­ï¼š\n",
    "- ç¬¬ 1 è¡Œå¯¹åº”è¾“å‡ºé€šé“ 0 åœ¨æ‰€æœ‰ 4 ä¸ªç©ºé—´ä½ç½®çš„è¾“å‡º\n",
    "- ç¬¬ 2 è¡Œå¯¹åº”è¾“å‡ºé€šé“ 1\n",
    "- å†æŠŠæ¯ä¸€è¡Œ reshape å› 2Ã—2ï¼Œå°±å¾—åˆ°æ ‡å‡†çš„ 2Ã—2 feature mapï¼š$Y^{(c)} \\in \\mathbb{R}^{2 \\times 2},\\quad c=0,1$\n",
    "\n",
    "### 1.5.4 åœ¨æ¨ç†åŠ é€Ÿä¸­çš„æ„ä¹‰\n",
    "\n",
    "1. **é‡ç”¨é«˜æ€§èƒ½ GEMM å†…æ ¸**  \n",
    "   - ä¸éœ€è¦ä¸ºæ¯ç§å·ç§¯å½¢çŠ¶å†™å…¨æ–°çš„ kernel  \n",
    "   - åªéœ€å®ç° im2colï¼ˆæˆ–æ›´é«˜çº§çš„å˜æ¢ï¼‰+ è°ƒç”¨ GEMM\n",
    "2. **ç»Ÿä¸€ä¼˜åŒ–ç›®æ ‡**  \n",
    "   - GEMM çš„ tilingã€å‘é‡åŒ–ã€cache åˆ©ç”¨ä¸€æ—¦å†™å¥½ï¼ŒConv ä¹Ÿèƒ½äº«å—\n",
    "3. **æ›´å®¹æ˜“åšé‡åŒ–/å‰ªæ**  \n",
    "   - æƒé‡åœ¨ im2col å½¢å¼ä¸‹å°±æ˜¯ä¸€ä¸ªçŸ©é˜µ Wï¼Œæ–¹ä¾¿ per-channel / per-row é‡åŒ–å’Œå‰ªæ\n",
    "\n",
    "> åœ¨é«˜æ€§èƒ½æ¨ç†åº“ä¸­ï¼ŒConv2d çš„å®ç°é€šå¸¸è¦ä¹ˆï¼š  \n",
    "> - èµ° im2col + GEMM è·¯çº¿ï¼Œ  \n",
    "> - è¦ä¹ˆèµ°ä¸“é—¨çš„ convolution kernelï¼Œä½†å…¶å†…éƒ¨ä»ç„¶å›´ç»•â€œå—çŸ©é˜µä¹˜æ³•â€çš„æ€æƒ³è®¾è®¡ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f545e5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_conv shape: torch.Size([1, 3, 4, 4])\n",
      "x_unfold shape: torch.Size([1, 18, 16])\n",
      "y_im2col shape: torch.Size([1, 3, 4, 4])\n",
      "max |y_conv - y_im2col| = 0.0\n",
      "âœ… Conv2d == im2col + matmul (within numerical tolerance)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ======================\n",
    "# 1. å®šä¹‰ä¸€ä¸ªç®€å•çš„ Conv2d\n",
    "# ======================\n",
    "in_channels = 2\n",
    "out_channels = 3\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "conv = nn.Conv2d(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "# æ„é€ è¾“å…¥ï¼šbatch_size=1, C=2, H=W=4\n",
    "x = torch.randn(1, in_channels, 4, 4)\n",
    "\n",
    "# ç”¨ nn.Conv2d ç›´æ¥ç®—ä¸€éï¼ˆæ ‡å‡†å®ç°ï¼‰\n",
    "y_conv = conv(x)  # å½¢çŠ¶ [1, out_channels, H_out, W_out]\n",
    "print(\"y_conv shape:\", y_conv.shape)\n",
    "\n",
    "# ======================================\n",
    "# 2. ç”¨ im2col + matmul æ‰‹å·¥å®ç°åŒæ ·çš„å·ç§¯\n",
    "# ======================================\n",
    "\n",
    "# unfold ç›¸å½“äº im2colï¼š\n",
    "# x_unfold: [N, C_in * k_h * k_w, L]\n",
    "# å…¶ä¸­ L = H_out * W_out\n",
    "x_unfold = F.unfold(\n",
    "    x,\n",
    "    kernel_size=kernel_size,\n",
    "    dilation=1,\n",
    "    padding=padding,\n",
    "    stride=stride,\n",
    ")\n",
    "# å–ä¸€ä¸‹å±•å¼€åçš„å°ºå¯¸\n",
    "N, K, L = x_unfold.shape  # K = C_in * k_h * k_w\n",
    "print(\"x_unfold shape:\", x_unfold.shape)  # [N, K, L]\n",
    "\n",
    "# å·ç§¯æ ¸æƒé‡ï¼š [C_out, C_in, k_h, k_w] -> [C_out, K]\n",
    "W = conv.weight.view(out_channels, -1)  # [C_out, C_in * k_h * k_w]\n",
    "b = conv.bias                           # [C_out]\n",
    "\n",
    "# å¯¹æ¯ä¸ª batch ç‹¬ç«‹åšçŸ©é˜µä¹˜æ³•ï¼š\n",
    "# å¯¹æŸä¸ªæ ·æœ¬ nï¼š\n",
    "#   x_unfold[n]: [K, L]\n",
    "#   è¾“å‡º feature: [C_out, L] = W @ x_unfold[n] + b\n",
    "y_im2col_list = []\n",
    "for n in range(N):\n",
    "    # [C_out, L] = [C_out, K] @ [K, L]\n",
    "    y_n = W @ x_unfold[n]  # [C_out, L]\n",
    "    # åŠ  biasï¼šåœ¨ç¬¬ 0 ç»´ broadcast\n",
    "    y_n = y_n + b.view(-1, 1)  # [C_out, L]\n",
    "    y_im2col_list.append(y_n)\n",
    "\n",
    "# å †å› batch ç»´ï¼š [N, C_out, L]\n",
    "y_im2col = torch.stack(y_im2col_list, dim=0)\n",
    "\n",
    "# å°† L å±•å› H_out, W_out\n",
    "H_out = y_conv.shape[2]\n",
    "W_out = y_conv.shape[3]\n",
    "y_im2col = y_im2col.view(N, out_channels, H_out, W_out)\n",
    "\n",
    "print(\"y_im2col shape:\", y_im2col.shape)\n",
    "\n",
    "# ============================\n",
    "# 3. å¯¹æ¯” nn.Conv2d ä¸ im2col+matmul\n",
    "# ============================\n",
    "\n",
    "diff = (y_conv - y_im2col).abs().max().item()\n",
    "print(\"max |y_conv - y_im2col| =\", diff)\n",
    "\n",
    "if diff < 1e-5:\n",
    "    print(\"âœ… Conv2d == im2col + matmul (within numerical tolerance)\")\n",
    "else:\n",
    "    print(\"âš ï¸ there is a discrepancy between Conv2d and im2col + matmul!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1f714",
   "metadata": {},
   "source": [
    "## 1.6 Attention çš„çº¿æ€§ä»£æ•°ç»“æ„\n",
    "\n",
    "ä»¥å•å¤´ self-attention ä¸ºä¾‹ï¼ˆä¸è€ƒè™‘ bias & maskï¼‰ï¼š\n",
    "\n",
    "$$\n",
    "Q = X W_Q,\\quad\n",
    "K = X W_K,\\quad\n",
    "V = X W_V \\\\\n",
    "A = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) \\\\\n",
    "O = A V\n",
    "$$\n",
    "\n",
    "\n",
    "- $X \\in \\mathbb{R}^{T \\times d_{\\text{model}}}$ï¼šè¾“å…¥åºåˆ—\n",
    "- $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "- $Q,K,V \\in \\mathbb{R}^{T \\times d_k}$\n",
    "- $A \\in \\mathbb{R}^{T \\times T}$\n",
    "- $O \\in \\mathbb{R}^{T \\times d_v}$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "1. $Q = X W_Q$ æ˜¯ä¸€ä¸ª GEMMï¼ˆ$T \\times d_{\\text{model}}$ ä¹˜ $d_{\\text{model}} \\times d_k$ï¼‰\n",
    "2. $K = X W_K$ã€$V = X W_V$ åŒç†\n",
    "3. $QK^\\top$ æ˜¯ä¸€ä¸ª **éå¸¸å¤§çš„ GEMM**ï¼ˆ$T \\times d_k$ ä¹˜ $d_k \\times T$ï¼‰\n",
    "4. $A V$ åˆæ˜¯ä¸€ä¸ª GEMMï¼ˆ$T \\times T$ ä¹˜ $T \\times d_v$ï¼‰\n",
    "- ${d_{\\text{model}}}$: æ¨¡å‹çš„éšè—ç»´åº¦ï¼Œæ¯ä¸ª token çš„å‘é‡é•¿åº¦ï¼ˆTransformer ä¸»é€šé“ç»´åº¦ï¼‰ã€‚\n",
    "- ${d_k}$: å•ä¸ªæ³¨æ„åŠ›å¤´çš„ Query/Key æŠ•å½±ç»´åº¦ï¼ˆé€šå¸¸ ${d_{\\text{model}}}$ / num_headsï¼‰ã€‚\n",
    "- ${d_v}$: Value æŠ•å½±ç»´åº¦ï¼ˆå¸¸è®¾æˆå’Œ ${d_k}$ ç›¸ç­‰ï¼‰ã€‚\n",
    "- ${T}$: åºåˆ—é•¿åº¦ï¼ˆæ—¶é—´æ­¥æ•°/ token æ•°\n",
    "\n",
    "\n",
    "**ã€å·¥ç¨‹è§†è§’ã€‘**  \n",
    "- Attention ä¸­çœŸæ­£â€œè´µâ€çš„åœ°æ–¹å°±æ˜¯ä¸¤ä¸ªçŸ©é˜µä¹˜æ³•ï¼š$QK^\\top$ å’Œ $A V$\n",
    "- softmax æœ¬èº« FLOPs ä¸å¤šï¼Œä½†å­˜åœ¨æ•°å€¼ç¨³å®šæ€§çš„æŒ‘æˆ˜ï¼ˆç¬¬ 8 ç« å±•å¼€ï¼‰\n",
    "- æ‰€æœ‰å¯¹ Attention çš„åŠ é€Ÿï¼ˆå¦‚ FlashAttentionã€å„ç§ kernel fusionï¼‰æœ¬è´¨éƒ½æ˜¯ï¼š\n",
    "  - å‡å°‘æ˜¾å­˜è¯»å†™\n",
    "  - æ”¹å–„è®¿é—®æ¨¡å¼\n",
    "  - åœ¨ä¸æ˜¾å¼æ„é€  $T \\times T$ çŸ©é˜µçš„å‰æä¸‹ï¼Œå®ç°ç­‰ä»·çš„çº¿æ€§ä»£æ•°è¿ç®—\n",
    "\n",
    "> ä¸€æ—¦ç«™åœ¨â€œçº¿æ€§ä»£æ•°â€è§†è§’çœ‹ Attentionï¼Œå°±æ›´å®¹æ˜“ç†è§£å„ç§åŠ é€Ÿè®ºæ–‡çš„æ€è·¯ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 1.7 Embeddingï¼šæŸ¥è¡¨ = é€‰è¡Œ = ç¨€ç– GEMM\n",
    "\n",
    "Embedding çŸ©é˜µï¼š$E \\in \\mathbb{R}^{V \\times D}$\n",
    "\n",
    "- Vï¼šè¯è¡¨å¤§å°\n",
    "- Dï¼šéšå±‚ç»´åº¦ / embedding ç»´åº¦\n",
    "\n",
    "ç»™å®š token id = iï¼ŒEmbedding çš„ä½œç”¨å°±æ˜¯ï¼š\n",
    "\n",
    "$$\n",
    "\\text{Embedding}(i) = E_i\n",
    "$$\n",
    "\n",
    "\n",
    "å³ï¼š**ä»çŸ©é˜µ E ä¸­é€‰å–ç¬¬ i è¡Œ**ã€‚\n",
    "\n",
    "å¦‚æœæ„é€ ä¸€ä¸ª one-hot å‘é‡ $x \\in \\mathbb{R}^V$ï¼š\n",
    "\n",
    "$$\n",
    "x_j = \\begin{cases}\n",
    "1, & j = i \\\\\n",
    "0, & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "åˆ™ï¼š\n",
    "\n",
    "$$\n",
    "y = x^\\top E = E_i\n",
    "$$\n",
    "\n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "\n",
    "> Embedding åœ¨æ•°å­¦ä¸Šç­‰ä»·äºï¼š**one-hot å‘é‡ä¹˜ embedding çŸ©é˜µçš„çŸ©é˜µä¹˜æ³•ï¼ˆä¸€ä¸ªæå…¶ç¨€ç–çš„ GEMMï¼‰**ã€‚\n",
    "\n",
    "**ã€å·¥ç¨‹è§†è§’ã€‘**  \n",
    "\n",
    "- å¯¹ Embedding æ¥è¯´ï¼Œç®—åŠ›ï¼ˆFMAï¼‰å‡ ä¹ä¸æ„æˆç“¶é¢ˆï¼Œå› ä¸ºæ²¡æœ‰å¤§è§„æ¨¡ä¹˜åŠ \n",
    "- çœŸæ­£çš„ç“¶é¢ˆæ˜¯ï¼š\n",
    "  - å†…å­˜è®¿é—®ï¼ˆéšæœºè®¿é—® E çš„ä¸åŒè¡Œï¼‰\n",
    "  - cache å‘½ä¸­ç‡\n",
    "  - å¸¦å®½\n",
    "\n",
    "è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆï¼š  \n",
    "> å·¨å¤§è¯è¡¨çš„ LLMï¼Œåœ¨éƒ¨ç½²æ—¶ç»å¸¸æ˜¯ â€œembedding å’Œ LM head å¾ˆåƒå¸¦å®½â€ï¼›  \n",
    "> è€Œä¸­é—´å±‚ï¼ˆGEMMï¼‰åˆ™æ›´åƒè®¡ç®—ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 1.8 æ•°æ®å¸ƒå±€ï¼ˆMemory Layoutï¼‰ä¸çº¿æ€§ä»£æ•°\n",
    "\n",
    "çŸ©é˜µåœ¨å†…å­˜ä¸­å¯ä»¥æœ‰ä¸¤ç§å¸¸è§å¸ƒå±€ï¼š\n",
    "\n",
    "- row-majorï¼ˆè¡Œä¼˜å…ˆï¼ŒC-styleï¼‰\n",
    "- column-majorï¼ˆåˆ—ä¼˜å…ˆï¼ŒFortran/BLAS-styleï¼‰\n",
    "\n",
    "é™„ï¼š\n",
    "BLASï¼ˆBasic Linear Algebra Subprogramsï¼‰æ˜¯å¯†é›†çº¿æ€§ä»£æ•°çš„æ ‡å‡† API è§„èŒƒï¼Œä¸€ç»„åŸºç¡€ç®—å­åº“æ¥å£ï¼š\n",
    "\n",
    "- Level 1/2/3ï¼šå‘é‡è¿ç®—ã€çŸ©é˜µâ€‘å‘é‡ã€çŸ©é˜µâ€‘çŸ©é˜µï¼ˆGEMM åœ¨ Level 3ï¼‰ã€‚\n",
    "- ä¸åŒå¹³å°æœ‰ä¸åŒå®ç°ï¼šOpenBLAS/BLISã€MKLã€Apple Accelerateã€cuBLAS ç­‰ï¼Œæä¾›é«˜åº¦ä¼˜åŒ–çš„å†…æ ¸ã€‚\n",
    "\n",
    "å¯¹äº $A \\in \\mathbb{R}^{M \\times K}$ï¼š\n",
    "\n",
    "- row-majorï¼šåŒä¸€è¡Œçš„å…ƒç´ è¿ç»­å­˜æ”¾\n",
    "- column-majorï¼šåŒä¸€åˆ—çš„å…ƒç´ è¿ç»­å­˜æ”¾\n",
    "\n",
    "è¿™ä¼šå½±å“ï¼š\n",
    "\n",
    "- è¿ç»­è®¿é—®çš„æ–¹å‘ï¼ˆstride=1ï¼‰\n",
    "- ç¼“å­˜å±€éƒ¨æ€§ï¼ˆcache localityï¼‰\n",
    "- æ˜¯å¦å®¹æ˜“è¢« SIMD å‘é‡åŒ–\n",
    "\n",
    "**ã€å·¥ç¨‹è§†è§’ã€‘**  \n",
    "\n",
    "- å¾ˆå¤š GEMM å†…æ ¸ä¼šè¦æ±‚ A/B/C çŸ©é˜µé‡‡ç”¨ç‰¹å®šå¸ƒå±€ï¼ˆå¦‚ A row-majorï¼ŒB col-majorï¼‰ï¼Œä»¥ä¾¿ï¼š\n",
    "  - åœ¨ inner-k loop ä¸­è¿ç»­è®¿é—®æ•°æ®\n",
    "  - åˆ©ç”¨ç¡¬ä»¶çš„å‘é‡åŠ è½½æŒ‡ä»¤ï¼ˆå¦‚ `ld1q`ã€`vmovaps` ç­‰ï¼‰\n",
    "- è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ åœ¨ kernel ä»£ç ä¸­ä¼šçœ‹åˆ°å¤§é‡ transpose / layout transformï¼Œå®ƒä»¬æœ¬è´¨æ˜¯ä¸ºçº¿æ€§ä»£æ•°è¿ç®—åˆ›å»ºâ€œæ›´å‹å¥½çš„ memory layoutâ€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 1.9 å°ç»“ï¼šçº¿æ€§ä»£æ•°æ˜¯ Runtime Inference çš„å…±åŒè¯­è¨€\n",
    "\n",
    "æœ¬ç« æ ¸å¿ƒç»“è®ºï¼š\n",
    "\n",
    "1. **æ¨ç†çš„å¤§éƒ¨åˆ† FLOPs éƒ½æ¥è‡ª GEMMï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰**  \n",
    "   - å…¨è¿æ¥ / MLP / Attention projection / Conv2d (im2col) ç­‰\n",
    "2. **ä½ç§©è¿‘ä¼¼ï¼ˆSVDï¼‰æ˜¯æœ€é‡è¦çš„å‹ç¼©æ•°å­¦ä¹‹ä¸€**  \n",
    "   - é€šè¿‡ rank-k è¿‘ä¼¼ï¼ŒæŠŠå¤§çŸ©é˜µæ‹†æˆä¸¤ä¸ªå°çŸ©é˜µä¹˜æ³•\n",
    "3. **çŸ©é˜µèŒƒæ•°ç»™å‡ºåº¦é‡å‹ç¼©è¯¯å·®çš„å·¥å…·**  \n",
    "   - Frobeniusã€è°±èŒƒæ•°ã€L1/L0\n",
    "4. **Convã€Attentionã€Embedding éƒ½å¯ä»¥åœ¨â€œçŸ©é˜µè§†è§’â€ä¸‹ç»Ÿä¸€çœ‹å¾…**\n",
    "5. **æ•°æ®å¸ƒå±€ä¸çº¿æ€§ä»£æ•°ç´§å¯†ç›¸å…³**  \n",
    "   - Row-major / col-major / tiling å†³å®šæ˜¯å¦èƒ½é«˜æ•ˆåˆ©ç”¨ç¡¬ä»¶\n",
    "\n",
    "> æ¥ä¸‹æ¥å‡ ç« ä¼šåœ¨è¿™ä¸ªåŸºç¡€ä¸Šï¼Œç»§ç»­å¼•å…¥ï¼š  \n",
    "> - ä¸ºä½•è¿™äº›çŸ©é˜µè¦è¢«â€œä¼˜åŒ–â€ï¼ˆæ•°å€¼ä¼˜åŒ–ç« èŠ‚ï¼‰  \n",
    "> - ä¸ºä½•æˆ‘ä»¬æ•¢äºç”¨ç²—ç³™çš„è¿‘ä¼¼ï¼ˆè¿‘ä¼¼ç†è®ºç« èŠ‚ï¼‰  \n",
    "> - å¦‚ä½•åœ¨æ¦‚ç‡/ä¿¡æ¯è®ºæ¡†æ¶ä¸‹ç†è§£å‹ç¼©ä¸é‡åŒ–  \n",
    "> - å¦‚ä½•ç»“åˆç¡¬ä»¶ç‰¹æ€§çœŸæ­£åšåˆ°â€œè·‘æ»¡â€æ¨ç†èŠ¯ç‰‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e8a74",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
