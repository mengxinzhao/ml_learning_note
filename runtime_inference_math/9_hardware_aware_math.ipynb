{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7caa8b",
   "metadata": {},
   "source": [
    "# 第 9 章：硬件相关数学（Hardware-Aware Math: Roofline, Tiling, SIMD, FMA）\n",
    "\n",
    "本章聚焦：\n",
    "\n",
    "- Roofline 模型：算力 vs 带宽\n",
    "- 算术强度（arithmetic intensity）\n",
    "- 分块（tiling）与数据重用\n",
    "- SIMD / FMA / GEMM 内核的数学视角\n",
    "\n",
    "部署 = 数学 × 数值稳定性 × 结构优化 × 硬件利用率\n",
    "本章进入“接近底层硬件”的数学，解释为什么你的模型在 GPU/NPU/CPU 上表现完全不同，以及编译器/内核为什么要做 tiling、packing、fusion。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f5d56",
   "metadata": {},
   "source": [
    "\n",
    "## 9.1 Roofline 模型\n",
    "\n",
    "Roofline 模型给出在给定硬件上，某个 kernel 的理论性能上界：\n",
    "\n",
    "$$\n",
    "P_{\\text{attainable}} = \\min\\left(P_{\\text{peak}},\\ I \\cdot B\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "| 量                 | 含义            | 单位            | 举例                     |\n",
    "| ----------------- | ------------- | ------------- | ---------------------- |\n",
    "| $P_{\\text{peak}}$ | 峰值算力（FLOPs/s） | TFLOPs / TOPS | GPU: 400 TFLOPs        |\n",
    "| $B$               | DRAM 带宽       | GB/s          | GPU: 1200 GB/s         |\n",
    "| $I$               | 算术强度          | FLOPs/byte    | Conv: 高；elementwise: 低 |\n",
    "\n",
    "\n",
    "算术强度定义为：\n",
    "\n",
    "$$\n",
    "I = \\frac{\\text{FLOPs}}{\\text{Bytes moved}}\n",
    "$$\n",
    "\n",
    "### 9.1.1 Roofline 的两种 regime\n",
    "\n",
    "1. **memory-bound**（带宽受限）：\n",
    "   - 若 $I \\cdot B < P_{\\text{peak}}$\n",
    "   - 性能主要受内存带宽限制，FLOPs 很充裕\n",
    "2. **compute-bound**（算力受限）：\n",
    "   - 若 $I \\cdot B \\ge P_{\\text{peak}}$\n",
    "   - 性能由算力上限决定\n",
    "\n",
    "- 典型 memory-bound 算子：\n",
    "\n",
    "| 算子                            | 原因                 |\n",
    "| ----------------------------- | ------------------ |\n",
    "| ReLU / GELU / Add / LayerNorm | 每元素只有 1–3 次计算，但要访存 |\n",
    "| 变形/transpose 类算子              | 全是搬运数据             |\n",
    "| Embedding lookup              | 几乎是纯读内存            |\n",
    "\n",
    "→ 工程手段：fusion、tiling、减少内存往返、cache-friendly layout\n",
    "\n",
    "- 典型 compute-bound 算子：\n",
    "\n",
    "| 算子               | 原因      |\n",
    "| ---------------- | ------- |\n",
    "| GEMM / Conv      | 乘加密集    |\n",
    "| Attention matmul | 高 FLOPs |\n",
    "\n",
    "→ 工程手段：使用最佳内核、tensor core、int8/int4、tiling\n",
    "\n",
    "**【工程视角】**  \n",
    "\n",
    "- 对算子而言：\n",
    "  - Conv/GEMM 通常具有较高 $I$，更容易成为 compute-bound\n",
    "  - 简单 element-wise op 的 $I$ 很低，通常是 memory-bound\n",
    "- 对 Embedding 而言：几乎没有 FLOPs，但读写大量数据 → 典型 memory-bound\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1.2 算术强度差异可大到几个数量级\n",
    "示例（FP16）：\n",
    "| 操作               | FLOPs | Byte   | I = FLOPs/Byte |\n",
    "| ---------------- | ----- | ------ | -------------- |\n",
    "| `x = x + y`      | 1     | 8      | 0.125          |\n",
    "| `GEMM 1024×1024` | 2·10⁹ | ~32 MB | ~60            |\n",
    "| `Conv 3×3, 64ch` | 1000+ | 64     | >15            |\n",
    "\n",
    "解释：\n",
    "\n",
    "Conv/GEMM 一次加载数据可重复使用多次，因此算术强度高。\n",
    "\n",
    "而 elementwise op 没有数据重用。\n",
    "→ fusion 对 LN/GELU/Activation 有巨大收益，而对 GEMM 几乎没意义。\n",
    "\n",
    "## 9.2 分块（Tiling）与数据重用\n",
    "\n",
    "以 GEMM 为例：$C = A B$，其中：\n",
    "\n",
    "- $A \\in \\mathbb{R}^{M \\times K}$\n",
    "- $B \\in \\mathbb{R}^{K \\times N}$\n",
    "- $C \\in \\mathbb{R}^{M \\times N}$\n",
    "\n",
    "如果直接三重循环，会频繁从内存加载 A/B/C，导致带宽浪费。\n",
    "```python\n",
    "for (i = 0; i < M; ++i)\n",
    "  for (j = 0; j < N; ++j)\n",
    "    for (k = 0; k < K; ++k)\n",
    "        C[i][j] += A[i][k] * B[k][j];\n",
    "```\n",
    "原因：假设 $M=N=K= 2048$\n",
    "- 矩阵大小约 ~16MB（FP32），肯定 放不进 L1/L2 cache。\n",
    "- 每次访问 B[k][j] 都是在跨行跳跃（Cache line 效率极低）B[0][0], B[1][0], B[2][0], ...\n",
    "- C[i][j] 也不断被读/写到 DRAM\n",
    "- 数据重用非常差. 某行 A[i][:] 会被重复使用 M 次, 但 naive 代码里：\n",
    "\n",
    "    - 内层 k 改变得太快\n",
    "\n",
    "    - cache line 刚读进来就被挤掉\n",
    "\n",
    "    - 重用不发生在 cache 里，而是再次从 DRAM load\n",
    "    \n",
    "    - B 的重用：每列 B[:][j] 理论上应重复用 N 次. 但 naive 访问 B 是跨行 → cache line 完全白费。\n",
    "- 结果：几乎所有 load 都来自 DRAM\n",
    "\n",
    "分块思想：\n",
    "\n",
    "- 将 C 分成小块（tile）：$M_b \\times N_b$\n",
    "- 对每个 tile：\n",
    "  - 加载对应的 A 子块（$M_b \\times K_b$）\n",
    "  - 加载对应的 B 子块（$K_b \\times N_b$）\n",
    "  - 在寄存器或小缓存中完成所有 FMA\n",
    "\n",
    "约束条件：\n",
    "\n",
    "$$\n",
    "(M_b \\cdot K_b + K_b \\cdot N_b + M_b \\cdot N_b) \\cdot \\text{bytes} \\le \\text{cache size}\n",
    "$$\n",
    "\n",
    "关键逻辑：\n",
    "一个 tile 在 cache/寄存器中反复使用，使得数据复用最大化。\n",
    "访问一次 DRAM → 用几十次、上百次。\n",
    "\n",
    "**【直观理解】**  \n",
    "\n",
    "- 通过分块，让同一块数据被多次重用（reuse），有效提高算术强度 $I$\n",
    "- 减少对 DRAM 的访问次数\n",
    "\n",
    "---\n",
    "\n",
    "### 9.2.2 为什么 tiling 是所有编译器/内核的“母操作”？\n",
    "\n",
    "\n",
    "- PyTorch Inductor\n",
    "- TensorRT\n",
    "- ONNX Runtime\n",
    "- CUTLASS/CUBLAS\n",
    "- TVM\n",
    "- Apple ANE 内核\n",
    "- Qualcomm Hexagon DSP\n",
    "- NVIDIA NPU\n",
    "\n",
    "都会一定进行：Tiling → Vectorization → Reuse → FMA/TensorCore mapping\n",
    "在 runtime inference 中：\n",
    "\n",
    "- tiling 大小 = 软件决定\n",
    "\n",
    "- tile 下沉到 tensor core → 必须是硬件决定\n",
    "（如 16×16、32×8、32×32）\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555aacf",
   "metadata": {},
   "source": [
    "## 9.3 SIMD / FMA / Tensor Core\n",
    "\n",
    "### 9.3.1 FMA 再回顾\n",
    "\n",
    "$$\n",
    "\\text{FMA}(a,b,c) = a \\times b + c\n",
    "$$\n",
    "\n",
    "\n",
    "- 在硬件中可以作为一个基本指令\n",
    "- dot product 与 GEMM 内部都是大量 FMA\n",
    "- 精度更好（融合运算只做一次 rounding）\n",
    "\n",
    "矩阵乘内部全是 FMA：\n",
    "$$C_{ij} += A_{ik}\\cdot B_{kj}$$\n",
    "\n",
    "### 9.3.2 SIMD（Single Instruction, Multiple Data）\n",
    "\n",
    "SIMD 指令可以：\n",
    "\n",
    "- 一条指令对多个数据元素执行相同运算\n",
    "- 例如 \n",
    "   - AVX2: 256-bit → 8 × FP32\n",
    "\n",
    "   - AVX512: 512-bit → 16 × FP32\n",
    "\n",
    "   - ARM NEON: 128-bit\n",
    "\n",
    "   - Apple AMX/Tensor Core: tile-based SIMD\n",
    "\n",
    "矩阵乘法内部：\n",
    "\n",
    "- 每次从内存加载一个向量 block\n",
    "- 使用 SIMD/FMA 对多个元素并行乘加\n",
    "\n",
    "### 9.3.3 Tensor Core / Matrix Unit\n",
    "\n",
    "在现代 GPU/NPU 中：\n",
    "\n",
    "- 提供专门的矩阵乘单元（如 NVIDIA Tensor Core、Intel AMX）\n",
    "- 每条指令执行一个小矩阵乘运算（如 16×16 × 16×16）\n",
    "\n",
    "**【数学视角】**  \n",
    "\n",
    "- 这些专用单元本质上实现了一个：\n",
    "\n",
    "$$\n",
    "C_{\\text{tile}} = A_{\\text{tile}} B_{\\text{tile}} + C_{\\text{tile}}\n",
    "$$\n",
    "\n",
    "\n",
    "- 你的任务是：\n",
    "  - 把大矩阵拆成符合硬件 tile 大小的小块\n",
    "  - 做好数据布局，使得这些 tile 在内存中连续、对齐\n",
    "\n",
    "---\n",
    "\n",
    "## 9.4 硬件意识下的量化与打包（Packing）\n",
    "\n",
    "对 INT8 / INT4 GEMM：\n",
    "\n",
    "- 权重/激活不仅被量化，还会按特定 pattern 打包（pack）到寄存器友好的格式\n",
    "- 例如把多个 int4 压在一个 16-bit 或 32-bit 容器中\n",
    "\n",
    "这涉及：\n",
    "\n",
    "- 位运算（bitwise ops）\n",
    "- 对齐（alignment）约束\n",
    "- 多个元素的“并行 unpack”数学\n",
    "\n",
    "虽然这里不展开细节，但要知道：\n",
    "\n",
    "> 高性能量化推理内核的数学本质是：  \n",
    "> **用整数算术和位运算实现“向量化 FMA”的等价行为。**\n",
    "整个过程实现的是：\n",
    "$$\\sum_{k=1}^n w_k x_k$$\n",
    "\n",
    "但硬件看到的是：\n",
    "\n",
    "- 一堆 32-bit load\n",
    "\n",
    "- 一堆 bitwise\n",
    "\n",
    "- 一堆 dot-product-like 指令（如 DP4A）\n",
    "\n",
    "### 9.4.1 DP4A 是量化内核的数学来源\n",
    "DP4A：一次指令做 4 个 int8 dot products\n",
    "$$result = \\sum_{i=0}^{3} a_i b_i$$\n",
    "INT8 GEMM 实际上是：\n",
    "\n",
    "把所有 dot product 尽可能映射到 DP4A/Tensor Core int8 MMA\n",
    "\n",
    "### 9.4.2 常见打包格式示例\n",
    "| 格式                      | 描述                         |\n",
    "| ----------------------- | -------------------------- |\n",
    "| NCHW16c                 | 通道 16 对齐 → vector friendly |\n",
    "| weight packing for GEMM | K blocking into 32         |\n",
    "| int4 → int32 pack       | 连续 8 个 int4 合并到 1 个 32-bit |\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79661ff",
   "metadata": {},
   "source": [
    "\n",
    "## 9.5 Nvidia Thor Hardware-Aware Math\n",
    "NVIDIA Thor（Drive Thor） 是 2025–2026 车端部署的旗舰 SoC，用于 L2++/L4 自动驾驶与多任务推理。\n",
    "> 其核心意义：把高吞吐 Tensor Core、GPU SM、Deep Learning Accelerator（DLA）、高带宽内存整合在一个汽车级芯片上。\n",
    "\n",
    "Thor 的关键指标\n",
    "| 项           | 大致规格（公开）                    |\n",
    "| ----------- | --------------------------- |\n",
    "| 峰值算力（AI 推理） | ~800–1000+ TOPS（INT8）       |\n",
    "| Tensor Core | 新一代、支持 FP8 / INT8 / INT4    |\n",
    "| GPU         | Ada/Lovelace 世代             |\n",
    "| 内存          | LPDDR5/X，带宽约数百 GB/s         |\n",
    "| 功耗          | < 250W                      |\n",
    "| 生态          | TensorRT、DriveOS、CUDA、cuDLA |\n",
    "\n",
    "对部署工程师而言：Thor = 有强大 TensorCore 的 edge device\n",
    "算力大、带宽中等、功耗有限、热容量有限 → 非常依赖 tiling、packing、quantization、compute-bound ↔ memory-bound 判断。\n",
    "\n",
    "---\n",
    "\n",
    "9.5.1 Thor 的 Roofline 分析\n",
    "1. compute roof（算力上限）\n",
    "如果 Thor INT8 = 1000 TOPS：\n",
    "$$P_{\\text{peak}} = 10^{15}\\text{ ops/s}$$\n",
    "\n",
    "2. 内存带宽\n",
    "假设 LPDDR5X ≈ 200–300 GB/s（合理估计）：\n",
    "$$B \\approx 2.5\\times10^{11} \\text{ bytes/s}$$\n",
    "\n",
    "3. 计算“分界线算术强度” I*：\n",
    "$$I^*=\\frac{P_{\\text{peak}}}{B}\n",
    "=\\frac{10^{15}}{2.5\\times10^{11}}\\approx4000$$\n",
    "\n",
    "结论：\n",
    "算术强度低于 4000 FLOPs/Byte 的算子全部 memory-bound。\n",
    "\n",
    "| 算子                  | I     | 判断               |\n",
    "| ------------------- | ----- | ---------------- |\n",
    "| ReLU / Add / GELU   | ~1–5  | memory-bound     |\n",
    "| LN（W=768）           | ~10   | memory-bound     |\n",
    "| Embedding           | ~0    | memory-bound     |\n",
    "| Attention Softmax   | < 10  | memory-bound     |\n",
    "| Conv 3×3 / GEMM 大矩阵 | 数百〜数千 | 接近 compute-bound |\n",
    "\n",
    "→ Thor 的大多数 Transformer 非矩阵算子全是 memory-bound\n",
    "→ 所以 fusion、tiling、weight packing 对 Thor 至关重要。\n",
    "\n",
    "9.5.2 Thor 上的 Tiling / Tensor Core 映射\n",
    "Thor 的 Tensor Core 可以执行：\n",
    "\n",
    "- FP16 / FP8 / INT8 / INT4\n",
    "\n",
    "- 小 tile（如 16×16 / 32×8 / etc.）矩阵乘\n",
    "\n",
    "- 类似 Hopper/Lovelace 的 MMA（matrix-multiply-accumulate）\n",
    "为了吃满 Tensor Core：\n",
    "需要满足 3 个条件：\n",
    "---\n",
    "(1) tile 大小必须匹配 MMA 固定尺寸\n",
    "\n",
    "例如 INT8 核心是：\n",
    "\n",
    "- 16×16×32\n",
    "\n",
    "- 32×8×16\n",
    "\n",
    "- 等等（依 GPU gen变化）\n",
    "你的 GEMM 必须拆成这些 block：\n",
    "$$A_{tile}\\in\\mathbb{R}^{16\\times32},\\ \n",
    "B_{tile}\\in\\mathbb{R}^{32\\times16},\\ \n",
    "C_{tile}\\in\\mathbb{R}^{16\\times16}$$\n",
    "若维度不能整除，会带来 padding overhead。\n",
    "---\n",
    "(2) weight/activation 必须 pack 成 TensorCore-friendly 格式\n",
    "Thor 上最典型布局：\n",
    "\n",
    "- INT8：K 方向 block 32 对齐， K = 内积维度（inner dimension / reduction dimension） $C_{M \\times N} = A_{M \\times K} \\times B_{K \\times N}$\n",
    "\n",
    "- INT4：K 方向 block 64 对齐\n",
    "\n",
    "- 权重量化后还要 re-order 成 warp-friendly 格式\n",
    "否则：性能可能下降 5–20×。\n",
    "---\n",
    "(3) Tiling 必须足够大，避免 memory-bound\n",
    "\n",
    "因为 I*≈4000 FLOPs/byte 非常高，如果 tile 太小：\n",
    "\n",
    "- 数据重用少\n",
    "\n",
    "- 算术强度低\n",
    "\n",
    "- 直接掉到 memory-bound 区域\n",
    "\n",
    "- 不可能吃满 1000 TOPS\n",
    "\n",
    "### 9.5.3 Thor 上的 Transformer 推理：算子分类\n",
    "\n",
    "典型 L2+/L4 自动驾驶模型包含：\n",
    "\n",
    "- 感知（视觉 backbone + BEV）\n",
    "\n",
    "- 融合（点云/BEV 融合）\n",
    "\n",
    "- Transformer（BEVFormer、DETR3D、Plato、OCC 等）\n",
    "\n",
    "- 预测\n",
    "\n",
    "- 规划\n",
    "\n",
    "- 控制网络\n",
    "\n",
    "这些算子在 Thor 上的 Roofline 表现：\n",
    "\n",
    "(1) 大 GEMM (Multi-Head Attention Q/K/V、FFN)\n",
    "\n",
    "- compute-bound 或接近上限\n",
    "\n",
    "- 只要维度够大：hidden=1024/2048 就能占满 Tensor Core\n",
    "\n",
    "工程应用：\n",
    "\n",
    "- 用 INT8/FP8 weights\n",
    "\n",
    "- K 维度必须 32/64 对齐（依硬件）\n",
    "\n",
    "- 采用 weight packing（W8A8/W4A8）\n",
    "\n",
    "(2) Softmax + LayerNorm + GELU → memory-bound\n",
    "\n",
    "无法吃算力：\n",
    "\n",
    "- 访存次数 = O(N)\n",
    "\n",
    "- FLOPs = 常数级\n",
    "\n",
    "工程应用：\n",
    "\n",
    "- 最好 fusion：LNorm + GELU + residual\n",
    "\n",
    "- 或者切片 tiling（利用 L2 cache 提升算术强度）\n",
    "\n",
    "- 或者使用 “FlashAttention” (DRAM I/O 降低)\n",
    "\n",
    "(3) BEV/2D/3D 插值、reshape、concat → 严重 memory-bound\n",
    "\n",
    "工程应用：\n",
    "\n",
    "- layout 优化（NCHW → NHWC16）\n",
    "\n",
    "- instrument L2 cache\n",
    "\n",
    "- fuse transpose + copy\n",
    "\n",
    "- 避免 Strided access\n",
    "\n",
    "(4) CNN backbone / Conv2d → compute-bound（FP16 / INT8）\n",
    "\n",
    "Conv 本身数据重用很高\n",
    "→ 非常适合 Thor TensorCore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703dce7",
   "metadata": {},
   "source": [
    "Thor 上的 Roofline 映射总结图\n",
    "| 算子类型                     | 算术强度 (Ops/Byte)       | Thor 区域             | 瓶颈                  | 优化手段                                      |\n",
    "|------------------------------|----------------------------|------------------------|------------------------|-----------------------------------------------|\n",
    "| Large GEMM (Block > 128)     |  High (> 4000)             | Compute Plateau       | Tensor Core TOPS      | WGMMA, FP8, Pipeline                          |\n",
    "| Small GEMM (Batch=1)         | Medium (~100-500)         | Transition            | Latency / Bandwidth   | Batching, Persistent Kernel                   |\n",
    "| Conv2d (ResNet)              | High                      | Compute Plateau       | Tensor Core TOPS      | NHWC Layout                                   |\n",
    "| Depthwise Conv               | Low                       | Memory Slope          | Bandwidth             | Kernel Fusion                                 |\n",
    "| LayerNorm / Activation       | Very Low (< 10)           | Memory Slope          | Bandwidth             | Fused Kernels (e.g. TRT Plugin)               |\n",
    "| BEV Grid Sample              | Very Low + High Latency   | Deep Memory Bound     | Cache Miss Rate       | Tiling, L2 Persistence, Shared Mem Pre-fetch  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8e920",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52563774",
   "metadata": {},
   "source": [
    "## 9.6 本章小结\n",
    "部署需要完成以下判断：\n",
    "1. 这个算子的 arithmetic intensity 是高还是低？\n",
    "\n",
    "    - GEMM / Conv → 高 → compute-bound → 用最佳内核、tensor core、int8\n",
    "\n",
    "    - LN / GELU / ReLU → 低 → memory-bound → fuse, reorder, reduce bandwidth\n",
    "\n",
    "    - Embedding → 超低 → 完全看带宽 → layout + cache-friendly\n",
    "\n",
    "2. 是否需要 tiling？如何选择 tile size？\n",
    "你的推理应带着数学约束：\n",
    "$$(M_bK_b+K_bN_b+M_bN_b)\\cdot bytes \\le cache$$\n",
    "\n",
    "tile 越大：\n",
    "- 算术强度高\n",
    "- 但可能对齐/缓存不够\n",
    "\n",
    "tile 越小：\n",
    "\n",
    "- DRAM 来回更多\n",
    "- 算术强度降低\n",
    "\n",
    "3. 是否能利用 SIMD/FMA/Tensor Core？\n",
    "    - weight layout 是否对齐？\n",
    "\n",
    "    - activation 是否 contiguous？\n",
    "\n",
    "    - tile 大小是否满足硬件 MMA 大小？\n",
    "\n",
    "4. 是否能使用量化（INT8/INT4）与打包？\n",
    "    - 带宽 ↓ 4x\n",
    "\n",
    "    - tile reuse ↑\n",
    "\n",
    "    - DP4A/TensorCore int8 ↑\n",
    "\n",
    "    - 算术强度 I ↑\n",
    "\n",
    "    - 性能 ↑ 2–6×（视硬件）-\n",
    "\n",
    "5. 对整个模型性能的“Roofline 定位”\n",
    "把所有算子画在 Roofline 上：\n",
    "\n",
    "    - GEMM/Conv：靠顶部（compute-bound）\n",
    "\n",
    "    - LN/GELU/Activation：靠左侧下方（memory-bound）\n",
    "\n",
    "    - Embedding：最左侧最低（pure bandwidth）\n",
    "\n",
    "这可以指导：\n",
    "\n",
    "- fusion priority\n",
    "\n",
    "- tiling priority\n",
    "\n",
    "- quantization priority\n",
    "\n",
    "- kernel scheduling\n",
    "\n",
    "- 算子 placement（CPU/NPU/GPU）\n",
    "\n",
    "> 到这里为止，你已经拥有了一套：  \n",
    "> **从线性代数 → 优化 → 近似 → 统计 → 信息论 → 信号处理 → 图论 → 数值分析 → 硬件数学** 的完整思维链条。\n",
    "\n",
    "在实际工程中，每一次做部署决策，你都可以在脑中快速走一遍这条链：\n",
    "\n",
    "- 这个算子本质是什么线性代数？\n",
    "- 能不能用压缩/近似/剪枝优化？\n",
    "- 对分布/信息量有何影响？\n",
    "- 会不会引发数值问题？\n",
    "- 在硬件上是 compute-bound 还是 memory-bound？\n",
    "- 该如何做 tiling / fusion / placement？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedccb1e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
