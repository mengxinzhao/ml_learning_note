{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd9fd9f",
   "metadata": {},
   "source": [
    "# ç¬¬ 8 ç« ï¼šæ•°å€¼ç¨³å®šæ€§ä¸å¤æ‚åº¦ï¼ˆNumerical Stability & Complexityï¼‰\n",
    "\n",
    "æœ¬ç« å…³æ³¨ï¼š\n",
    "\n",
    "- æµ®ç‚¹æ•°çš„è¯¯å·®æ¨¡å‹\n",
    "- ç¨³å®š softmax / LayerNorm\n",
    "- ç®—æ³•å¤æ‚åº¦ï¼ˆæ—¶é—´/ç©ºé—´ï¼‰ä¸æ¨ç†æ€§èƒ½\n",
    "\n",
    "---\n",
    "\n",
    "## 8.1 æµ®ç‚¹æ•°ä¸èˆå…¥è¯¯å·®ï¼ˆFloating-Point Error Modelï¼‰\n",
    "### 8.1.1 æµ®ç‚¹æ¨¡å‹ recap\n",
    "\n",
    "ä¸€ä¸ªå…¸å‹çš„æµ®ç‚¹è¿ç®—å¯ä»¥æŠ½è±¡ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\text{fl}(x \\circ y) = (x \\circ y)(1 + \\delta), \\quad |\\delta| \\le \\epsilon\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\circ$ï¼šåŸºæœ¬è¿ç®—ï¼ˆ+,-,Ã—,Ã·ï¼‰\n",
    "- $\\epsilon$ï¼šæœºå™¨ç²¾åº¦ï¼ˆmachine epsilonï¼‰\n",
    "- $\\delta$ï¼šèˆå…¥è¯¯å·®\n",
    "\n",
    "è¿ç»­å¤šæ¬¡è¿ç®—åï¼Œè¯¯å·®ä¼šç§¯ç´¯ï¼Œæœ‰æ—¶è¿˜ä¼šè¢«æ”¾å¤§ã€‚ï¼ˆ32ä½å•ç²¾åº¦æµ®ç‚¹æ•°ä¸­ï¼Œé€šå¸¸æœ‰1ä½ç¬¦å·ä½ï¼Œ8ä½æŒ‡æ•°ä½ï¼Œä»¥åŠ23ä½å°¾æ•°ï¼‰\n",
    "å¸¸è§ç²¾åº¦å¯¹æ¯”ï¼š\n",
    "| æ ¼å¼   | mantissa bitsï¼ˆå°¾æ•°ä½æ•°ï¼‰ | Îµï¼ˆè¯¯å·®é‡çº§ï¼‰ | æ˜¯å¦å¸¸ç”¨åœ¨æ¨ç†                     |\n",
    "| ---- | ------------- | ------- | --------------------------- |\n",
    "| FP32 | 23            | 1e-7    | âœ“ accumulator / LN / matmul |\n",
    "| FP16 | 10            | 1e-3    | âœ“ æ¨ç†ä¸»åŠ›                      |\n",
    "| BF16 | 7             | 1e-2    | âœ“ å¤§æ¨¡å‹æ›´ç¨³ã€è®­ç»ƒæ›´å¸¸ç”¨               |\n",
    "| FP8  | 4~5           | 1e-1    | âœ“ æ–°ç¡¬ä»¶ï¼ˆHopperã€Blackwellï¼‰æ¨ç†å¸¸ç”¨ |\n",
    "\n",
    "### 8.1.2 å·¥ç¨‹ä¸­è¯¯å·®ç´¯ç§¯çš„çœŸå®æ¡ˆä¾‹\n",
    "æ¡ˆä¾‹ 1ï¼šé•¿åºåˆ—æ³¨æ„åŠ›ä¸­ï¼Œsoftmax æº¢å‡ºå¯¼è‡´å…¨ NaN\n",
    "\n",
    "åœºæ™¯ï¼šT = 8192, d = 128, FP16\n",
    "\n",
    "å¦‚æœä½¿ç”¨ naive softmaxï¼Œexp(80) è¿™ç±»æ•°ä¼šç›´æ¥ âˆ\n",
    "\n",
    "åç»­ A V ä¸­å…¨æ˜¯ âˆï¼Œå¯¼è‡´æœ€ç»ˆè¾“å‡º NaN\n",
    "\n",
    "FlashAttention è®ºæ–‡ä¸­æ˜ç¡®å¼ºè°ƒï¼š\n",
    "\n",
    "> å¿…é¡» block-wise ç»´æŠ¤æœ€å¤§å€¼ï¼Œå¢é‡å¼ stable softmaxï¼Œå¦åˆ™ FP16 å¿…ç‚¸ã€‚\n",
    "---\n",
    "\n",
    "æ¡ˆä¾‹ 2ï¼šKV cache ç´¯åŠ å¯¼è‡´è¯¯å·®åç§»ï¼ˆå¤§æ¨¡å‹ï¼‰\n",
    "\n",
    "FP16 KV cache åœ¨ decoder ä¸­åå¤å‚ä¸ QK^T\n",
    "æ—¶é—´æ­¥å¤šæ—¶ï¼Œç´¯åŠ æ–¹å·®å¤±çœŸï¼š\n",
    "\n",
    "- æ³¨æ„åŠ›åˆ†å¸ƒæ…¢æ…¢åå‘å°‘é‡ token\n",
    "\n",
    "- å¯¼è‡´â€œé•¿å¯¹è¯ä¸¢è®°å¿†â€ç°è±¡\n",
    "è§£å†³ï¼šä½¿ç”¨ FP16/INT8 æƒé‡ï¼Œä½† KV cache ä¿ç•™ FP32 accumulatorã€‚ OpenAIã€Anthropic éƒ½æ˜¯è¿™æ ·åšã€‚\n",
    "---\n",
    "\n",
    "**ã€å·¥ç¨‹æ„ä¹‰ã€‘**  \n",
    "\n",
    "- åœ¨ ä» FP32 â†’ FP16 â†’ FP8ï¼ŒÎµ ä» 1e-7 â†’ 1e-3 â†’ 1e-1ï¼Œè¯¯å·®æ”¾å¤§ 10,000 å€ã€‚\n",
    "- åœ¨é•¿åºåˆ—ã€æ·±ç½‘ç»œä¸­ï¼Œç´¯ç§¯è¯¯å·®ä¸å®¹å¿½è§†\n",
    "- éœ€è¦è®¾è®¡æ•°å€¼ç¨³å®šçš„å®ç°æ–¹å¼ï¼ˆè§ softmax ç¤ºä¾‹ï¼‰\n",
    "\n",
    "### 8.1.3 Python  FP16 ä¸ FP32 çš„è¯¯å·®å·®å¼‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a09f81e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element value 0.0010000000474974513\n",
      "FP32: 10.000000953674316\n",
      "FP16: 10.0078125\n",
      "printed: tensor(0.0010)\n",
      "as float: 0.0010000000474974513\n",
      "high precision: 0.0010000000\n",
      "printed: tensor(0.0010, dtype=torch.float16)\n",
      "as float: 0.0010004043579101562\n",
      "high precision: 0.0010004044\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# æ„é€ ä¸€ä¸ªç´¯è®¡æ±‚å’Œçš„ä¾‹å­\n",
    "x = torch.ones(10000) * 1e-3\n",
    "print(\"element value\", x[0].item())\n",
    "\n",
    "s_fp32 = x.float().sum()\n",
    "s_fp16 = x.half().sum()\n",
    "\n",
    "print(\"FP32:\", s_fp32.item())\n",
    "print(\"FP16:\", s_fp16.item())\n",
    "\n",
    "f = torch.tensor(1e-3)  # default to float32\n",
    "print(\"printed:\", f)\n",
    "print(\"as float:\", float(f))\n",
    "print(\"high precision:\", format(float(f), \".10f\"))\n",
    "\n",
    "h = torch.tensor(1e-3).half()\n",
    "print(\"printed:\", h)              \n",
    "print(\"as float:\", float(h))\n",
    "print(\"high precision:\", format(float(h), \".10f\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c7b84",
   "metadata": {},
   "source": [
    "âœ… ä¸ºä»€ä¹ˆ FP16 ä¼šåå¤§ï¼Ÿ\n",
    "1. FP16 çš„ mantissa åªæœ‰ 10 bitsï¼ˆâ‰ˆ3 ä½åè¿›åˆ¶ç²¾åº¦ï¼‰\n",
    "è¿™æ„å‘³ç€ FP16 æ— æ³•å‡†ç¡®è¡¨ç¤º 1e-3ã€‚\n",
    "\n",
    "- å®é™… FP16 å­˜å‚¨çš„å€¼å¹¶ä¸æ˜¯ 0.0010000ï¼Œ\n",
    "- è€Œæ˜¯æœ€æ¥è¿‘å®ƒçš„ representable numberã€‚å®é™…å­˜çš„æ˜¯ 0.0010004044ï¼Œå³æ¯” 0.001 ç¨å¤§\n",
    "- FP16 ç´¯åŠ è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥è¿˜æœ‰èˆå…¥è¯¯å·®ï¼ˆround-to-nearest-evenï¼‰\n",
    "    - æ¯ä¸€æ­¥éƒ½å¯èƒ½å¾€â€œæœ€è¿‘çš„å¶æ•°å°¾æ•°â€èˆå…¥\n",
    "    - å¦‚æœä¸­é—´å€¼æ°å¥½åœ¨ä¸¤è¾¹ä¹‹é—´åå³ï¼Œå°±ä¼šè¢«å¾€ä¸Šèˆå…¥\n",
    "\n",
    "å¤§é‡é‡å¤åï¼Œå°±æ˜¯ç°åœ¨çš„ç»“æœï¼š\n",
    "FP16: 10.0078125  ï¼ˆæ¯” FP32 ç¨å¤§ï¼‰\n",
    "\n",
    "ã€å·¥ç¨‹æ„ä¹‰ã€‘\n",
    "éå¸¸å…³é”®çš„ä¸€ç‚¹ç»“è®ºï¼š\n",
    "\n",
    "å³ä¾¿æ˜¯çœ‹èµ·æ¥â€œå¾ˆå°â€çš„æ•°ï¼Œåœ¨ä½ç²¾åº¦ä¸‹ä¹Ÿç»å¸¸æ˜¯è¢«ç³»ç»Ÿæ€§â€œæ”¾å¤§â€æˆ–è€…â€œå‹ç¼©â€åçš„è¿‘ä¼¼å€¼ã€‚\n",
    "\n",
    "é…åˆé•¿åºåˆ— / å¤§ batch / å¤§ç»´åº¦çš„ç´¯åŠ ï¼Œå°±ä¼šå¯¼è‡´ï¼š\n",
    "\n",
    "- LayerNorm / RMSNorm æ±‚å’Œæœ‰ç³»ç»Ÿæ€§ bias\n",
    "\n",
    "- attention score ç´¯ç§¯åç§»\n",
    "\n",
    "- loss / logit ç»Ÿè®¡é‡åæ‰ï¼Œdebug å¾ˆéš¾è‚‰çœ¼çœ‹å‡º\n",
    "\n",
    "å·¥ä¸šåšæ³•ï¼š\n",
    "\n",
    "- è¾“å…¥å¯ä»¥æ˜¯ FP16 / BF16 / FP8\n",
    "\n",
    "- æ‰€æœ‰é‡è¦çš„ç´¯åŠ ï¼ˆsum / mean / variance / matmul çš„ accumulatorï¼‰éƒ½ç”¨ FP32\n",
    "\n",
    "- åšå®Œå† cast å›ä½ç²¾åº¦å­˜å’Œç®—\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e1c9c",
   "metadata": {},
   "source": [
    "âœ… ä¸ºä»€ä¹ˆ FP16 ä¸èƒ½ç²¾ç¡®è¡¨ç¤º 0.001ï¼Ÿ\n",
    "æ ¹æœ¬åŸå› ï¼š0.001 çš„äºŒè¿›åˆ¶å±•å¼€æ˜¯æ— é™ä¸å¾ªç¯å°æ•°ï¼Œè€Œ FP16 åªèƒ½å­˜æœ‰é™ 10 bit å°æ•°\n",
    "\n",
    "1. FP16 æµ®ç‚¹æ ¼å¼å›é¡¾ï¼ˆIEEE 754 half precisionï¼‰\n",
    "\n",
    "| éƒ¨åˆ†                  | bits        |\n",
    "| ------------------- | ----------- |\n",
    "| sign                | 1           |\n",
    "| exponent            | 5           |\n",
    "| mantissa (fraction) | **10 bits**  å°¾æ•°ç²¾åº¦ 11 bits ï¼ˆæœ‰10ä½è¢«æ˜¾å¼å­˜å‚¨ï¼‰ |\n",
    "\n",
    "æµ®ç‚¹æ•°è¡¨ç¤ºçš„æ˜¯ï¼š$\\text{value} = (-1)^s \\cdot (1 + m) \\cdot 2^{e - 15}$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- ğ‘šæœ‰ 10 bits\n",
    "- FP16 çš„æœ‰æ•ˆåè¿›åˆ¶ç²¾åº¦çº¦ä¸º 3 åˆ° 4 ä½æœ‰æ•ˆæ•°å­—\n",
    "- æŒ‡æ•°ä½ é‡‡ç”¨åç§»äºŒè¿›åˆ¶è¡¨ç¤ºæ³•ï¼Œåç§»é‡ï¼ˆBiasï¼‰ä¸º 15ã€‚å®é™…æŒ‡æ•°å€¼ç­‰äºå­˜å‚¨å€¼å‡å» 15\n",
    "- å¯è¡¨ç¤ºçš„èŒƒå›´çº¦ä¸º \\(2^{-14}\\) åˆ° \\(2^{15}\\)\n",
    "\n",
    "2. 0.001 è½¬æˆäºŒè¿›åˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "```block\n",
    "0.001 decimal = 0.0000000000000000000001010001111010111000010100011110... (binary)\n",
    "```\n",
    "äºŒè¿›åˆ¶èƒ½è¢«ç²¾ç¡®è¡¨ç¤ºçš„åè¿›åˆ¶åˆ†æ•°æ»¡è¶³ï¼š åˆ†æ¯å¿…é¡»æ˜¯ 2 çš„å¹‚ã€‚\n",
    "ä¾‹å¦‚ï¼š\n",
    "\n",
    "- 0.5 = 1/2 â†’ å¯ä»¥è¡¨ç¤º\n",
    "\n",
    "- 0.25 = 1/4 â†’ å¯ä»¥\n",
    "\n",
    "- 0.75 = 3/4 â†’ å¯ä»¥\n",
    "\n",
    "- 0.1 = 1/10 â†’ âŒ ä¸èƒ½\n",
    "\n",
    "- 0.001 = 1/1000 â†’ ä¸èƒ½ï¼ˆ1000=2Â³Ã—125ï¼Œå…¶ä¸­125ä¸æ˜¯2çš„å¹‚ï¼‰\n",
    "\n",
    "3. å› ä¸º FP16 åªèƒ½å®¹çº³ 10 bits mantissa â†’ åªèƒ½â€œéå¸¸ç²—ç³™åœ°â€é€¼è¿‘å®ƒ\n",
    "PyTorch å®é™…å­˜çš„å€¼ï¼š0.0010004044\n",
    "$$0.0010004044 \\approx 1/999.6$$\n",
    "å®ƒæ¯”çœŸæ­£çš„ 0.001 ç¨å¾®å¤§ä¸€ç‚¹ã€‚\n",
    "\n",
    "4. è®¡ç®—æœ€æ¥è¿‘çš„ FP16 è¡¨ç¤ºï¼šæ•°å­¦éªŒè¯\n",
    "æ¨å¯¼FP16 åœ¨è¿™ä¸€æ®µçš„â€œé—´éš”â€ï¼ˆULP:Unit of Least Precisionï¼‰æœ€ä½æœ‰æ•ˆä½å•å…ƒ\n",
    "- ULP æ˜¯æ•°å€¼åˆ†æä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼Œè¡¨ç¤ºä¸€ä¸ªæµ®ç‚¹æ•°ä¸å…¶ç›¸é‚»çš„ä¸‹ä¸€ä¸ªå¯ç²¾ç¡®è¡¨ç¤ºçš„æµ®ç‚¹æ•°ä¹‹é—´çš„æœ€å°è·ç¦»æˆ–æ­¥é•¿ã€‚æ˜¯æ”¹å˜ mantissa æœ€ä½ bitï¼ˆLSBï¼‰æ—¶ï¼Œæµ®ç‚¹å€¼çš„ç»å¯¹å·®ã€‚\n",
    "- ULP çš„å¤§å°å¹¶éå›ºå®šä¸å˜ï¼Œè€Œæ˜¯å–å†³äºæµ®ç‚¹æ•°æœ¬èº«çš„å½“å‰é‡çº§ï¼ˆå³å…¶æŒ‡æ•°å€¼ï¼‰ã€‚ç¦»é›¶è¶Šè¿‘ï¼ŒFP16 æ•°å­—ä¹‹é—´çš„é—´è·è¶Šå°ï¼›ç¦»é›¶è¶Šè¿œï¼Œé—´è·è¶Šå¤§ã€‚\n",
    "- å¯¹äºä¸€ä¸ªè§„æ ¼åŒ–normalizedçš„ FP16 æ•° $x$, å…¶ ULP å¯ä»¥ç”¨ä»¥ä¸‹å…¬å¼è¡¨ç¤ºï¼š\n",
    "$$\\text{ULP}(x)=2^{\\text{e}-\\text{mantissa}+1}$$\n",
    "å°† FP16 çš„å…·ä½“å‚æ•°ä»£å…¥ï¼š\n",
    "- å°¾æ•°ä½æ•°ï¼ˆå«éšå¼ä½ï¼‰ä¸º \\(p=11\\) æ¯”ç‰¹ã€‚\n",
    "- å®é™…æŒ‡æ•°å€¼ä¸º \\(E-15\\)\n",
    "\n",
    "$$ \\text{ULP}(x)=2^{(e-15)-11+1}=2^{(e-25)}$$\n",
    "å…¶ä¸­ \\(e\\) æ˜¯å­˜å‚¨åœ¨æŒ‡æ•°åŸŸçš„ 5 æ¯”ç‰¹æ•´æ•°å€¼ã€‚\n",
    "\n",
    "ç†è§£æˆ$\\text{ULP}(x)=(\\text{å°¾æ•°çš„æœ€å°å¢é‡})\\times (\\text{ç”±æŒ‡æ•°å†³å®šçš„æ¯”ä¾‹å› å­})$\n",
    "\n",
    "æŠŠ 0.001 ç”¨äºŒè¿›åˆ¶ç§‘å­¦è®¡æ•°æ³•å†™å‡ºæ¥ï¼š\n",
    "$0.001 \\approx 0.0010004044 = 1.0244 \\times 2^{-10}$\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "$$e - bias = -10$$\n",
    "$$e = -10 + 15 = 5$$\n",
    "\n",
    "\n",
    "å¸¦å…¥å…¬å¼è®¡ç®— FP16 åœ¨æ­¤å¤„çš„ ULP\n",
    "$$ULP = 2^{(e - 25)}$$\n",
    "$$ULP = 2^{(5 - 25)} = 2^{-20}$$\n",
    "è¿™æ­£å¥½æ˜¯ï¼š\n",
    "$0.00000095367431640625$\n",
    "\n",
    "è€Œä½ çœ‹åˆ°çš„è¯¯å·®ï¼š\n",
    "$$0.0010004044 - 0.001 = 4.044 \\times 10^{-7}$$\n",
    "æ­£å¥½æ˜¯åŠä¸ª step å¤šä¸€ç‚¹ã€‚\n",
    "\n",
    "è¿™è¯´æ˜ï¼š\n",
    "\n",
    "0.001 çš„ FP16 è¡¨ç¤ºå°±æ˜¯å®ƒé™„è¿‘æœ€æ¥è¿‘çš„ FP16 decimal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7279d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down: 0.00099945068359375\n",
      "x: 0.0010004043579101562\n",
      "up: 0.0010013580322265625\n",
      "ULP down: 9.5367431640625e-07\n",
      "ULP up: 9.5367431640625e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n3/5t978bxj59ddk47g5b0wqg4h0000gn/T/ipykernel_23870/1481770706.py:7: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  down16 = np.nextafter(x, np.float16(0), dtype=np.float16)\n",
      "/var/folders/n3/5t978bxj59ddk47g5b0wqg4h0000gn/T/ipykernel_23870/1481770706.py:8: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  up16   = np.nextafter(x, np.float16(1), dtype=np.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.tensor(1e-3).half()\n",
    "\n",
    "# ç”¨ numpy çš„ nextafter æ‰¾ half ç²¾åº¦å¯¹åº” float çš„é‚»è¿‘å€¼\n",
    "down16 = np.nextafter(x, np.float16(0), dtype=np.float16)\n",
    "up16   = np.nextafter(x, np.float16(1), dtype=np.float16)\n",
    "\n",
    "# validate ULP around 0.001 are very close to 2^-20~=9.5367Ã—10â»â·\n",
    "print(\"down:\", float(down16))\n",
    "print(\"x:\", float(x))\n",
    "print(\"up:\", float(up16))\n",
    "print(\"ULP down:\", float(x) - float(down16))\n",
    "print(\"ULP up:\", float(up16) - float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6c3b9",
   "metadata": {},
   "source": [
    "\n",
    "## 8.2 ç¨³å®š softmax\n",
    "### 8.2.1 æ•°å€¼é—®é¢˜æ¥æº\n",
    "æœ´ç´ å†™æ³•ï¼š\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "å¦‚æœæœ€å¤§å€¼å¾ˆå¤§ï¼Œä¾‹å¦‚ 80ï¼š\n",
    "\n",
    "FP16 çš„ exp(80) ç«‹å³æº¢å‡ºä¸º inf\n",
    "\n",
    "åˆ†æ¯ä¹Ÿæ˜¯ infï¼Œäº§ç”Ÿ NaNã€\n",
    "\n",
    "- å½“æŸä¸ª $x_k$ å¾ˆå¤§æ—¶ï¼Œ$e^{x_k}$ æº¢å‡º \n",
    "- å…¶å®ƒé¡¹ç›¸å¯¹å˜æˆ 0ï¼Œå¯¼è‡´æ•°å€¼é—®é¢˜\n",
    "\n",
    "### 8.2.1 ç¨³å®šå†™æ³•\n",
    "ç¨³å®šå†™æ³•ï¼š\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i - m}}{\\sum_j e^{x_j - m}}, \\quad m = \\max_j x_j\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "æ¨å¯¼ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "= \\frac{e^{x_i - m} e^m}{\\sum_j e^{x_j - m} e^m}\n",
    "= \\frac{e^{x_i - m}}{\\sum_j e^{x_j - m}}\n",
    "$$\n",
    "\n",
    "\n",
    "- åˆ†å­åˆ†æ¯åŒæ—¶ä¹˜ä»¥ $e^{-m}$ï¼Œæ•°å€¼ä¸Šé¿å…äº†æº¢å‡º/ä¸‹æº¢\n",
    "\n",
    "**ã€å·¥ç¨‹è§†è§’ã€‘**  \n",
    "\n",
    "- åœ¨ FP16/FP8 æˆ–æ··åˆç²¾åº¦ä¸­ï¼Œè¿™ç§ç¨³å®šåŒ–å¤„ç†å°¤å…¶é‡è¦\n",
    "- FlashAttention ç­‰ç®—æ³•åœ¨å®ç°æ—¶éå¸¸å¼ºè°ƒï¼š\n",
    "  - æŒ‰ block è®¡ç®— max\n",
    "  - å¢é‡å¼ç»´æŠ¤ç¨³å®š softmax\n",
    "\n",
    "---\n",
    "### 8.2.3 PyTorch naive vs stable å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "331c3820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive softmax:\n",
      "tensor([nan, nan, nan])\n",
      "\n",
      "Stable softmax:\n",
      "tensor([0.6652, 0.2447, 0.0900])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([1000.0, 999.0, 998.0])\n",
    "\n",
    "print(\"Naive softmax:\")\n",
    "try:\n",
    "    naive = torch.exp(x) / torch.exp(x).sum()\n",
    "    print(naive)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "print(\"\\nStable softmax:\")\n",
    "print(F.softmax(x, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8565ce6",
   "metadata": {},
   "source": [
    "### 8.2.4 å·¥ç¨‹çº§ç¨³å®š softmaxï¼ˆFlashAttention é£æ ¼ï¼‰\n",
    "æ ¸å¿ƒæŠ€å·§ï¼š\n",
    "\n",
    "- block çº§æœ€å¤§å€¼ç»´æŠ¤\n",
    "\n",
    "- å¢é‡å¼ log-sum-exp æ›´æ–°\n",
    "ä¼ªä»£ç \n",
    "```python\n",
    "for block in blocks:\n",
    "    m_new = max(m, max(block))\n",
    "    s = s * exp(m - m_new) + sum(exp(block - m_new))\n",
    "    m = m_new\n",
    "```\n",
    "\n",
    "**flash-attention æœ€é‡è¦ä¸æ˜¯å‡å°‘ FLOPsï¼Œè€Œæ˜¯ï¼š ä¿æŒ softmax å…¨ç¨‹æ•°å€¼ç¨³å®šï¼ˆFP16/FP8**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50f9b9",
   "metadata": {},
   "source": [
    "\n",
    "## 8.3 LayerNorm / RMSNorm çš„æ•°å€¼è€ƒè™‘\n",
    "### 8.3.1 LayerNorm çš„ FP16 é—®é¢˜\n",
    "LayerNormï¼š\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{d}\\sum_{i=1}^d x_i, \\quad\n",
    "\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^d (x_i - \\mu)^2 \\\\\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "\n",
    "æ•°å€¼ç¨³å®šæ€§æŒ‘æˆ˜ï¼š\n",
    "\n",
    "- FP16 ç´¯åŠ ä¼šä¸¥é‡æŸå¤±ç²¾åº¦\n",
    "\n",
    "- $(x_i - \\mu)^2$ åœ¨ FP16 ä¸‹å¯èƒ½ä¸‹æº¢æˆ 0\n",
    "\n",
    "- $\\sigma^2 + \\epsilon$ å¯èƒ½ä¸å¤Ÿå¤§ï¼Œå¯¼è‡´é™¤æ³•ä¸ç¨³å®š\n",
    "\n",
    "### 8.3.2 å·¥ä¸šç•Œé»˜è®¤è§£å†³æ–¹æ¡ˆ\n",
    "æ–¹æ¡ˆ Aï¼šFP32 ä½œä¸º accumulatorï¼ŒFP16 è¾“å…¥è¾“å‡º\n",
    "\n",
    "æµç¨‹ï¼š\n",
    "\n",
    "- cast x â†’ FP32\n",
    "\n",
    "- æ±‚å‡å€¼ä¸æ–¹å·®\n",
    "\n",
    "- å† cast å› FP16\n",
    "\n",
    "PyTorchã€xFormersã€TensorRT å…¨æ˜¯è¿™æ ·ã€‚\n",
    "\n",
    "---\n",
    "æ–¹æ¡ˆ Bï¼šRMSNorm æ›´ç¨³å®šï¼ˆTransformer é»˜è®¤ï¼‰\n",
    "\n",
    "$$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum x_i^2 + \\epsilon}}$$\n",
    "ä¼˜åŠ¿ï¼š\n",
    "\n",
    "- ä¸è®¡ç®— mean â†’ å°‘ä¸€ä¸ªé«˜æ–¹å·®æ“ä½œ\n",
    "\n",
    "- FP16 ä¸­æ›´ç¨³å®š\n",
    "æ‰€ä»¥ LLaMA å…¨éƒ¨ç”¨ RMSNormã€‚\n",
    "---\n",
    "\n",
    "RMSNorm  å’Œ LayerNorm åŒºåˆ«ï¼Ÿ\n",
    "- LayerNormï¼šå…ˆå‡å‡å€¼å†é™¤ä»¥æ ‡å‡†å·®ï¼Œå…¬å¼ (x-mean)/sqrt(var+eps)ï¼Œæœ‰å¯å­¦ä¹ çš„ Î³/Î²ã€‚å¯¹å‡å€¼ä¸å¹…åº¦éƒ½åšæ ‡å‡†åŒ–ã€‚\n",
    "- RMSNormï¼šåªç”¨å‡æ–¹æ ¹å½’ä¸€ï¼Œä¸å‡å‡å€¼ï¼Œå…¬å¼ x / rms(x)ï¼Œé€šå¸¸ä¹Ÿæœ‰å¯å­¦ä¹ çš„ç¼©æ”¾ï¼ˆæœ‰äº›å®ç°æ— åç½®ï¼‰ã€‚åªç¼©æ”¾å¹…åº¦ï¼Œä¸ä¸­å¿ƒåŒ–ã€‚\n",
    "\n",
    "å®é™…å½±å“ï¼š\n",
    "\n",
    "- RMSNorm è®¡ç®—æ›´ç®€å•ã€ç•¥å¿«ï¼Œæ•°å€¼ä¸Šæ›´ç¨³å®šä¸€äº›ï¼ˆå°‘ä¸€æ¬¡å‡å‡å€¼ï¼‰ï¼›åœ¨éƒ¨åˆ†å¤§æ¨¡å‹ï¼ˆå¦‚ LLaMA ç³»åˆ—ï¼‰è¡¨ç°ä¸é”™ã€‚\n",
    "- LayerNorm æ›´å¸¸è§ï¼Œè¾“å‡ºé›¶å‡å€¼ã€å•ä½æ–¹å·®ï¼Œå¯¹å‡å€¼æ¼‚ç§»ä¹Ÿåšäº†å½’ä¸€ï¼Œä¸€äº›ä»»åŠ¡ä¸Šå¯èƒ½æ›´ç¨³ã€‚\n",
    "\n",
    "åœ¨å¤§å¤šæ•° Transformer ä»»åŠ¡ä¸­ï¼Œä¸¤è€…ç²¾åº¦å·®å¼‚ä¸å¤§ï¼Œéœ€è¦å®æµ‹ï¼›è‹¥è¿½æ±‚é€Ÿåº¦/ç®€åŒ–ï¼ŒRMSNorm æ˜¯è½»é‡æ›¿ä»£ã€‚\n",
    "\n",
    "---\n",
    "ğŸ”¥ ä¸ºä»€ä¹ˆ FP16 ä¸­ $(x_i - \\mu)^2$ä¼šä¸‹æº¢æˆ 0ï¼Ÿ\n",
    "\n",
    "1. FP16 çš„æœ€å°è§„æ ¼åŒ–æ•°éå¸¸å¤§ï¼ˆâ‰ˆ 6e-8ï¼‰ï¼Œsquare æ“ä½œä¼šç¬é—´æ‰åˆ° FP16 çš„éè§„æ ¼åŒ–åŒºç”šè‡³ç›´æ¥å˜æˆ 0ã€‚\n",
    "\n",
    "è®¡ç®—è¿‡ç¨‹ï¼š\n",
    "\n",
    "- æœ€å°æ­£çš„ normal æ•°ï¼ˆæœ€å°æ­£è§„åŒ–æ•°ï¼‰\n",
    "   - ç¬¦å·ä½ï¼š0\n",
    "   - æŒ‡æ•°ä½ï¼š00001ï¼ˆå®é™…æŒ‡æ•° = 1 - 15 = -14ï¼‰\n",
    "   - å°¾æ•°ä½ï¼š0000000001ï¼ˆéšå«å‰å¯¼1 â†’ å®é™…å°¾æ•° 1.0000000001â‚‚ï¼‰\n",
    "   - å€¼ = 2â»Â¹â´ Ã— 1.0000000001â‚‚ â‰ˆ 2â»Â¹â´ Ã— 1.000000000 = 6.103515625e-5\n",
    "\n",
    "- æœ€å°æ­£çš„ subnormal æ•°ï¼ˆæœ€å°éæ­£è§„åŒ–æ•° / æœ€å°æ­£æ•°ï¼‰\n",
    "   - ç¬¦å·ä½ï¼š0\n",
    "   - æŒ‡æ•°ä½ï¼š00000ï¼ˆå›ºå®šè§£é‡Šä¸ºæŒ‡æ•° -14ï¼‰\n",
    "   - å°¾æ•°ä½ï¼š0000000001ï¼ˆæ— éšå«å‰å¯¼1 â†’ å®é™…å°¾æ•° 0.0000000001â‚‚ = 2â»Â¹â°ï¼‰\n",
    "   - å€¼ = 2â»Â¹â´ Ã— 2â»Â¹â° = 2â»Â²â´ â‰ˆ 5.9604644775390625e-8\n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´å°äº6e-8çš„æ•°ä¼šä¸‹æº¢åˆ°0\n",
    "\n",
    "---\n",
    "\n",
    "2. LayerNorm / RMSNorm è®¡ç®—æœ¬è´¨å«å¤§é‡æå°æ•°\n",
    "\n",
    "åœ¨ LN ä¸­ï¼š$x_i - \\mu$\n",
    "å¦‚æœæŸä¸ª$x_i$å¾ˆæ¥è¿‘å‡å€¼$\\mu$ï¼Œæ¯”å¦‚ï¼š\n",
    "- $1e-3 - 1.000001e-3 = -1e-9$\n",
    "- $0.035412 - 0.035411 = 1e-6$\n",
    "- $0.004 - 0.00400003 = -3e-8$\n",
    "è¿™äº›åœ¨ FP32 å®Œå…¨æ²¡é—®é¢˜ã€‚\n",
    "\n",
    "ä½†åœ¨ FP16ä¸­ä»»ä½•$|x_i - \\mu| < 6e-8$çš„æƒ…å†µï¼Œç›´æ¥å˜æˆ 0ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "3. æ¥ä¸‹æ¥ LN è¦åšå¹³æ–¹ï¼š\n",
    "$$(x_i - \\mu)^2$$\n",
    "å¦‚æœ$|x_i - \\mu|$æœ¬æ¥æ˜¯ï¼š\n",
    "- $1e-6$\n",
    "- $1e-7$\n",
    "- $1e-8$\n",
    "å¹³æ–¹ååˆ†åˆ«å˜æˆï¼š\n",
    "- $1e-12$\n",
    "- $1e-14$\n",
    "- $1e-16$\n",
    "è¿™äº›æ•°å…¨éƒ¨ï¼šè¿œè¿œ å°äº FP16 çš„æœ€å°æ­£æ•°ï¼ˆsubnormalï¼‰ 5.96e-8 â†’ ç›´æ¥ä¸‹æº¢æˆ /æˆ–è¢«é‡åŒ–ä¸º0ã€‚\n",
    "\n",
    "è¿™æ„å‘³ç€ï¼š\n",
    "$$\\sigma^2 = \\frac{1}{d}\\sum (x_i - \\mu)^2 = 0$$\n",
    "LN è¾“å‡ºå˜æˆï¼š\n",
    "\n",
    "$$\\frac{x_i - \\mu}{\\sqrt{0 + \\epsilon}} \\approx 0$$\n",
    "æœ€ç»ˆ LN æ•´å±‚ä¼šè¾“å‡ºä¸€å †å‡ ä¹ç›¸åŒçš„å€¼ â†’ ä¿¡æ¯è¢«æŠ¹æ‰ â†’ æ¨¡å‹ç¨³å®šæ€§ä¸‹é™ã€æ¨ç†æ•ˆæœå˜å·®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93931b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 x: tensor([1., 1., 1., 1.], dtype=torch.float16)\n",
      "FP16 mean: tensor(1., dtype=torch.float16)\n",
      "FP16 variance terms: tensor([0., 0., 0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# pytorch to reproduce FP16 variance disappearance issue\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0000, 1.0001, 0.9999, 1.00005], dtype=torch.float16)\n",
    "mu = x.mean()\n",
    "sq = (x - mu) ** 2\n",
    "\n",
    "print(\"FP16 x:\", x)\n",
    "print(\"FP16 mean:\", mu)\n",
    "print(\"FP16 variance terms:\", sq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec06aa",
   "metadata": {},
   "source": [
    "### 8.3.4 å·¥ç¨‹ç»“è®º\n",
    "\n",
    "- FP16/FP8 ç´¯åŠ å¿…é¡»ä½¿ç”¨ FP32 accumulator\n",
    "\n",
    "     - LayerNorm ç´¯åŠ \n",
    " \n",
    "     - RMSNorm ç´¯åŠ \n",
    "\n",
    "     - Attention ä¸­çš„ QKã€AV ç´¯åŠ \n",
    "\n",
    "     - Softmax çš„æŒ‡æ•°å’Œæ±‚å’Œ\n",
    "\n",
    "     - KV cache decaying ç´¯åŠ \n",
    "\n",
    "     - matmul å†…éƒ¨ partial sum\n",
    "\n",
    "æµç¨‹æ˜¯ï¼š\n",
    "\n",
    "- cast FP16 â†’ FP32\n",
    "\n",
    "- FP32 åš mean\n",
    "\n",
    "- FP32 åšå¹³æ–¹\n",
    "\n",
    "- FP32 åš variance\n",
    "\n",
    "- æœ€å cast å› FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71770645",
   "metadata": {},
   "source": [
    "## 8.4 ç®—æ³•å¤æ‚åº¦ï¼ˆComplexityï¼‰\n",
    "### 8.4.1 ä¸ºä»€ä¹ˆå¤æ‚åº¦å¯¹æ¨ç†è‡³å…³é‡è¦'\n",
    "æ¨ç†ç“¶é¢ˆæ¥è‡ªï¼š\n",
    "- æ—¶é—´å¤æ‚åº¦ï¼šFLOPs\n",
    "- ç©ºé—´å¤æ‚åº¦ï¼šactivation + KV Cache\n",
    "- å¸¦å®½ï¼šä» DRAM â†’ NPU\n",
    "---\n",
    "### 8.4.2 è‡ªæ³¨æ„åŠ›çš„å¤æ‚åº¦\n",
    "ä¼ ç»Ÿ attentionï¼š\n",
    "$$O(T^2 d)$$\n",
    "å¦‚æœ T=4096, d=128:\n",
    "- QKáµ€ FLOPs ~ 4096Â² Ã— 128 â‰ˆ 2.1e9ï¼ˆå·¨å¤§ï¼ï¼‰\n",
    "- V åŠ æƒåŒçº§åˆ«\n",
    "---\n",
    "8.4.3 é™ä½å¤æ‚åº¦çš„å…¸å‹æ–¹å‘\n",
    "1. FlashAttentionï¼ˆåŒé˜¶ä½†å¸¸æ•°æ›´å°ï¼‰\n",
    "   - ä¿ç•™ $O(T^2)$\n",
    "\n",
    "   - å‡å°‘ activation + memory bandwidth\n",
    "\n",
    "   - æå‡ 2~4x\n",
    "2. ç»†ç²’åº¦ KV cache compression\n",
    "   - top-k è®°å¿†\n",
    "\n",
    "   - å±€éƒ¨æ³¨æ„åŠ›\n",
    "\n",
    "   - memory budget <-> latency tradeoff\n",
    "\n",
    "3. ä½ç§©è¿‘ä¼¼ï¼ˆSVD/low-rank attentionï¼‰\n",
    "å°† $$QK^\\top â‰ˆ U V^\\top$$\n",
    "å¤æ‚åº¦ä»\n",
    "$O(T^2 d)$ â†’ $O(T r)$ï¼Œå…¶ä¸­ r â‰ª d\n",
    "\n",
    "4. K-NN Attention / Linear Attention\n",
    "åˆ©ç”¨æ ¸æŠ€å·§ï¼š\n",
    "$$\\text{softmax}(QK^T)V = Q (K^T V)$$\n",
    "å¤æ‚åº¦å˜æˆ $O(T d)$ã€‚\n",
    "\n",
    "\n",
    "## 8.5 æœ¬ç« å°ç»“\n",
    "\n",
    "1. æµ®ç‚¹è¯¯å·®æ¨¡å‹\n",
    "\n",
    "ç†è§£ï¼š\n",
    "\n",
    "- FP16/FP8 ä¼šæ”¾å¤§æ¯ä¸ª kernel çš„æ•°å€¼é—®é¢˜\n",
    "\n",
    "- æ‰€ä»¥éƒ¨ç½²å¿…é¡»ä¸¥æ ¼éµå¾ªâ€œç¨³å®šå†™æ³•â€\n",
    "\n",
    "2. ç¨³å®š softmax\n",
    "\n",
    "å¿…è¦æ€§ï¼š\n",
    "\n",
    "- é•¿åºåˆ— attention ä¸­æ˜¯ç¬¬ä¸€å¤§ç‚¸ç‚¹\n",
    "\n",
    "- FlashAttention çš„æ ¸å¿ƒä¸æ˜¯åŠ é€Ÿï¼Œè€Œæ˜¯ç¨³å®š\n",
    "\n",
    "3. LayerNorm / RMSNorm\n",
    "\n",
    "å·¥ç¨‹å¸¸ç”¨æŠ€å·§ï¼š\n",
    "\n",
    "- accumulators = FP32\n",
    "\n",
    "- RMSNorm æ¯” LN æ›´ç¨³\n",
    "\n",
    "4. ç®—æ³•å¤æ‚åº¦\n",
    "\n",
    "ä½ è¦èƒ½ï¼š\n",
    "\n",
    "- ä¼°ç®— FLOPs\n",
    "\n",
    "- åˆ¤æ–­ç“¶é¢ˆï¼ˆmatmul / softmax / KV cacheï¼‰\n",
    "\n",
    "- é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ï¼ˆfusion / low-rank / local attentionï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f0eaa4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0adeed9a",
   "metadata": {},
   "source": [
    "## 8.6 FP16ï¼ˆIEEE 754 binary16ï¼‰å…³é”®æ•°å€¼ä¸€è§ˆ\n",
    "| é¡¹ç›®               | äºŒè¿›åˆ¶è¡¨ç¤º            | åè¿›åˆ¶ç²¾ç¡®å€¼                  | å¸¸ç”¨ç§‘å­¦è®¡æ•°æ³•          |\n",
    "|--------------------|-----------------------|-------------------------------|--------------------------|\n",
    "| æœ€å°æ­£ normal æ•°   | 0 00001 0000000001   | 6.103515625 Ã— 10â»âµ           | 6.103515625e-5          |\n",
    "| æœ€å°æ­£ subnormal æ•°| 0 00000 0000000001   | 5.9604644775390625 Ã— 10â»â¸    | 5.9604644775390625e-8   |\n",
    "| æœ€å¤§ subnormal æ•°  | 0 00000 1111111111   | 6.09765625 Ã— 10â»âµ            | 6.09765625e-5           |\n",
    "| æœ€å¤§ normal æ•°     | 0 11110 1111111111   | 65504                         | 6.5504 Ã— 10â´            |\n",
    "| æ­£é›¶               | 0 00000 0000000000   | 0                             | 0                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feae6f7",
   "metadata": {},
   "source": [
    "å¸¸è§æµ®ç‚¹æ ¼å¼æœ€å°æ­£æ•°å¯¹æ¯”è¡¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8030c472",
   "metadata": {},
   "source": [
    "| æ ¼å¼   | ä½å®½åˆ†é…ï¼ˆç¬¦å·+æŒ‡æ•°+å°¾æ•°ï¼‰ | æœ€å°æ­£ normal æ•°               | æœ€å°æ­£ subnormal æ•°                  | æ˜¯å¦æ”¯æŒ subnormal |\n",
    "|--------|----------------------------|--------------------------------|---------------------------------------|---------------------|\n",
    "| FP16   | 1+5+10                     | 6.103515625 Ã— 10â»âµ            | 5.9604644775390625 Ã— 10â»â¸            | æ”¯æŒ                |\n",
    "| BF16   | 1+8+7                      | 1.17549435 Ã— 10â»Â³â¸            | ä¸æ”¯æŒï¼ˆç›´æ¥ä¸‹æº¢åˆ° 0ï¼‰                | ä¸æ”¯æŒ              |\n",
    "| FP32   | 1+8+23                     | 1.17549435 Ã— 10â»Â³â¸            | 1.40129846 Ã— 10â»â´âµ                   | æ”¯æŒ                |\n",
    "| FP64   | 1+11+52                    | 2.2250738585072014 Ã— 10â»Â³â°â¸   | 4.9406564584124654 Ã— 10â»Â³Â²â´          | æ”¯æŒ                |\n",
    "---\n",
    "| æ ¼å¼ | æœ€å° normal         | æœ€å°æ­£æ•° (subnormal)       | 2çš„å¹‚è¡¨ç¤º   |\n",
    "|------|---------------------|----------------------------|-------------|\n",
    "| FP16 | 6.1035e-5           | 5.9605e-8                  | 2â»Â²â´        |\n",
    "| BF16 | 1.1755e-38          | ä¸æ”¯æŒ                     | -           |\n",
    "| FP32 | 1.1755e-38          | 1.4013e-45                 | 2â»Â¹Â²â¶       |\n",
    "| FP64 | 2.2251e-308         | 4.9407e-324                | 2â»Â¹â°â·â´      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a778c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b74bf1fa",
   "metadata": {},
   "source": [
    "FP16 æœ€å¸¸èƒŒçš„ä¸¤ä¸ªæ•°å­—ï¼ˆ99%çš„æƒ…å†µåªç”¨è®°è¿™ä¸¤ä¸ªï¼‰ï¼š\n",
    "- æœ€å° normal   â†’ 6.103515625e-5   ï¼ˆçº¦ 2â»Â¹â´ï¼‰\n",
    "- æœ€å°æ­£æ•°      â†’ 5.9604644775e-8   ï¼ˆç²¾ç¡® 2â»Â²â´ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062eda8",
   "metadata": {},
   "source": [
    "## 8.7 FP16/FP8 æ•°å€¼ç¨³å®šæ€§å®éªŒåˆé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "326d2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "torch.set_printoptions(sci_mode=True, precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e4771cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element (FP32): 0.0010000000474974513\n",
      "element (FP16): 0.0010004043579101562\n",
      "FP32 sum: 10.000000953674316\n",
      "FP16 sum: 10.0078125\n",
      "FP32(1e-3) as float: 0.0010000000474974513\n",
      "FP16(1e-3) as float: 0.0010004043579101562\n",
      "delta per element: 4.043104127049446e-07\n"
     ]
    }
   ],
   "source": [
    "# FP16 vs FP32 ç®€å•ç´¯åŠ è¯¯å·®\n",
    "import torch\n",
    "\n",
    "N = 10000\n",
    "x = torch.ones(N) * 1e-3  # FP32\n",
    "\n",
    "print(\"element (FP32):\", x[0].item())\n",
    "print(\"element (FP16):\", x[0].half().item())\n",
    "\n",
    "s_fp32 = x.float().sum()\n",
    "s_fp16 = x.half().sum()\n",
    "\n",
    "print(\"FP32 sum:\", s_fp32.item())\n",
    "print(\"FP16 sum:\", s_fp16.item())\n",
    "\n",
    "# çœ‹çœ‹å•ä¸ª 1e-3 åœ¨ FP32 / FP16 ä¸­çš„çœŸå®å€¼\n",
    "v32 = torch.tensor(1e-3, dtype=torch.float32)\n",
    "v16 = torch.tensor(1e-3, dtype=torch.float16)\n",
    "\n",
    "print(\"FP32(1e-3) as float:\", float(v32))\n",
    "print(\"FP16(1e-3) as float:\", float(v16))\n",
    "print(\"delta per element:\", float(v16) - float(v32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 sum: 0.027622273191809654\n",
      "FP16 sum: 0.02764892578125\n",
      "absolute drift: 2.6652589440345764e-05\n"
     ]
    }
   ],
   "source": [
    "# éšæœºæ•°ç´¯åŠ åœ¨ FP32 vs FP16 ä¸‹ï¼Œè¯¯å·®ä¼šéšæœºæ¸¸èµ°ï¼ˆdriftï¼‰ï¼Œä¸æ˜¯ç®€å•çš„ cancel outã€‚\n",
    "# ï¼ˆéšæœº Â±Î´ çš„é•¿æœŸæ¼‚ç§»ï¼‰\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N = 100000\n",
    "x = (torch.rand(N) - 0.5) * 1e-3  # é›¶å‡å€¼å°å™ªå£°\n",
    "\n",
    "s32 = x.float().sum()\n",
    "s16 = x.half().sum()\n",
    "\n",
    "print(\"FP32 sum:\", s32.item())\n",
    "print(\"FP16 sum:\", s16.item())\n",
    "print(\"absolute drift:\", (s16 - s32).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf25747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 a + b: 10000.0009765625\n",
      "FP16 a + b: 10000.0\n",
      "FP32 delta: 0.0009765625\n",
      "FP16 delta: 0.0\n"
     ]
    }
   ],
   "source": [
    "# å¤§æ•° + å°æ•°åœ¨ FP16 ä¸­ï¼Œå°æ•°ä¼šå®Œå…¨è¢«â€œåƒæ‰â€ã€‚\n",
    "# å› ä¸º FP16 çš„æœ‰æ•ˆä½åªæœ‰ 10 ä½ï¼ˆçº¦ 11 ä½ç²¾åº¦ï¼‰ï¼Œå¤§æ•°å’Œå°æ•°ç›¸åŠ æ—¶è¦å¯¹é½æŒ‡æ•°ï¼š\n",
    "# å°æ•°çš„å°¾æ•°ä¼šè¢«å³ç§»å¤šä½å†ç›¸åŠ ã€‚å¦‚æœå°æ•°çš„é‡çº§æ¯”å¤§æ•°çš„ ULPï¼ˆå•ä½é—´è·ï¼‰è¿˜å°ï¼Œå°±è¢«èˆå…¥åˆ° 0ï¼Œç»“æœä¿æŒå¤§æ•°ä¸å˜ã€‚\n",
    "# æ¢å¥è¯è¯´ï¼Œåœ¨æŸä¸ªé‡çº§ä¸‹ FP16 èƒ½è¡¨ç¤ºçš„æœ€å°æ­¥é•¿æ˜¯(2^(e-10)ï¼›å½“å°æ•° < è¿™ä¸ªæ­¥é•¿æ—¶ï¼ŒåŠ æ³•è¢«â€œåƒæ‰â€\n",
    "a32 = torch.tensor(1e4, dtype=torch.float32)\n",
    "b32 = torch.tensor(1e-3, dtype=torch.float32)\n",
    "\n",
    "a16 = a32.half()\n",
    "b16 = b32.half()\n",
    "\n",
    "print(\"FP32 a + b:\", float(a32 + b32))\n",
    "print(\"FP16 a + b:\", float(a16 + b16))\n",
    "\n",
    "print(\"FP32 delta:\", float((a32 + b32) - a32))\n",
    "print(\"FP16 delta:\", float((a16 + b16) - a16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae075450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive exp(x): tensor([inf, inf, inf])\n",
      "Naive softmax: tensor([nan, nan, nan])\n",
      "Stable softmax: tensor([6.65240943e-01, 2.44728476e-01, 9.00305733e-02])\n"
     ]
    }
   ],
   "source": [
    "# naive softmax åœ¨å¤§ logits æ—¶ç›´æ¥ inf / nanï¼Œç¨³å®šå†™æ³•æ²¡é—®é¢˜ã€‚\n",
    "x = torch.tensor([1000.0, 999.0, 998.0])  # æç«¯ logits\n",
    "\n",
    "# naive\n",
    "naive_num = torch.exp(x)\n",
    "naive_den = naive_num.sum()\n",
    "naive = naive_num / naive_den\n",
    "\n",
    "print(\"Naive exp(x):\", naive_num)\n",
    "print(\"Naive softmax:\", naive)\n",
    "\n",
    "# stable\n",
    "stable = F.softmax(x, dim=-1)\n",
    "print(\"Stable softmax:\", stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "071c9bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 diff: 0.007571723312139511\n",
      "Max diff: 0.0038841962814331055\n",
      "Sum32: 1.000000238418579 Sum16: 0.9998015761375427\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡æ‹Ÿé•¿åºåˆ—æ—¶ï¼Œåœ¨ FP16 ä¸‹ softmax çš„ç²¾åº¦æ˜æ˜¾æ¯” FP32 å·®ã€‚\n",
    "torch.manual_seed(0)\n",
    "\n",
    "T = 4096\n",
    "x = torch.randn(T) * 10  # æ”¾å¤§ä¸€äº›ï¼Œæ¨¡æ‹Ÿæ³¨æ„åŠ› logits\n",
    "\n",
    "soft32 = F.softmax(x.float(), dim=-1)\n",
    "soft16 = F.softmax(x.half(), dim=-1).float()\n",
    "\n",
    "print(\"L1 diff:\", (soft16 - soft32).abs().sum().item())\n",
    "print(\"Max diff:\", (soft16 - soft32).abs().max().item())\n",
    "print(\"Sum32:\", soft32.sum().item(), \"Sum16:\", soft16.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "578e0ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean abs diff LN(FP16 vs FP32): 0.056217942386865616\n",
      "Max abs diff: 0.147597536444664\n"
     ]
    }
   ],
   "source": [
    "# æ„é€ ä¸€ç»„æ•°å€¼å¾ˆæ¥è¿‘çš„å‘é‡ï¼Œè§‚å¯Ÿ FP16 åš LN æ—¶ variance è®¡ç®—ä¸ç¨³å®šã€‚\n",
    "torch.manual_seed(0)\n",
    "\n",
    "d = 1024\n",
    "base = torch.ones(d)  # å…¨ 1\n",
    "eps = torch.randn(d) * 1e-3\n",
    "x = base + eps\n",
    "\n",
    "ln = nn.LayerNorm(d)\n",
    "\n",
    "y32 = ln(x.float())\n",
    "y16 = ln(x.half()).float()\n",
    "\n",
    "print(\"Mean abs diff LN(FP16 vs FP32):\", (y16 - y32).abs().mean().item())\n",
    "print(\"Max abs diff:\", (y16 - y32).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([9.99000013e-01, 9.99133348e-01, 9.99266684e-01, 9.99400020e-01, 9.99533355e-01, 9.99666691e-01,\n",
      "        9.99800026e-01, 9.99933362e-01, 1.00006664e+00, 1.00020003e+00, 1.00033331e+00, 1.00046670e+00,\n",
      "        1.00059998e+00, 1.00073338e+00, 1.00086665e+00, 1.00100005e+00])\n",
      "mu32: 1.0 mu16: 1.0\n",
      "var terms FP32: tensor([9.99974304e-07, 7.51084883e-07, 5.37752271e-07, 3.59976411e-07,\n",
      "        2.17757332e-07, 1.11095005e-07, 3.99894340e-08, 4.44062209e-09,\n",
      "        4.44062209e-09, 4.00132762e-08, 1.11095005e-07, 2.17812968e-07,\n",
      "        3.59976411e-07, 5.37839696e-07, 7.51084883e-07, 1.00009345e-06])\n",
      "var terms FP16: tensor([9.53674316e-07, 9.53674316e-07, 9.53674316e-07, 2.38418579e-07,\n",
      "        2.38418579e-07, 2.38418579e-07, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        9.53674316e-07, 9.53674316e-07, 9.53674316e-07, 9.53674316e-07],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# ç›´æ¥æ‰‹å†™ LN çš„å…¬å¼ï¼Œè§‚å¯Ÿ (x - Î¼)^2 åœ¨ FP16 ä¸‹è¢«å‹æ‰ã€‚\n",
    "# å¤š FP16 çš„ variance term ä¼šéå¸¸æ¥è¿‘ 0ã€‚\n",
    "torch.manual_seed(0)\n",
    "\n",
    "d = 16\n",
    "base = torch.ones(d) * 1.0\n",
    "eps = torch.linspace(-1e-3, 1e-3, d)\n",
    "x = base + eps\n",
    "\n",
    "x16 = x.half()\n",
    "mu32 = x.mean()\n",
    "mu16 = x16.mean()\n",
    "\n",
    "var_terms32 = (x - mu32) ** 2\n",
    "var_terms16 = (x16 - mu16) ** 2\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"mu32:\", mu32.item(), \"mu16:\", mu16.item())\n",
    "print(\"var terms FP32:\", var_terms32)\n",
    "print(\"var terms FP16:\", var_terms16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8185d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LN diff (FP16 vs FP32): 0.056217942386865616\n",
      "RMS diff (FP16 vs FP32): 0.00018921459559351206\n"
     ]
    }
   ],
   "source": [
    "# å±•ç¤º RMSNorm ä¸å‡å‡å€¼ï¼Œåœ¨å°å™ªå£°åœºæ™¯ä¸‹æ¯” LN æ›´ç¨³å®šã€‚\n",
    "# RMSNorm çš„ diff ä¼šæ›´å°ã€‚\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., d)\n",
    "        norm = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_hat = x / torch.sqrt(norm + self.eps)\n",
    "        return x_hat * self.weight\n",
    "\n",
    "torch.manual_seed(0)\n",
    "d = 1024\n",
    "base = torch.ones(d) * 1.0\n",
    "eps = torch.randn(d) * 1e-3\n",
    "x = base + eps\n",
    "\n",
    "ln = nn.LayerNorm(d)\n",
    "rms = RMSNorm(d)\n",
    "\n",
    "y_ln32 = ln(x.float())\n",
    "y_ln16 = ln(x.half()).float()\n",
    "\n",
    "y_rms32 = rms(x.float())\n",
    "y_rms16 = rms(x.half()).float()\n",
    "\n",
    "print(\"LN diff (FP16 vs FP32):\", (y_ln16 - y_ln32).abs().mean().item())\n",
    "print(\"RMS diff (FP16 vs FP32):\", (y_rms16 - y_rms32).abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21037060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean abs diff: 0.005231778137385845\n",
      "Max abs diff: 0.03464508056640625\n"
     ]
    }
   ],
   "source": [
    "# å±•ç¤ºçŸ©é˜µä¹˜æ³•ä¸­ï¼Œå¦‚æœå¼ºåˆ¶ç”¨ FP16 ç´¯åŠ ï¼Œè¯¯å·®ä¼šè¿…é€Ÿæ‰©å¤§ï¼ˆPyTorch é»˜è®¤å·²ç»ç”¨ FP32 ç´¯åŠ ï¼‰ã€‚\n",
    "torch.manual_seed(0)\n",
    "\n",
    "m, k, n = 256, 256, 256\n",
    "A = torch.randn(m, k)\n",
    "B = torch.randn(k, n)\n",
    "\n",
    "# å‚è€ƒç»“æœï¼ˆFP32 matmulï¼‰\n",
    "C32 = A @ B\n",
    "\n",
    "# å¼ºåˆ¶ FP16 è®¡ç®—ä¸ç´¯åŠ \n",
    "A16 = A.half()\n",
    "B16 = B.half()\n",
    "\n",
    "# ç›´æ¥ matmul ä»ç„¶å¯èƒ½å†…éƒ¨ç”¨ FP32 ç´¯åŠ ï¼Œæ‰‹åŠ¨å®ç° naive matmul\n",
    "C16_naive = torch.zeros(m, n, dtype=torch.float16)\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        C16_naive[i, j] = (A16[i, :] * B16[:, j]).sum()\n",
    "\n",
    "C16_naive32 = C16_naive.float()\n",
    "print(\"Mean abs diff:\", (C16_naive32 - C32).abs().mean().item())\n",
    "print(\"Max abs diff:\", (C16_naive32 - C32).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70d8ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale: tensor(3.41990590e-01)\n",
      "original std: 10.019336700439453 quant std: 10.018792152404785\n",
      "L2 error: 0.09892825782299042\n",
      "max abs error: 0.1709756851196289\n"
     ]
    }
   ],
   "source": [
    "# ç”¨ç®€å•çš„çº¿æ€§é‡åŒ–æ¨¡æ‹Ÿ FP8ï¼šï¼ˆper-tensor scalingï¼‰\n",
    "# x_fp8 = clamp(round(x / s), -127, 127) * sï¼Œè§‚å¯Ÿ dynamic range vs ç²¾åº¦ã€‚\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def fake_fp8_quant(x, num_bits=8, symmetric=True):\n",
    "    # simple per-tensor symmetric quant\n",
    "    qmax = 2**(num_bits-1) - 1  # 127\n",
    "    max_val = x.abs().max()\n",
    "    s = max_val / qmax if max_val > 0 else 1.0\n",
    "    q = torch.round(x / s).clamp(-qmax, qmax)\n",
    "    x_q = q * s\n",
    "    return x_q, s\n",
    "\n",
    "x = torch.randn(10000) * 10  # å®½åŠ¨æ€èŒƒå›´\n",
    "x_q, s = fake_fp8_quant(x)\n",
    "\n",
    "print(\"scale:\", s)\n",
    "print(\"original std:\", x.std().item(), \"quant std:\", x_q.std().item())\n",
    "print(\"L2 error:\", (x - x_q).pow(2).mean().sqrt().item())\n",
    "print(\"max abs error:\", (x - x_q).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e1e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-tensor mean abs error: 0.06652781367301941\n",
      "Per-channel mean abs error: 0.028985798358917236\n"
     ]
    }
   ],
   "source": [
    "# å±•ç¤º per-channel scaling æ¯” per-tensor å¯¹æŸäº›é€šé“ï¼ˆä¾‹å¦‚å¤§ / å°é‡çº§ï¼‰æ›´å‹å¥½ã€‚\n",
    "# FP8 per-channel scalingï¼ˆæ¯” per-tensor æ›´ç¨³å®šï¼‰\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def fake_fp8_quant_per_channel(W, dim=0, num_bits=8):\n",
    "    qmax = 2**(num_bits-1) - 1\n",
    "    scales = W.abs().amax(dim=dim, keepdim=True)\n",
    "    scales = torch.where(scales == 0, torch.ones_like(scales), scales)\n",
    "    s = scales / qmax\n",
    "    q = torch.round(W / s).clamp(-qmax, qmax)\n",
    "    W_q = q * s\n",
    "    return W_q, s\n",
    "\n",
    "# æ¨¡æ‹Ÿä¸€ä¸ª weight çŸ©é˜µï¼ŒæŸäº›åˆ—å¾ˆå¤§ï¼ŒæŸäº›åˆ—å¾ˆå°\n",
    "W = torch.randn(128, 128) * torch.linspace(0.1, 10.0, 128)\n",
    "\n",
    "W_q_tensor, s_tensor = fake_fp8_quant(W)  # per-tensor\n",
    "W_q_channel, s_channel = fake_fp8_quant_per_channel(W, dim=0)  # per-channel\n",
    "\n",
    "err_tensor = (W - W_q_tensor).abs().mean().item()\n",
    "err_channel = (W - W_q_channel).abs().mean().item()\n",
    "\n",
    "print(\"Per-tensor mean abs error:\", err_tensor)\n",
    "print(\"Per-channel mean abs error:\", err_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eddc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(ref || q): 1.4770594134461135e-06\n",
      "KL(ref || scaled): 0.0005748498952016234\n"
     ]
    }
   ],
   "source": [
    "# FP8 attention logits scaling ç­–ç•¥\n",
    "# æ¨¡æ‹Ÿ attention logits åœ¨ FP8 ä¸­ï¼š\n",
    "# ä¸ scale çš„è¯ logits å®¹æ˜“é¥±å’Œï¼›\n",
    "# åŠ åˆé€‚çš„ scale å¯ä¿ç•™ shapeã€‚\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def fake_fp8_quant_with_scale(x, scale):\n",
    "    qmax = 127\n",
    "    x_scaled = x / scale\n",
    "    q = torch.round(x_scaled).clamp(-qmax, qmax)\n",
    "    return q * scale\n",
    "\n",
    "T = 128\n",
    "logits = torch.randn(T) * 10  # æ¨¡æ‹Ÿ attention logits\n",
    "\n",
    "# ä¸é‡åŒ–\n",
    "p_ref = F.softmax(logits, dim=-1)\n",
    "\n",
    "# ç›´æ¥ FP8 é‡åŒ–ï¼ˆç”¨ per-tensor scaleï¼‰\n",
    "logits_q, s = fake_fp8_quant(logits)\n",
    "p_q = F.softmax(logits_q, dim=-1)\n",
    "\n",
    "# æ›´ aggressive scaleï¼ˆå‡å° logits å¤§å°ï¼‰\n",
    "scale_manual = 4.0\n",
    "logits_scaled = fake_fp8_quant_with_scale(logits, scale_manual)\n",
    "p_scaled = F.softmax(logits_scaled, dim=-1)\n",
    "\n",
    "print(\"KL(ref || q):\", (p_ref * (p_ref / p_q).log()).sum().item())\n",
    "print(\"KL(ref || scaled):\", (p_ref * (p_ref / p_scaled).log()).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "258b594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final L2 diff: 0.00012237243936397135\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡æ‹Ÿ decoder ä¸­çš„ KV cache å¤šæ­¥ä½¿ç”¨ï¼ŒFP16 ä¼šæ¸æ¸åç¦» FP32ã€‚\n",
    "torch.manual_seed(0)\n",
    "\n",
    "T = 512\n",
    "d = 64\n",
    "steps = 128\n",
    "\n",
    "K = torch.randn(T, d)\n",
    "V = torch.randn(T, d)\n",
    "q0 = torch.randn(d)\n",
    "\n",
    "def step(q, K, V, dtype=torch.float32):\n",
    "    q = q.to(dtype)\n",
    "    K = K.to(dtype)\n",
    "    V = V.to(dtype)\n",
    "    logits = (K @ q) / math.sqrt(d)\n",
    "    attn = F.softmax(logits, dim=-1)\n",
    "    out = attn @ V\n",
    "    return out\n",
    "\n",
    "q32 = q0.clone()\n",
    "q16 = q0.clone()\n",
    "\n",
    "for i in range(steps):\n",
    "    q32 = step(q32, K, V, torch.float32)\n",
    "    q16 = step(q16, K, V, torch.float16).float()\n",
    "\n",
    "print(\"Final L2 diff:\", (q32 - q16).pow(2).sum().sqrt().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb4845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 diff: 7.480236519086247e-08\n",
      "Max diff: 5.960464477539063e-08\n"
     ]
    }
   ],
   "source": [
    "# FlashAttention-style å— softmax çš„æ•°å€¼ä¼˜åŠ¿ï¼ˆç©å…·ç¤ºä¾‹ï¼‰\n",
    "# difference should be very very tiny\n",
    "torch.manual_seed(0)\n",
    "\n",
    "T = 1024\n",
    "x = torch.randn(T) * 10\n",
    "\n",
    "# å…¨å±€ stable softmax\n",
    "def global_softmax(x):\n",
    "    m = x.max()\n",
    "    ex = torch.exp(x - m)\n",
    "    return ex / ex.sum()\n",
    "\n",
    "# ç®€åŒ– block softmax\n",
    "def block_softmax(x, block_size=128):\n",
    "    m = -float('inf')\n",
    "    s = 0.0\n",
    "    for i in range(0, len(x), block_size):\n",
    "        xb = x[i:i+block_size]\n",
    "        mb = xb.max()\n",
    "        # æ›´æ–°å…¨å±€ max\n",
    "        m_new = max(m, mb)\n",
    "        # æŒ‰è®ºæ–‡é‡Œçš„æ–¹æ³•ä¿®æ­£æ—§çš„ sum\n",
    "        s = s * math.exp(m - m_new) + torch.exp(xb - m_new).sum().item()\n",
    "        m = m_new\n",
    "    # æœ€ç»ˆå†ç®—ä¸€éè¾“å‡º\n",
    "    out = []\n",
    "    for i in range(0, len(x), block_size):\n",
    "        xb = x[i:i+block_size]\n",
    "        out.append(torch.exp(xb - m) / s)\n",
    "    return torch.cat(out, dim=0)\n",
    "\n",
    "p_global = global_softmax(x)\n",
    "p_block = block_softmax(x)\n",
    "\n",
    "print(\"L1 diff:\", (p_global - p_block).abs().sum().item())\n",
    "print(\"Max diff:\", (p_global - p_block).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2c549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
