{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae158a80",
   "metadata": {},
   "source": [
    "# 第 3 章：近似理论（Approximation Theory for Quantization & Cheap Ops）\n",
    "\n",
    "近似理论回答的问题是：\n",
    "\n",
    "> “在允许一定误差的前提下，如何用更简单的函数 / 更粗的精度来逼近原函数？”\n",
    "\n",
    "在推理加速中主要应用于：\n",
    "\n",
    "- 量化误差建模\n",
    "- 激活函数近似（GELU/SiLU/tanh 等）\n",
    "- 用多项式 / 分段线性函数替代复杂算子\n",
    "- approximate computing（例如用 cheap ops 替换 expensive ops）\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 均匀量化的误差模型\n",
    "\n",
    "考虑步长为 $\\Delta$ 的均匀量化：\n",
    "\n",
    "$$\n",
    "\\hat{w} = Q(w) = \\Delta \\cdot \\text{round}\\Big(\\frac{w}{\\Delta}\\Big)\n",
    "$$\n",
    "\n",
    "\n",
    "定义量化误差：\n",
    "\n",
    "$$\n",
    "e = w - \\hat{w}\n",
    "$$\n",
    "\n",
    "\n",
    "在很多假设下（信号在每个量化区间内分布比较“均匀”），可以近似认为：\n",
    "\n",
    "- $e$ 在 $[-\\Delta/2, \\Delta/2]$ 上均匀分布\n",
    "- 即：$e \\sim \\mathcal{U}(-\\Delta/2, \\Delta/2)$\n",
    "\n",
    "于是：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[e] \\approx 0, \\quad\n",
    "\\mathbb{E}[e^2] = \\text{Var}(e) \\approx \\frac{\\Delta^2}{12}\n",
    "$$\n",
    "\n",
    "\n",
    "**【工程意义】** \n",
    "- 量化噪声方差与步长平方成正比。 \n",
    "- 量化步长 $\\Delta$ 越大 → 误差方差越大\n",
    "- 对某层使用更低 bit（更少级别）时，可以估算噪声能量：$\\sigma_e^2 \\approx \\Delta^2/12$\n",
    "\n",
    "这为“按层动态选择 bit 宽”提供了理论依据：\n",
    "\n",
    "- 对更敏感的层使用小 $\\Delta$（更多 bit）\n",
    "- 对不敏感的层使用大 $\\Delta$（更少 bit）\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 激活函数的近似：以 GELU 为例\n",
    "### 3.2.1 近似数学原理\n",
    "GELU（Gaussian Error Linear Unit） 定义：\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$$\n",
    "\n",
    "\n",
    "其中 $\\Phi(x)$ 是标准正态分布的 CDF：\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-t^2/2} \\, dt\n",
    "$$\n",
    "\n",
    "\n",
    "直接计算 CDF 非常昂贵，因此常用近似：\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5 x \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}}(x + 0.044715 x^3)\\right]\\right)\n",
    "$$\n",
    "\n",
    "```python\n",
    "def gelu_approx(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(\n",
    "        math.sqrt(2 / math.pi) * (x + 0.044715 * x ** 3)\n",
    "    ))\n",
    "\n",
    "```\n",
    "\n",
    "进一步，可以将 tanh 又近似为一个多项式或分段线性函数，使整体计算只包含：\n",
    "\n",
    "- 加法\n",
    "- 乘法\n",
    "- 少量表查\n",
    "\n",
    "**【近似理论视角】**  \n",
    "\n",
    "- 在一个有限区间 $[a,b]$ 上，用多项式 $P_n(x)$ 逼近一个平滑函数 $f(x)$ 是可行的：\n",
    "\n",
    "$$\n",
    "\\|f - P_n\\|_{\\infty, [a,b]} \\to 0 \\quad \\text{(当 n → ∞ 时)}\n",
    "$$\n",
    "\n",
    "\n",
    "- 对应到工程中，就是选取一个多项式阶数 n，使得：\n",
    "  - 逼近误差足够小（精度要求）\n",
    "  - 计算代价足够低（乘法/加法次数）\n",
    "\n",
    "### 3.2.2 为什么用GELU而不是RELU\n",
    "#### 3.2.2.1 数学性质：ReLU 是分段线性，GELU 是平滑可导\n",
    "\n",
    "**ReLU：**\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "- 在 $x=0$ 不可导  \n",
    "- 左侧完全为 0（死区）  \n",
    "- 梯度是 0 或 1（硬切断）\n",
    "\n",
    "**GELU：**\n",
    "\n",
    "$$\n",
    "\\mathrm{GELU}(x) = x \\, \\Phi(x)\n",
    "$$\n",
    "\n",
    "其中 $\\Phi(x)$ 是标准正态分布 CDF。\n",
    "\n",
    "- 全程连续可导  \n",
    "- 梯度平滑  \n",
    "- 适用于深层网络和残差结构\n",
    "\n",
    "➡ **Transformer 需要平滑的非线性，而不是 ReLU 这种“硬切断”。**\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2.2.2 Transformer 的深层结构需要更稳定的激活梯度\n",
    "\n",
    "Transformer 中有大量：\n",
    "\n",
    "- Residual connections  \n",
    "- LayerNorm  \n",
    "- Attention（高度耦合）  \n",
    "- 深 MLP  \n",
    "- 数十甚至数百层深度\n",
    "\n",
    "ReLU 容易造成：\n",
    "\n",
    "- 梯度突然跳变（0 ↔ 1）  \n",
    "- 特征被切死（dead neurons）  \n",
    "- 残差路径中断  \n",
    "- 深层梯度碎裂\n",
    "\n",
    "GELU：\n",
    "\n",
    "- 保持梯度连续  \n",
    "- 让深层网络梯度传播更稳定  \n",
    "- 大幅提升收敛性能\n",
    "\n",
    "➡ **深度 Transformer 结构用 GELU 的训练稳定性显著优于 ReLU。**\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2.2.3 GELU 能学习“输入重要性”，ReLU 不能\n",
    "\n",
    "ReLU 的行为非常粗糙：\n",
    "\n",
    "- $x > 0$：全部通过  \n",
    "- $x \\le 0$：全部切断  \n",
    "\n",
    "不关心“小负数 vs 大负数”的区别。\n",
    "\n",
    "GELU 本质上是：\n",
    "\n",
    "$$\n",
    "\\text{“按概率通过”} = x \\cdot \\Phi(x)\n",
    "$$\n",
    "\n",
    "直觉：\n",
    "\n",
    "- 小正数 → 部分通过  \n",
    "- 小负数 → 部分抑制  \n",
    "- 大正数 → 完全通过  \n",
    "- 大负数 → 完全抑制  \n",
    "\n",
    "➡ **GELU 有 soft gating 机制，比 ReLU 的硬切断强得多。**\n",
    "\n",
    "这点对表示能力和模型容量影响巨大。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2.2.4 实证：所有 SOTA 大模型都不用 ReLU\n",
    "\n",
    "| 模型 | 激活函数 |\n",
    "|------|----------|\n",
    "| BERT | GELU |\n",
    "| GPT-2 / GPT-3 / GPT-4 | GELU / SiLU |\n",
    "| PaLM | GELU |\n",
    "| T5 | GELU |\n",
    "| ViT | GELU |\n",
    "| LLaMA / Qwen | SiLU（Swish） |\n",
    "| Stable Diffusion / DiT | SiLU |\n",
    "\n",
    "ReLU 基本从现代 Transformer 中消失。\n",
    "\n",
    "➡ **行业共识：Transformer 必须用 GELU/SiLU，而不是 ReLU。**\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2.2.5 量化视角：ReLU 输出无界，GELU 输出更“高斯化”\n",
    "\n",
    "**ReLU 输出范围：**\n",
    "\n",
    "$$\n",
    "[0, +\\infty)\n",
    "$$\n",
    "\n",
    "这会导致：\n",
    "\n",
    "- 激活值出现长尾  \n",
    "- int8/int4 量化 scale 不稳定  \n",
    "- 大值压缩小值（降低有效 bit 精度）  \n",
    "- 容易饱和（尤其在 int4 中）\n",
    "\n",
    "**GELU 输出分布：**\n",
    "\n",
    "- 更接近高斯  \n",
    "- 有自然的抑制  \n",
    "- 输出值范围更可控  \n",
    "- 更利于量化（更好的对称性 + 更小的 tail）\n",
    "\n",
    "➡ **GELU 比 ReLU 更适合 quantization-aware inference。**\n",
    "\n",
    "这对你做 Runtime Inference Acceleration 非常关键。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2.2.6 工程层面：GELU 稍贵，但对总推理时间影响极小\n",
    "\n",
    "ReLU 是极快（一个 `max` 操作）。\n",
    "\n",
    "但是 Transformer 的开销主要在：\n",
    "\n",
    "- GEMM / MatMul  \n",
    "- Attention  \n",
    "- MLP Projection\n",
    "\n",
    "激活函数通常 < 1% 的计算成本。\n",
    "\n",
    "GELU 的额外成本可以通过：\n",
    "\n",
    "- fused kernels  \n",
    "- polynomial approximation  \n",
    "- LUT approximation  \n",
    "- fast GELU (tanh-based)  \n",
    "\n",
    "来大幅减少。\n",
    "\n",
    "➡ **GELU 的计算成本远不如它在性能上的好处重要。**\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2.2.7 最终总结\n",
    "| 对比点               | GELU      | ReLU              |\n",
    "| ----------------- | --------- | ----------------- |\n",
    "| 平滑性               | ✔ 连续可导    | ✘ 在 0 不可导         |\n",
    "| 梯度传播              | ✔ 平滑且稳定   | ✘ 梯度跳变 0→1        |\n",
    "| 深层网络（Transformer） | ✔ 强稳定性    | ✘ 易出现梯度碎裂         |\n",
    "| 表达能力              | ✔ “概率性保留” | ✘ “硬切断”           |\n",
    "| 量化友好性             | ✔ 输出分布好   | ✘ 长尾影响 scale      |\n",
    "| 大模型最佳实践           | ✔ 默认选择    | ✘ 不用于 Transformer |\n",
    "| kernel 代价         | 略贵但可接受    | 超快                |\n",
    "\n",
    "Transformer 是深、宽、残差耦合的结构，需要平滑、可导、稳定梯度的激活函数。  \n",
    "GELU 在训练稳定性、梯度流动、表达能力和量化友好性上全面优于 ReLU，  \n",
    "因此现代大模型几乎统一选择 GELU/SiLU，而不是 ReLU。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60df4a",
   "metadata": {},
   "source": [
    "\n",
    "## 3.3 多项式近似的构造（最小二乘 + Chebyshev）\n",
    "### 3.3.1 数学原理\n",
    "给定函数 $f(x)$，希望在区间 $[a,b]$ 上用多项式：\n",
    "\n",
    "$$\n",
    "P_n(x) = \\sum_{k=0}^n c_k x^k\n",
    "$$\n",
    "\n",
    "\n",
    "来逼近它。\n",
    "\n",
    "常见方法：\n",
    "\n",
    "1. **最小二乘拟合**（离散点）：\n",
    "   - 在区间内采样若干点 $x_i$，最小化：\n",
    "\n",
    "$$\n",
    "\\sum_i (f(x_i) - P_n(x_i))^2\n",
    "$$\n",
    "\n",
    "\n",
    "   - 这会导致一个线性最小二乘问题求 $\\{c_k\\}$\n",
    "\n",
    "2. **Chebyshev 逼近**（均匀最大误差最小）：\n",
    "   - 用 Chebyshev 多项式作为基函数\n",
    "   - 在理论上能提供更好的最大误差界\n",
    "使用 Chebyshev 多项式 $T_k(x)$,\n",
    "$$P_n(x)=\\sum_{k=0}^{n} a_k T_k(x)$$\n",
    "\n",
    "**【对推理工程师的要求】**  \n",
    "- 不需要会手推 Chebyshev 多项式\n",
    "- 需要知道：\n",
    "  - 激活函数/特殊函数的近似来自“多项式拟合”或“有理函数拟合”\n",
    "  - 阶数越高、基函数越复杂 → 精度越好，但计算也越贵\n",
    "\n",
    "### 3.3.2 应用场景：在推理加速中用 Chebyshev 多项式逼近激活函数\n",
    "🧩 背景：为什么推理需要逼近激活函数？\n",
    "\n",
    "在部署（尤其是边缘端、车载 NPU、DSP、低功耗 MCU）中：\n",
    "\n",
    "- erf、exp、tanh 这些函数很贵\n",
    "\n",
    "- NPU 没有原生指令（尤其是 exp 和 erf）\n",
    "\n",
    "- 高精度实现需要多次乘加、查表、迭代，太慢\n",
    "\n",
    "- kernel 想达到 1-cycle per op，就必须用简单算子（mul/add）\n",
    "\n",
    "于是工程上最典型的做法是：\n",
    "\n",
    "用一个低阶的多项式或分段多项式来逼近激活函数\n",
    "—— 尽可能便宜，又保证误差在可接受范围内。\n",
    "\n",
    "但是不希望：\n",
    "\n",
    "- 某些点误差很大\n",
    "\n",
    "- 某个 x 区域拟合很差\n",
    "\n",
    "- 影响后续 layernorm、attention、logits\n",
    "\n",
    "- 导致量化误差扩大\n",
    "\n",
    "- 所以要找 全区间最大误差最小 的逼近。\n",
    "\n",
    "这就是 Chebyshev 最小最大（minimax）逼近存在的理由。\n",
    "⭐ 例子：在区间 [-3, 3] 上用 3 阶 Chebyshev 多项式逼近 tanh(x)\n",
    "在 GM 的车载视觉模型里，有一个 SiLU 或 tanh 激活函数。\n",
    "NPU 上没有 tanh 指令，但有：\n",
    "\n",
    "- 加法\n",
    "\n",
    "- 乘法\n",
    "\n",
    "- FMA（fused multiply-add）\n",
    "\n",
    "- bitshift（整数右移）\n",
    "\n",
    "小容量 lookup table\n",
    "\n",
    "目标：\n",
    "写一个只用 mul/add 的自定义 kernel 来跑 tanh。\n",
    "\n",
    "但模型精度敏感，如果近似不好，会影响：\n",
    "\n",
    "- self-attention 输入分布\n",
    "\n",
    "- layernorm 稳定性\n",
    "\n",
    "- logits 分布\n",
    "\n",
    "- 量化 calibration\n",
    "\n",
    "- 最终 AP / accuracy\n",
    "\n",
    "你不能随便拟合一个多项式，需要控制“最坏情况误差”。\n",
    "\n",
    "✔ 使用 Chebyshev 最小最大误差近似\n",
    "目标：\n",
    "\n",
    "在区间 [-3,3] 上找一个 3 次多项式， $P_3(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3$ 使最大误差 $\\max_{x\\in [-3,3]} |P_3(x) - \\tanh(x)|$ 最小\n",
    "Chebyshev 理论告诉我们：\n",
    "\n",
    "- 最优逼近一定是以 Chebyshev 多项式为基\n",
    "\n",
    "- 系数可通过 Remez 交换算法得到\n",
    "\n",
    "- 得到的函数具有 等振性（equioscillation）：\n",
    "\n",
    "- 最大误差在区间内的若干点上完全相等\n",
    "\n",
    "\n",
    "🌟 结果是什么？（工程可用版本）\n",
    "逼近 tanh(x) 的 Chebyshev minimax 多项式（3 阶）可能是： \n",
    "$$P_3^{*}(x) = 0.814 x - 0.066 x^3$$ 而且\n",
    "\n",
    "- 使用 1 个乘方 + 2 个乘法 + 1 个减法\n",
    "\n",
    "- 在 NPU/DSP 上可用 fused ops\n",
    "\n",
    "现在kernel就有\n",
    "```python\n",
    "// 超快 tanh 近似，基于 Chebyshev minimax\n",
    "float fast_tanh(float x) {\n",
    "    float x3 = x * x * x;\n",
    "    return 0.814 * x - 0.066 * x3;\n",
    "}\n",
    "```\n",
    "\n",
    "🌟 Note: 这个和通过泰勒展开得到 然后截断的3阶\n",
    "$\\tanh(x) = x - \\frac{x^3}{3} + \\frac{2x^5}{15} - \\cdots$\n",
    "$$T_3(x) = x - \\frac13 x^3$$\n",
    "是不一样的。 它的性质是：在 𝑥=0 附近误差最小，但远离 0 误差急剧增大； 最小最大误差（minimax）意义下最佳的三次多项式，泰勒展开只有在靠近参考点出才有好的近似价值\n",
    "\n",
    "⭐ 主流加速库使用\n",
    "CoreML 的 fast GELU\n",
    "\n",
    "TensorRT 的 GELU\n",
    "\n",
    "ARM CMSIS 的 tanh/sigmoid\n",
    "\n",
    "TFLite 的 exp/log\n",
    "\n",
    "EdgeTPU 的 activation poly\n",
    "\n",
    "📌 三个方法的差异：\n",
    "| 方法                             | 目标                | 结果                    | 优点         | 缺点              |\n",
    "| ------------------------------ | ----------------- | --------------------- | ---------- | --------------- |\n",
    "| **泰勒近似**                       | 在中心点（(x=0)）附近误差最小 | 例：(x - \\frac13 x^3)   | 公式简单       | 区间外误差巨大，不适合推理部署 |\n",
    "| **最小二乘近似**                     | 平均误差最小            | 系数介于两者之间              | L2-optimal | 最坏点误差可能很大       |\n",
    "| **Chebyshev / Remez 最小最大误差近似** | 最坏点误差最小           | 例：(0.814x - 0.066x^3) | 工程最优、推理最友好 | 计算复杂，需要 Remez   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a9300",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Remez 最小最大误差近似实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096affb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "387e8539",
   "metadata": {},
   "source": [
    "\n",
    "## 3.4 Approximate Computing：用“更便宜”的算子替代\n",
    "\n",
    "典型思路：\n",
    "\n",
    "- 用 shift 替代乘法（当系数接近 $2^k$ 时）\n",
    "- 用 LUT（查表）替代复杂函数（如 exp / log / erf）\n",
    "- 用 piecewise linear 替代 smooth 非线性（如 ReLU6, HardSwish 等）\n",
    "\n",
    "这都可以看作是：\n",
    "\n",
    "> 在允许一定误差的前提下，用“更便宜的算子组合”近似原函数。\n",
    "\n",
    "常见 cheap ops：\n",
    "\n",
    "- 加法（add）\n",
    "\n",
    "- 乘法（mul）\n",
    "\n",
    "- 右移（logical / arithmetic shift）\n",
    "\n",
    "- max / min\n",
    "\n",
    "- LUT + 插值\n",
    "\n",
    "- 分段线性函数（PWL）\n",
    "\n",
    "\n",
    "典型替代策略：\n",
    "\n",
    "- 用 shift 近似乘法 w⋅x≈x≪k\n",
    "\n",
    "- 用分段线性逼近 exp/log\n",
    " - 当 $x \\approx 0$： $e^{x} \\approx 1 + x$（泰勒一阶）\n",
    " - 粗略近似（例如在硬件或 LUT 中）： $e^{x} \\approx 2^{\\mathrm{round}(x / \\ln 2)}$\n",
    "\n",
    "- HardSwish 作为 Swish 近似\n",
    "$$\\mathrm{HS}(x) = x \\cdot \\frac{\\mathrm{ReLU6}(x + 3)}{6}$$\n",
    "\n",
    "**【与你的工作关系】**  \n",
    "\n",
    "- 在设计自定义 kernel 时，你可以：\n",
    "  - 通过数学近似把复杂算子分解成“mul+add+max+shift+table lookup”\n",
    "  - 利用硬件对这些基本算子的高速支持\n",
    "- 在评估近似时，你需要有：\n",
    "  - 定性直觉：误差对下游的影响（例如激活输出范围变窄/变宽）\n",
    "  - 简单定量工具：最大误差、L2 误差等\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 小结：统一视角\n",
    "\n",
    "所有这些近似方法都可以统一写成：\n",
    "\n",
    "$$\n",
    "\\hat{f}\n",
    "= \\arg\\min_{g \\in \\mathcal{H}_{\\text{cheap}}} \\;\\|f - g\\|_{\\Omega}.\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $\\mathcal{H}_{\\text{cheap}}$：由 cheap ops 构成的函数族\n",
    "- $\\Omega$：定义误差的度量（如 $L_2$、$L_\\infty$、KL 等）\n",
    "\n",
    "这说明：\n",
    "- 量化：对数值范围的分段常数逼近\n",
    "- 激活近似：对激活函数的多项式/分段逼近\n",
    "- LUT + 插值：piecewise polynomial 逼近\n",
    "- shift-based kernel：对乘法算子的近似\n",
    "\n",
    "本质：在更低复杂度的函数族中寻找最优逼近。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70cc31c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
