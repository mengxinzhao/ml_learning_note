{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd670e5d",
   "metadata": {},
   "source": [
    "# 第 7 章：计算图与图论（Computational Graph & Graph Theory）\n",
    "\n",
    "本章关注：\n",
    "\n",
    "- 如何用 DAG（有向无环图）表示模型\n",
    "- graph rewrite / fusion 的数学抽象\n",
    "- 如何基于图结构做 scheduling / partitioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11def7b4",
   "metadata": {},
   "source": [
    "## 7.1 模型即计算图（DAG）\n",
    "\n",
    "现代深度学习模型在推理框架/编译器内部都表示为 **有向无环图 DAG（Directed Acyclic Graph）**。\n",
    "\n",
    "- **节点（Node）**：算子（Op），例如 MatMul、Add、GELU、LayerNorm、Conv、Transpose …\n",
    "- **边（Edge）**：张量的数据流，表示谁的输出被谁作为输入使用\n",
    "\n",
    "由于图是无环的（没有 op 依赖未来的结果），所以可以：\n",
    "\n",
    "- 对图进行 **拓扑排序（topological sort）** 得到合法执行顺序\n",
    "- 在图上做依赖分析、内存规划、并行调度等各种优化\n",
    "\n",
    "下面用 PyTorch FX 展示一个真实 IR（中间表示）的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe0c9b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /opt/miniconda3/lib/python3.13/site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "=== Tabular View ===\n",
      "opcode       name    target    args     kwargs\n",
      "-----------  ------  --------  -------  --------\n",
      "placeholder  x       x         ()       {}\n",
      "call_module  fc1     fc1       (x,)     {}\n",
      "call_module  relu    relu      (fc1,)   {}\n",
      "call_module  fc2     fc2       (relu,)  {}\n",
      "output       output  output    (fc2,)   {}\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fx as fx\n",
    "\n",
    "class MyBlock(nn.Module):\n",
    "    def __init__(self, in_features, hidden):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # y = fc2(relu(fc1(x)))\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "model = MyBlock(16, 32)\n",
    "gm = fx.symbolic_trace(model)\n",
    "\n",
    "print(\"\\n=== Tabular View ===\")\n",
    "gm.graph.print_tabular()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de91119",
   "metadata": {},
   "source": [
    "**说明：**\n",
    "\n",
    "- `gm` 是一个 `GraphModule`，内部的 `gm.graph` 就是 FX 的 **IR（Intermediate Representation，中间表示）**\n",
    "- 每个 `node` 是一个算子（`call_module`, `placeholder`, `output` 等）\n",
    "- `x → fc1 → relu → fc2 → output` 构成一条计算链，这本质上就是一个 **DAG**\n",
    "\n",
    "在 TensorRT / XLA / TVM 里，IR 结构更复杂，但本质是同一类东西：  \n",
    "**模型被抽象为“图 + 节点属性 + 边上的张量信息”**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9b03d",
   "metadata": {},
   "source": [
    "## 7.2 图重写（Graph Rewrite / Transform）\n",
    "\n",
    "图重写的目标：\n",
    "\n",
    "> 在保持 **输入输出语义完全一致** 的前提下，通过等价替换获得更高推理效率。\n",
    "\n",
    "常见操作包括：\n",
    "\n",
    "- **算子融合（Operator Fusion）**：如 MatMul + Bias + GELU → FusedMatMulBiasGelu\n",
    "- **常量折叠（Constant Folding）**：所有只依赖常量的子图在编译期算掉\n",
    "- **消除冗余（Elimination）**：合并/消除多余的 reshape / transpose / cast 等\n",
    "\n",
    "这一节先用 FX 做一个实际可运行的 **Linear + ReLU 融合** 示例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d29c9eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Fused FX Graph ===\n",
      "opcode       name    target    args     kwargs\n",
      "-----------  ------  --------  -------  --------\n",
      "placeholder  x       x         ()       {}\n",
      "call_module  fc1     fc1       (x,)     {}\n",
      "call_module  relu    relu      (fc1,)   {}\n",
      "call_module  fc2     fc2       (relu,)  {}\n",
      "output       output  output    (fc2,)   {}\n",
      "=== Fused FX Graph ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %fc1_relu_fused : [num_users=1] = call_module[target=fc1_relu_fused](args = (%x,), kwargs = {})\n",
      "    %fc2 : [num_users=1] = call_module[target=fc2](args = (%fc1_relu_fused,), kwargs = {})\n",
      "    return fc2\n",
      "opcode       name            target          args               kwargs\n",
      "-----------  --------------  --------------  -----------------  --------\n",
      "placeholder  x               x               ()                 {}\n",
      "call_module  fc1_relu_fused  fc1_relu_fused  (x,)               {}\n",
      "call_module  fc2             fc2             (fc1_relu_fused,)  {}\n",
      "output       output          output          (fc2,)             {}\n",
      "tensor(0., grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "class LinearReLU(nn.Module):\n",
    "    \"\"\"将 Linear + ReLU 融合成一个模块，用于示例。\"\"\"\n",
    "    def __init__(self, linear: nn.Linear):\n",
    "        super().__init__()\n",
    "        # 复用原来的权重和 bias\n",
    "        self.linear = linear\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.linear(x))\n",
    "\n",
    "\n",
    "def fuse_linear_relu(gm: fx.GraphModule) -> fx.GraphModule:\n",
    "    graph = gm.graph\n",
    "    modules = dict(gm.named_modules())\n",
    "\n",
    "    print(\"=== Before Fused FX Graph ===\")\n",
    "    graph.print_tabular()\n",
    "\n",
    "    for node in list(graph.nodes):\n",
    "        # 找 ReLU\n",
    "        if node.op != \"call_module\":\n",
    "            continue\n",
    "        if not isinstance(modules.get(node.target, None), nn.ReLU):\n",
    "            continue\n",
    "\n",
    "        relu_node = node\n",
    "        prev = relu_node.args[0]\n",
    "\n",
    "        # 要求前一个是 Linear\n",
    "        if prev.op != \"call_module\":\n",
    "            continue\n",
    "        if not isinstance(modules.get(prev.target, None), nn.Linear):\n",
    "            continue\n",
    "\n",
    "        linear_node = prev\n",
    "\n",
    "        # 安全检查：Linear 的输出只被 ReLU 用过一次\n",
    "        if len(linear_node.users) != 1:\n",
    "            # 如果 Linear 还有别的用户，就先别 fuse，避免改错语义\n",
    "            continue\n",
    "\n",
    "        linear_module = modules[linear_node.target]\n",
    "\n",
    "        fused = LinearReLU(linear_module)\n",
    "        fused_name = linear_node.target + \"_relu_fused\"\n",
    "        gm.add_submodule(fused_name, fused)\n",
    "\n",
    "        # 新节点用 linear 的输入作为输入\n",
    "        with graph.inserting_after(linear_node):\n",
    "            new_node = graph.call_module(fused_name, args=linear_node.args)\n",
    "\n",
    "        # 所有用 ReLU 输出的地方改为用 fused\n",
    "        relu_node.replace_all_uses_with(new_node)\n",
    "\n",
    "        # 删除 ReLU 节点和 Linear 节点 —— 此时图里只剩 fused\n",
    "        graph.erase_node(relu_node)\n",
    "        graph.erase_node(linear_node)\n",
    "\n",
    "    graph.lint()\n",
    "    gm.recompile()\n",
    "    return gm\n",
    "    \n",
    "# 对上一节中的 gm 做融合\n",
    "model = MyBlock(16, 32)\n",
    "gm = fx.symbolic_trace(model)\n",
    "\n",
    "gm_fused = fuse_linear_relu(gm)\n",
    "print(\"=== Fused FX Graph ===\")\n",
    "print(gm_fused.graph)\n",
    "\n",
    "gm.graph.print_tabular()\n",
    "\n",
    "# 验证融合前后结果相同\n",
    "x = torch.randn(4, 16)\n",
    "y_ref = model(x)\n",
    "y_fused = gm_fused(x)\n",
    "\n",
    "# should be very close to 0\n",
    "print((y_ref - y_fused).abs().max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d97297",
   "metadata": {},
   "source": [
    "**工程含义：**\n",
    "\n",
    "- 我们在 **IR 上进行 pattern 匹配和替换** → 这就是图重写\n",
    "- 真实框架中的 fusion pass 本质也是：  \n",
    "  找到 `P` 形式的子图，用更高效的 `R` 形式子图替换\n",
    "\n",
    "在车端推理（batch=1）场景下：\n",
    "\n",
    "- Kernel launch 开销与 DRAM 往返代价非常显著\n",
    "- 通过 fusion，减少中间 tensor 写回内存、减少 kernel 数量，是性能优化的关键步骤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d422f",
   "metadata": {},
   "source": [
    "### 7.2.2 ONNX 风格 IR：MatMul + Add Fusion 伪代码示例\n",
    "\n",
    "真实的 ONNX Runtime / TensorRT / TVM 都有自己的 IR。  \n",
    "下面构造一个极简 Graph IR，模仿 ONNX 风格。然后构造一个 `MatMul → Add → Relu` 的小子图，模拟从 PyTorch 导出的 ONNX 子图，然后写一个简化的 **MatMul + Add → FusedMatMulBias** 的融合 pass，逻辑与真实框架类似：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42e1af39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphIR(nodes=[Node(name='matmul1', op_type='MatMul', inputs=['X', 'W'], outputs=['Z'], attrs={}), Node(name='add1', op_type='Add', inputs=['Z', 'B'], outputs=['Z2'], attrs={}), Node(name='relu1', op_type='Relu', inputs=['Z2'], outputs=['Y'], attrs={})], initializers={'W': '...权重 tensor...', 'B': '...bias tensor...'}, inputs=['X'], outputs=['Y'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    name: str\n",
    "    op_type: str\n",
    "    inputs: List[str]\n",
    "    outputs: List[str]\n",
    "    attrs: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class GraphIR:\n",
    "    nodes: List[Node]\n",
    "    initializers: Dict[str, Any]  # 常量 tensor\n",
    "    inputs: List[str]\n",
    "    outputs: List[str]\n",
    "\n",
    "graph = GraphIR(\n",
    "    nodes=[\n",
    "        Node(\"matmul1\", \"MatMul\", [\"X\", \"W\"], [\"Z\"]),\n",
    "        Node(\"add1\", \"Add\", [\"Z\", \"B\"], [\"Z2\"]),\n",
    "        Node(\"relu1\", \"Relu\", [\"Z2\"], [\"Y\"]),\n",
    "    ],\n",
    "    initializers={\n",
    "        \"W\": \"...权重 tensor...\",\n",
    "        \"B\": \"...bias tensor...\",\n",
    "    },\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\"],\n",
    ")\n",
    "\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2846de88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphIR(nodes=[Node(name='matmul1_fused', op_type='FusedMatMulBias', inputs=['X', 'W', 'B'], outputs=['Z2'], attrs={}), Node(name='add1', op_type='Add', inputs=['Z', 'B'], outputs=['Z2'], attrs={}), Node(name='relu1', op_type='Relu', inputs=['Z2'], outputs=['Y'], attrs={})], initializers={'W': '...权重 tensor...', 'B': '...bias tensor...'}, inputs=['X'], outputs=['Y'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fuse_matmul_add(graph: GraphIR) -> GraphIR:\n",
    "    \"\"\"将 MatMul + Add (bias 为常量) 融合为单个 FusedMatMulBias 节点。\"\"\"\n",
    "    new_nodes: List[Node] = []\n",
    "    for node in graph.nodes:\n",
    "        if node.op_type == \"MatMul\":\n",
    "            matmul = node\n",
    "            matmul_out = matmul.outputs[0]\n",
    "\n",
    "            add_node = None\n",
    "            for n in graph.nodes:\n",
    "                if n.op_type == \"Add\" and matmul_out in n.inputs:\n",
    "                    add_node = n\n",
    "                    break\n",
    "\n",
    "            if add_node is not None:\n",
    "                # 另一个输入视作 bias\n",
    "                bias_name = [i for i in add_node.inputs if i != matmul_out][0]\n",
    "                if bias_name in graph.initializers:\n",
    "                    fused = Node(\n",
    "                        name=matmul.name + \"_fused\",\n",
    "                        op_type=\"FusedMatMulBias\",\n",
    "                        inputs=[matmul.inputs[0], matmul.inputs[1], bias_name],\n",
    "                        outputs=add_node.outputs,\n",
    "                    )\n",
    "                    new_nodes.append(fused)\n",
    "                    continue  # 不再保留原 MatMul / Add\n",
    "\n",
    "        # 默认保留\n",
    "        new_nodes.append(node)\n",
    "\n",
    "    graph.nodes = new_nodes\n",
    "    return graph\n",
    "\n",
    "fused_graph = fuse_matmul_add(graph)\n",
    "fused_graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4b757",
   "metadata": {},
   "source": [
    "**说明：**\n",
    "\n",
    "- 我们在 `GraphIR` 上做 pattern 匹配和替换，这就是 ONNX / TensorRT 里常见的 fusion pass 思路\n",
    "- 实际工程中会：\n",
    "  - 处理更多类型（Conv+BN+ReLU、LayerNorm+Add 等）\n",
    "  - 使用更复杂的 pattern 表达（子图匹配）\n",
    "  - 引入 cost model，根据形状/设备/数据类型决策是否值得融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8ef0f",
   "metadata": {},
   "source": [
    "### 7.2.3 Constant Folding（常量折叠）\n",
    "\n",
    "如果某个节点的所有输入均为常量，则可以在编译期直接算出结果，并将输出标记为新的常量，删除该节点。\n",
    "**工程意义：**\n",
    "\n",
    "- 编译期提前消掉常量路径，推理时可以少跑很多节点\n",
    "- 对于大模型（尤其是含有大量常量子图的结构），constant folding 可以显著减少图规模\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7dd128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before folding: dict_keys(['C1', 'C2']) ['add_const', 'add_mixed']\n",
      "After folding: dict_keys(['C1', 'C2', 'C3']) ['add_mixed']\n",
      "C3 value: [5 7 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def constant_folding(graph: GraphIR) -> GraphIR:\n",
    "    \"\"\"对 GraphIR 执行简化版常量折叠：这里只处理 Add 节点。\"\"\"\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        new_nodes: List[Node] = []\n",
    "        for node in graph.nodes:\n",
    "            if node.op_type == \"Add\":\n",
    "                inp0, inp1 = node.inputs\n",
    "                if inp0 in graph.initializers and inp1 in graph.initializers:\n",
    "                    v0 = np.array(graph.initializers[inp0])\n",
    "                    v1 = np.array(graph.initializers[inp1])\n",
    "                    out_name = node.outputs[0]\n",
    "                    graph.initializers[out_name] = (v0 + v1)\n",
    "                    changed = True\n",
    "                    # 这个 Add 节点可以被删除，不加入 new_nodes\n",
    "                    continue\n",
    "            new_nodes.append(node)\n",
    "        graph.nodes = new_nodes\n",
    "    return graph\n",
    "\n",
    "# 构造一个简单示例\n",
    "cf_graph = GraphIR(\n",
    "    nodes=[\n",
    "        Node(\"add_const\", \"Add\", [\"C1\", \"C2\"], [\"C3\"]),\n",
    "        Node(\"add_mixed\", \"Add\", [\"X\", \"C3\"], [\"Y\"]),\n",
    "    ],\n",
    "    initializers={\n",
    "        \"C1\": np.array([1, 2, 3]),\n",
    "        \"C2\": np.array([4, 5, 6]),\n",
    "    },\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\"],\n",
    ")\n",
    "\n",
    "print(\"Before folding:\", cf_graph.initializers.keys(), [n.name for n in cf_graph.nodes])\n",
    "cf_graph = constant_folding(cf_graph)\n",
    "print(\"After folding:\", cf_graph.initializers.keys(), [n.name for n in cf_graph.nodes])\n",
    "print(\"C3 value:\", cf_graph.initializers[\"C3\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cfe609",
   "metadata": {},
   "source": [
    "## 7.3 图划分与多设备并行（Partitioning & Parallelism）\n",
    "\n",
    "在车载/边缘设备中，通常存在多种计算单元：\n",
    "\n",
    "- 多核 CPU\n",
    "- GPU\n",
    "- NPU / DSP（自研加速器）\n",
    "- 有时还有特定功能单元（ISP、编码器等）\n",
    "\n",
    "需要根据 **图结构 + 设备特性** 决定：\n",
    "\n",
    "- 哪个子图放在 CPU 上\n",
    "- 哪个子图放在 GPU / NPU 上\n",
    "- 如何减少跨设备的数据拷贝（边 cut）\n",
    "\n",
    "数学上是一个 **图划分（graph partitioning）** 问题，一般是 NP-hard，只能用启发式方法/代价模型。\n",
    "\n",
    "### 7.3.1 简单 Partition 示例：Conv/MatMul → NPU，其余 → CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4671201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 子图节点: ['add1', 'relu1']\n",
      "NPU 子图节点: ['matmul1_fused']\n"
     ]
    }
   ],
   "source": [
    "def simple_partition(graph: GraphIR):\n",
    "    \"\"\"一个非常简化的 partition 示例：Conv/MatMul 放 NPU，其余放 CPU。\"\"\"\n",
    "    cpu_nodes = []\n",
    "    npu_nodes = []\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        if node.op_type in [\"Conv\", \"MatMul\", \"FusedMatMulBias\"]:\n",
    "            npu_nodes.append(node)\n",
    "        else:\n",
    "            cpu_nodes.append(node)\n",
    "\n",
    "    return cpu_nodes, npu_nodes\n",
    "\n",
    "# 这里复用前面的 fused_graph 作为示例\n",
    "cpu_subgraph, npu_subgraph = simple_partition(fused_graph)\n",
    "\n",
    "print(\"CPU 子图节点:\", [n.name for n in cpu_subgraph])\n",
    "print(\"NPU 子图节点:\", [n.name for n in npu_subgraph])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38668e8",
   "metadata": {},
   "source": [
    "真实系统中还会考虑：\n",
    "\n",
    "- 设备是否支持该算子、数据类型（如 INT8 / FP16）\n",
    "- 各设备的算力、带宽、latency\n",
    "- 跨设备拷贝的代价（PCIe / 内存总线）\n",
    "- 内存容量限制\n",
    "\n",
    "但本质思路都是在 **IR 的图结构上进行子图划分**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce8004",
   "metadata": {},
   "source": [
    "## 7.4 Liveness 分析与内存规划（Memory Planning）\n",
    "\n",
    "边缘设备/车载 ECU 常见瓶颈：\n",
    "\n",
    "- DRAM 容量有限\n",
    "- 带宽有限\n",
    "- activation 远大于参数大小\n",
    "\n",
    "因此需要：\n",
    "\n",
    "- 分析每个 tensor 的**生命周期（liveness interval）**\n",
    "- 通过 **区间图着色（interval graph coloring）** 方式复用 buffer\n",
    "- 降低峰值内存占用（peak memory）\n",
    "\n",
    "### 7.4.1 构造一个简单执行计划并做 Liveness 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdde3840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 OpSimple(name=op1, inputs=['x'], outputs=['t1'])\n",
      "1 OpSimple(name=op2, inputs=['t1'], outputs=['t2'])\n",
      "2 OpSimple(name=op3, inputs=['t2'], outputs=['t3'])\n",
      "3 OpSimple(name=op4, inputs=['t1'], outputs=['t4'])\n",
      "4 OpSimple(name=op5, inputs=['t3', 't4'], outputs=['y'])\n",
      "Tensor liveness intervals:\n",
      "  t1: [0, 3]\n",
      "  x: [0, 0]\n",
      "  t2: [1, 2]\n",
      "  t3: [2, 4]\n",
      "  t4: [3, 4]\n",
      "  y: [4, 4]\n"
     ]
    }
   ],
   "source": [
    "class OpSimple:\n",
    "    def __init__(self, name, inputs, outputs):\n",
    "        self.name = name\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"OpSimple(name={self.name}, inputs={self.inputs}, outputs={self.outputs})\"\n",
    "\n",
    "\n",
    "ops = [\n",
    "    OpSimple(\"op1\", [\"x\"], [\"t1\"]),\n",
    "    OpSimple(\"op2\", [\"t1\"], [\"t2\"]),\n",
    "    OpSimple(\"op3\", [\"t2\"], [\"t3\"]),\n",
    "    OpSimple(\"op4\", [\"t1\"], [\"t4\"]),\n",
    "    OpSimple(\"op5\", [\"t3\", \"t4\"], [\"y\"]),\n",
    "]\n",
    "\n",
    "for i, op in enumerate(ops):\n",
    "    print(i, op)\n",
    "\n",
    "def compute_liveness(ops):\n",
    "    \"\"\"计算每个 tensor 的 [start, end] 生命周期区间。\"\"\"\n",
    "    first_use = {}\n",
    "    last_use = {}\n",
    "\n",
    "    for idx, op in enumerate(ops):\n",
    "        # 输出：birth\n",
    "        for out in op.outputs:\n",
    "            if out not in first_use:\n",
    "                first_use[out] = idx\n",
    "            last_use[out] = idx\n",
    "\n",
    "        # 输入：被使用\n",
    "        for inp in op.inputs:\n",
    "            if inp not in first_use:\n",
    "                # 认为图输入在 0 之前就存在\n",
    "                first_use[inp] = 0\n",
    "            last_use[inp] = idx\n",
    "\n",
    "    intervals = {t: (first_use[t], last_use[t]) for t in first_use}\n",
    "    return intervals\n",
    "\n",
    "intervals = compute_liveness(ops)\n",
    "print(\"Tensor liveness intervals:\")\n",
    "for t, (s, e) in intervals.items():\n",
    "    print(f\"  {t}: [{s}, {e}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee45967",
   "metadata": {},
   "source": [
    "### 7.4.2 区间图着色（Interval Coloring）实现 Buffer 复用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "193d91d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer assignment:\n",
      "  tensor t1 -> buffer 0\n",
      "  tensor x -> buffer 1\n",
      "  tensor t2 -> buffer 1\n",
      "  tensor t3 -> buffer 2\n",
      "  tensor t4 -> buffer 1\n",
      "  tensor y -> buffer 0\n",
      "Total buffers used: 3\n"
     ]
    }
   ],
   "source": [
    "def allocate_buffers(intervals):\n",
    "    \"\"\"使用最简单的 greedy 算法为每个 tensor 分配 buffer ID。\"\"\"\n",
    "    # 按照开始时间排序\n",
    "    items = sorted(intervals.items(), key=lambda x: x[1][0])\n",
    "    buffers = []  # 每个 buffer 当前的 end 时间\n",
    "    assignment = {}\n",
    "\n",
    "    for tensor, (start, end) in items:\n",
    "        assigned = False\n",
    "        for buf_id, buf_end in enumerate(buffers):\n",
    "            if start > buf_end:\n",
    "                # 该 buffer 已空闲，可复用\n",
    "                buffers[buf_id] = end\n",
    "                assignment[tensor] = buf_id\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            # 需要新建一个 buffer\n",
    "            new_id = len(buffers)\n",
    "            buffers.append(end)\n",
    "            assignment[tensor] = new_id\n",
    "\n",
    "    return assignment, len(buffers)\n",
    "\n",
    "buf_assign, num_buf = allocate_buffers(intervals)\n",
    "print(\"Buffer assignment:\")\n",
    "for t, b in buf_assign.items():\n",
    "    print(f\"  tensor {t} -> buffer {b}\")\n",
    "print(\"Total buffers used:\", num_buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d613d30",
   "metadata": {},
   "source": [
    "**含义：**\n",
    "\n",
    "- 多个 tensor 在时间上“错峰”使用同一个 buffer，从而降低了内存峰值\n",
    "- 在真实框架中还会考虑：\n",
    "  - tensor 大小不同\n",
    "  - 对齐（alignment）\n",
    "  - 不同设备（CPU/GPU/NPU）各自的内存池\n",
    "  - activation checkpointing / recompute 策略\n",
    "\n",
    "但其数学内核就是：**区间图着色问题**。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff988cf",
   "metadata": {},
   "source": [
    "⭐ 1. 工程中是否手写区间着色算法？\n",
    "**不需要**。\n",
    "\n",
    "**内存规划完全由编译器自动完成。**\n",
    "\n",
    "几乎所有现代推理编译器 / runtime 都内置了内存规划器：\n",
    "| 框架                           | 内存规划组件                             | 是否使用图着色思想                              |\n",
    "| ---------------------------- | ---------------------------------- | -------------------------------------- |\n",
    "| **TensorRT**                 | TensorRT Memory Planner            | ✔️ 是，buffer reuse + lifetime inference |\n",
    "| **TensorFlow Lite**          | Arena Planner                      | ✔️ 是（interval reuse）                   |\n",
    "| **ONNX Runtime**             | Memory Planner / Custom Allocators | ✔️                                     |\n",
    "| **XLA**                      | Global Buffer Assignment           | ✔️（区间 + interference graph coloring）   |\n",
    "| **TVM**                      | Memory Plan Pass                   | ✔️（reuse + sharing + pool allocator）   |\n",
    "| **PyTorch 2.0 AOT Autograd** | Memory Planner                     | ✔️                                     |\n",
    "| **AITemplate**               | Memory Planning Pass               | ✔️（GPU static memory planning）         |\n",
    "| **CoreML**                   | Storage Fuse Optimization          | ✔️                                     |\n",
    "所以在工业界：\n",
    "\n",
    "工程师不会手工分配 tensor buffer，所有内存复用都通过 compile-time pass 自动生成。\n",
    "\n",
    "---\n",
    "2. 工程师需要写的是“策略”，不是着色算法本身\n",
    "内存规划分为两个层次：\n",
    "（1）算法层（内核级）— 自动完成\n",
    "\n",
    "包括：\n",
    "\n",
    "- liveness interval 计算\n",
    "\n",
    "- interference graph 构建\n",
    "\n",
    "- interval graph coloring\n",
    "\n",
    "- global memory assignment\n",
    "\n",
    "- pool / arena / chunking\n",
    "\n",
    "这些都由框架本身实现，工程师不手写。\n",
    "例如 TensorRT 内存规划：\n",
    "- Build dependency DAG\n",
    "- Compute tensor liveness\n",
    "- Build interference graph\n",
    "- Allocate global memory pool\n",
    "- Assign offsets for each tensor\n",
    "\n",
    "\n",
    "（2）策略层（policy level）— 工程师需要参与\n",
    "\n",
    "你需要提供/调整：\n",
    "\n",
    "- 哪些 tensor 强制不复用（如 debug dump、persistent memory）\n",
    "\n",
    "- 哪些 tensor 允许 in-place\n",
    "\n",
    "- 哪些 op 可以 in-place 推导（Add、ReLU、BiasAdd 等）\n",
    "\n",
    "- 不同 device（CPU/NPU/GPU）是否共用 memory pool\n",
    "\n",
    "- 特殊 tensor 是否 pin memory / lock memory\n",
    "\n",
    "- 特定硬件（NPU）是否需要 alignment=64/128/256\n",
    "\n",
    "- 需要的 buffer chunk size（如 quantization scratch buffer）\n",
    "\n",
    "- mixed precision 是否改变 tensor 大小\n",
    "\n",
    "- dynamic shape 时是否使用 cheapest-fit / first-fit / dynamic pool\n",
    "\n",
    "你写的是 “规则”，由编译器根据规则生成最终 layout。\n",
    "\n",
    "（3） 优化角度就是算出一个 tensor buffer 的最小化分配策略，满足所有 tensor 的内存需求\n",
    "\n",
    "原则上：是的，内存规划器的目标就是最小化峰值内存。\n",
    "\n",
    "但实际工程上会用启发式策略（近似最优），不是严格全局最优。\n",
    "\n",
    "为什么？\n",
    "因为严格求最优是 NP-hard（图着色问题）。\n",
    "\n",
    "实际框架用的方法：\n",
    "\n",
    "- Greedy first-fit / best-fit\n",
    "\n",
    "- Interval coloring + heuristics\n",
    "\n",
    "- Chunk pool / arena allocator\n",
    "\n",
    "- Buffer coalescing\n",
    "\n",
    "- In-place propagation\n",
    "\n",
    "这些方法很快（O(n log n) ~ O(n)），且在图形化 workload（DNN）中通常接近最优。\n",
    "\n",
    "（4） 工程实际：Tensor buffer 就是\n",
    "\n",
    "- CPU RAM buffer\n",
    "\n",
    "- GPU VRAM buffer\n",
    "\n",
    "- NPU SRAM/DRAM buffer\n",
    "\n",
    "- 或 device-specific unified memory\n",
    "\n",
    "它们都是：\n",
    "\n",
    "实际的物理内存池里的一个连续区间（offset + size）。\n",
    "\n",
    "框架会把每个 tensor 输出分配到：\n",
    "\n",
    "global_memory_pool[offset : offset + size]\n",
    "\n",
    "并确保：\n",
    "\n",
    "- 活跃区间不重叠\n",
    "\n",
    "- 对齐（alignment）满足硬件要求\n",
    "\n",
    "- 不同设备可能有不同 memory pool\n",
    "\n",
    "（5）. 在实际工程中，你会做什么？\n",
    "\n",
    "你做的是 Runtime Inference / Deployment Engineer，所以你会：\n",
    "\n",
    "- 理解 IR + liveness + memory planner 的逻辑\n",
    "     -  某个 tensor 为什么不能复用 buffer\n",
    "\n",
    "     - 内存峰值为什么上不去（瓶颈 subgraph）\n",
    "\n",
    "     - 哪个 pass 把某些 in-place 机会给消掉了\n",
    "\n",
    "     -  NPU 的 memory layout 改变是否导致 buffer 分裂\n",
    "\n",
    "- 写策略（policy rules）\n",
    "     - 某些算子（BatchNorm、LayerNorm）可以 in-place\n",
    "\n",
    "     - 某些算子必须分配新 buffer（因为 hardware hazard）\n",
    "\n",
    "     - 不同 device 要分离 memory pool 或共用\n",
    "\n",
    "     -  增加特殊 alignment（比如 128 bytes）以符合 NPU DMA 要求\n",
    "\n",
    "     - 动态 shape 模型用 dynamic memory pool 而不是 static graph plan\n",
    "\n",
    "- 调试和验证\n",
    "\n",
    "     - 查看内存分配 trace（很多 runtime 会 output memory timeline）\n",
    "\n",
    "     - 优化 out-of-memory 场景\n",
    "\n",
    "     - 减少 peak memory，使大模型跑上车载设备\n",
    "\n",
    "     - 在 low-memory 设备上通过：\n",
    "          - activation recompute\n",
    "          - chunking\n",
    "          - streaming\n",
    "\n",
    "    - 降低 footprint\n",
    "\n",
    "- 协助实现（或改进）某些 pass\n",
    "     - “linear fusion” pass 后 op 的 size 变了，会影响 memory planning\n",
    "\n",
    "     - 优化 transpose folding，会减少中间 tensor 数量\n",
    "\n",
    "     - quantization pass 改变 tensor type 和 size，影响 buffer reuse\n",
    "\n",
    "     - operator placement（CPU/GPU/NPU）影响 memory pool 结构\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ec439",
   "metadata": {},
   "source": [
    "## 7.5 工程级小结：图论如何支撑 Runtime Inference 优化\n",
    "\n",
    "| 图论概念 | 工程价值 | 对 Runtime Inference / Edge Deployment 的作用 |\n",
    "|---------|----------|-----------------------------------------------|\n",
    "| DAG（有向无环图） | 定义执行顺序与依赖关系 | 支撑拓扑排序、调度、分析依赖、可视化性能瓶颈 |\n",
    "| 图重写（graph rewrite） | fusion/folding/elimination | 减少 kernel 数量、减少 DRAM 往返、清理无用子图，提升吞吐/降低延迟 |\n",
    "| 图划分（graph partitioning） | 多设备并行与负载均衡 | 决定子图放在哪个引擎（CPU/GPU/NPU），减少跨设备复制 |\n",
    "| Liveness 分析 & 区间图着色 | 内存复用与 buffer 规划 | 降低峰值内存，使大模型 fit 进受限的车载/边缘设备 |\n",
    "\n",
    "对实际的工作来说：\n",
    "\n",
    "- **IR 阅读能力**：看得懂内部图、能快速定位性能瓶颈子图\n",
    "- **图重写/融合能力**：可以和编译器团队一起设计 pass，提高算子覆盖和效率\n",
    "- **Partition/Placement 直觉**：理解为什么某些 op 被调度在特定引擎上，以及如何优化\n",
    "- **内存规划意识**：在设计模型/优化 pipeline 时主动考虑内存峰值，而不是事后救火"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
