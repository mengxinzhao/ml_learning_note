{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ecd59b",
   "metadata": {},
   "source": [
    "# ç¬¬ 2 ç« ï¼šæ•°å€¼ä¼˜åŒ–ï¼ˆOptimization for Pruning, Quantization & Distillationï¼‰\n",
    "\n",
    "æœ¬ç« ç›®æ ‡ï¼šç†è§£å‰ªæã€é‡åŒ–ã€è’¸é¦ç­‰æ“ä½œèƒŒåçš„â€œä¼˜åŒ–é—®é¢˜â€è§†è§’ã€‚  \n",
    "ä½ ä¸éœ€è¦æˆä¸ºä¼˜åŒ–ç†è®ºä¸“å®¶ï¼Œä½†éœ€è¦ï¼š\n",
    "\n",
    "- çœ‹æ‡‚å¸¸è§æŸå¤±å‡½æ•°ä¸æ­£åˆ™é¡¹çš„å½¢å¼\n",
    "- ç†è§£ L0/L1 ç¨€ç–åŒ–ã€é‡åŒ–å‚æ•°ä¼˜åŒ–çš„å¤§è‡´æ€è·¯\n",
    "- çŸ¥é“å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆç²¾åº¦ vs å»¶è¿Ÿï¼‰çš„å…¸å‹å†™æ³•\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 å‰ªæï¼ˆPruningï¼‰çš„ä¼˜åŒ–è§†è§’\n",
    "\n",
    "### 2.1.1 åŸºæœ¬ç›®æ ‡\n",
    "\n",
    "ç»™å®šè®­ç»ƒå¥½çš„æƒé‡ $W$ã€æŸå¤±å‡½æ•° $L(W)$ï¼Œå‰ªæå¸Œæœ›ï¼š\n",
    "\n",
    "- å¤§é‡å…ƒç´ å˜ä¸º 0ï¼ˆç¨€ç–ï¼‰\n",
    "- æŸå¤±/ç²¾åº¦å˜åŒ–å°½é‡å°\n",
    "\n",
    "å¯ä»¥å½¢å¼åŒ–ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\hat{W}} L(\\hat{W}) + \\lambda \\|\\hat{W}\\|_0\n",
    "$$\n",
    "\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- è¿™é‡Œ $L$ æ˜¯æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼ˆè®­ç»ƒæ—¶ç”¨çš„ç›®æ ‡å‡½æ•°ï¼‰ï¼Œåœ¨æƒé‡ä¸º $\\hat{W}$ æ—¶çš„æŸå¤±å€¼\n",
    "- $\\|\\hat{W}\\|_0$ï¼šéé›¶å…ƒç´ ä¸ªæ•°ï¼ˆç¨€ç–æ€§åº¦é‡ï¼‰\n",
    "- $\\lambda$ï¼šæƒè¡¡â€œç²¾åº¦ vs ç¨€ç–åº¦â€çš„è¶…å‚æ•°\n",
    "\n",
    "è¿™ä¸ªé—®é¢˜ä¸€èˆ¬æ˜¯ NP-hardï¼Œå¸¸è§è¿‘ä¼¼æ–¹å¼æœ‰ï¼š\n",
    "\n",
    "1. ç”¨ $L_1$ èŒƒæ•°æ›¿ä»£ $L_0$ï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\hat{W}} L(\\hat{W}) + \\lambda \\|\\hat{W}\\|_1\n",
    "$$\n",
    "\n",
    "\n",
    "2. å…ˆæ­£å¸¸è®­ç»ƒï¼Œå†åšåŸºäºæŸç§ saliency çš„åå¤„ç†å‰ªæï¼š\n",
    "   - æŒ‰æƒé‡ç»å¯¹å€¼å¤§å°å‰ªæ‰å°å€¼\n",
    "   - æŒ‰ Hessian è¿‘ä¼¼ï¼ˆå¦‚ OBS/OBDï¼‰è®¡ç®—æ•æ„Ÿåº¦\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2  HessiançŸ©é˜µ & saliency score\n",
    "åè®­ç»ƒå‰ªæï¼ˆpost-training pruningï¼‰æ ¸å¿ƒæ˜¯\n",
    "\n",
    "- è¿™æ¡è¾¹ï¼ˆæŸä¸ª weightï¼‰åˆ æ‰ä¼šä¸ä¼šä¼¤å®³ lossï¼Ÿ\n",
    "\n",
    "- å“ªä¸ªé€šé“æ›´é‡è¦ï¼Ÿ\n",
    "\n",
    "- å“ªå±‚è¯¥å¤šå‰ªï¼Ÿå“ªå±‚ä¸èƒ½ä¹±åŠ¨ï¼Ÿj z\n",
    "\n",
    "ğŸ”· 1. ä»€ä¹ˆæ˜¯ Hessianï¼Ÿ\n",
    "Hessian æ˜¯ loss å¯¹å‚æ•°çš„äºŒé˜¶å¯¼æ•°çŸ©é˜µï¼š\n",
    "$$H = \\nabla^2_{W} L(W)$$\n",
    "æ›´ç²¾ç¡®åœ°ï¼š\n",
    "- å¦‚æœæ¨¡å‹å‚æ•°æ˜¯$$W \\in \\mathbb{R}^n$$ï¼Œ åˆ™ Hessian æ˜¯ï¼š$$H \\in \\mathbb{R}^{n \\times n}$$\n",
    "å…¶ä¸­æ¯ä¸ªå…ƒç´ ï¼š $$H_{ij} = \\frac{\\partial^2 L}{\\partial W_i \\partial W_j}$$\n",
    "ä»£è¡¨ loss å¯¹ä¸¤ä¸ª weight çš„äºŒé˜¶ç›¸äº’å½±å“ã€‚\n",
    "\n",
    "ğŸ”· 2. ä¸ºä»€ä¹ˆ Hessian ä¸å‰ªææœ‰å…³ï¼Ÿ\n",
    "\n",
    "é—®é¢˜ï¼š\n",
    "å¦‚æœæŠŠæŸä¸ª weightä» $w_i$->0, loss ä¼šå˜å¤šå°‘ï¼Ÿ\n",
    "å¦‚æœä½ ç›´æ¥çœ‹$|w_i|$,åªèƒ½çŸ¥é“å®ƒâ€œå¤§å°â€ï¼Œä¸çŸ¥é“ï¼š\n",
    "- è¿™ä¸ªæƒé‡å¯¹ loss çš„æ•æ„Ÿåº¦å¦‚ä½•\n",
    "\n",
    "- æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯ä¸æ˜¯â€œå…³é”®è·¯å¾„â€ä¸Šçš„é‡è¦å‚æ•°ï¼Ÿ\n",
    "\n",
    "ç®€å•ç»å¯¹å€¼æ˜¯**ç²—ç³™ä»£ç†**ã€‚Hessian åˆ™è¡¡é‡äº†ï¼š**æ”¹å˜ä¸€ä¸ªæƒé‡ï¼Œä¼šè®© loss å¢å¤§å¤šå°‘**, æ›´ç²¾ç¡®ã€‚\n",
    "\n",
    "ğŸ”¥ 3. ç”¨äºŒé˜¶æ³°å‹’å±•å¼€ä¼°è®¡â€œå‰ªæ‰æŸä¸ª weight ä¼šå¸¦æ¥å¤šå°‘æŸå¤±å¢åŠ â€ï¼Ÿ\n",
    "å‡è®¾æˆ‘ä»¬è¦æŠŠç¬¬$i$ä¸ª weightä»å½“å‰$w_i$è®¾ç½®æˆ 0ï¼š\n",
    "$$\\Delta w_i = -w_i$$\n",
    "loss çš„å˜åŒ–ï¼ˆç”¨äºŒé˜¶æ³°å‹’å±•å¼€ï¼‰ï¼š\n",
    "$$\\Delta L \\approx \n",
    "\\frac{\\partial L}{\\partial w_i}\\Delta w_i + \\frac{1}{2} H_{ii} (\\Delta w_i)^2$$\n",
    "ä½†ï¼š\n",
    "- åœ¨è®­ç»ƒå®Œçš„ç‚¹, æ¢¯åº¦$\\partial L/\\partial w_i \\approx 0$ï¼ˆå› ä¸ºè®­ç»ƒæ”¶æ•›ï¼‰\n",
    "- æ‰€ä»¥å‰©ä¸‹ï¼š$$\\Delta L \\approx \\frac{1}{2} H_{ii} w_i^2$$\n",
    "\n",
    "è‡³æ­¤ **saliency score** = $H_{ii} w_i^2$\n",
    "è¿™æ¯”å•çœ‹$|w_i|$æ›´æœ‰æ„ä¹‰\n",
    "\n",
    "ğŸ”· ç›´è§‰çš„è§£é‡Š\n",
    "- æ³°å‹’å±•å¼€è¯´ï¼š\n",
    "â€œåœ¨ä¸€ä¸ªç‚¹é™„è¿‘ï¼Œå‡½æ•°çš„å˜åŒ– â‰ˆ ä¸€é˜¶ï¼ˆæ–œç‡ï¼‰ + äºŒé˜¶ï¼ˆå¼¯æ›²ç¨‹åº¦ï¼‰ã€‚â€\n",
    "\n",
    "- åœ¨å·²ç»è®­ç»ƒå¥½çš„ç‚¹ï¼š\n",
    "ä¸€é˜¶å¯¼ï¼ˆæ–œç‡ï¼‰æ¥è¿‘ 0ï¼Œæ‰€ä»¥å˜åŒ–ä¸»è¦é äºŒé˜¶å¯¼å†³å®šã€‚\n",
    "\n",
    "- å¯¹äºå‰ªæï¼š\n",
    "ä½ æŠŠ$w_i$æ”¹æˆ 0ï¼Œç›¸å½“äºåœ¨å‚æ•°ç©ºé—´é‡Œæ²¿ç€ç¬¬ i ä¸ªåæ ‡æ–¹å‘ï¼Œèµ°äº†ä¸€ä¸ªæ­¥é•¿ $-w_i$\n",
    "æŸå¤±å¢åŠ  â‰ˆ å¼¯æ›²ç¨‹åº¦ Ã— ä½ èµ°çš„è·ç¦»çš„å¹³æ–¹ã€‚\n",
    "\tâ€‹\n",
    "- â€œå¼¯æ›²ç¨‹åº¦â€åœ¨è¿™ä¸ªæ–¹å‘å°±æ˜¯ Hessian çš„å¯¹è§’çº¿å…ƒç´  $H_{ii}$\n",
    " - å¦‚æœè¿™ä¸ªæ–¹å‘ä¸Šå¼¯å¾—å¾ˆå‰å®³ï¼ˆH å¤§ï¼‰ï¼Œè¯´æ˜å¯¹è¿™ä¸ªå‚æ•°å¾ˆæ•æ„Ÿï¼Œä¸èƒ½ä¹±å‰ªã€‚\n",
    " - å¦‚æœè¿™ä¸ªæ–¹å‘å‡ ä¹æ˜¯å¹³çš„ï¼ˆH å°ï¼‰ï¼Œå³ä½¿ $w_i$ä¸ä¸ºé›¶ï¼Œä¹Ÿå¯ä»¥å¤§èƒ†å‰ª\n",
    "\n",
    "æ‰€ä»¥ï¼š\n",
    "- åªçœ‹$|w_i|$ â†’ åªæ˜¯çœ‹è¿™ä¸ªå‚æ•°â€œå¤§ä¸å¤§â€ã€‚\n",
    "- çœ‹$H_{ii} w_i^2$ â†’ çœ‹â€œåˆ æ‰å®ƒé€ æˆ loss ä¸Šå‡æœ‰å¤šå¤§â€ï¼Œè¿™æ˜¯æ›´åˆç†çš„â€œé‡è¦æ€§â€åº¦é‡ã€‚\n",
    "\n",
    "ğŸ”· 4. OBSï¼ˆOptimal Brain Surgeonï¼‰ä¸ OBDï¼ˆOptimal Brain Damageï¼‰\n",
    "è¿™æ˜¯ 1990 å¹´ä»£çš„ç»å…¸å‰ªææ–¹æ³•ï¼š\n",
    "\n",
    "ğŸŸ¦ OBDï¼ˆOptimal Brain Damageï¼‰\n",
    "\n",
    "å®ƒä½¿ç”¨å¯¹è§’ Hessianï¼š\n",
    "$$\\text{saliency}_i = \\frac{1}{2} H_{ii} w_i^2$$\n",
    "ç›´è§‰ï¼š\n",
    "* å¦‚æœ $H_{ii}$å¾ˆå¤§ â†’ è¯¥weight éå¸¸æ•æ„Ÿ\n",
    "* å°±ç®—å®ƒå¾ˆå°ï¼Œä¹Ÿä¸èƒ½è½»æ˜“åˆ \n",
    "* å¦‚æœ $H_{ii}$ å¾ˆå° â†’ è¿™ä¸ª weight ä¸é‡è¦\n",
    "* å¯ä»¥å®‰å…¨å‰ªæ‰\n",
    "\n",
    "ç¼ºç‚¹ï¼šå¿½ç•¥ Hessian çš„ off-diagonalï¼ˆè·¨ weight ç›¸å…³æ€§ï¼‰ã€‚\n",
    "\n",
    "ğŸŸ© OBSï¼ˆOptimal Brain Surgeonï¼‰\n",
    "OBS æ›´ç²¾ç¡®ï¼š\n",
    "$$\\text{saliency}_i = \n",
    "\\frac{1}{2} \\frac{w_i^2}{(H^{-1})_{ii}}$$\n",
    "\n",
    "OBS ä½¿ç”¨é€† Hessianï¼Œè€ƒè™‘æ‰€æœ‰ weight ä¹‹é—´çš„ç›¸äº’å½±å“ï¼Œç†è®ºä¸Šæ›´å‡†ç¡®ï¼š\n",
    "\n",
    "- å¦‚æœ weight èƒ½è¢«å…¶ä»–å‚æ•°è¡¥å¿ï¼ˆHessian inverse åæ˜ ç›¸å…³æ€§ï¼‰ï¼Œå®ƒæ›´é€‚åˆåˆ \n",
    "- ä½†è®¡ç®— Hessian inverse å¤ªè´µï¼Œç°ä»£å¾ˆå°‘ç”¨å…¨é‡ OBSã€‚\n",
    "\n",
    "ğŸ”¥ 5. å·¥ç¨‹åšæ³•ï¼ˆPruning-aware methodsï¼‰\n",
    "çœŸå®å·¥ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸å¯èƒ½ç®—çœŸæ­£çš„ Hessianï¼ˆå¤ªå¤§ï¼‰ï¼Œä½†å¯ä»¥ï¼š\n",
    "\n",
    "âœ” è¿‘ä¼¼å¯¹è§’çº¿ï¼š\n",
    "- ç”¨ Fisher ä¿¡æ¯çŸ©é˜µå¯¹è§’çº¿\n",
    "- ç”¨ Hutchinson estimator\n",
    "- ç”¨æ¢¯åº¦å¹³æ–¹çš„ moving average , ç±»ä¼¼ Adam çš„ $v_t$ ï¼Œè¿™å°±æŠŠ Hessian ä¼°è®¡å˜æˆå¯è®­ç»ƒã€å¯éƒ¨ç½²çš„ä¸œè¥¿ã€‚\n",
    "æ­¤å¤„\n",
    "$H_{ii} \\approx \\mathbb{E}[g_i^2]$ï¼Œ \n",
    "\n",
    "$g_i = \\frac{\\partial L}{\\partial w_i}$\n",
    "è¿™ä¸ªæ¢¯åº¦éšç€ batch è€Œå˜åŒ–ï¼Œå› ä¸º loss æ˜¯åŸºäº mini-batch æ±‚å‡ºæ¥çš„ï¼Œæ‰€ä»¥é€šå¸¸ç”¨ï¼š\n",
    "$$\\mathbb{E}[g_i^2] = \\text{over minibatches }$$\n",
    "æ¢å¥è¯è¯´ï¼š\n",
    "\n",
    "- æ¯ä¸ª batch ä¼šç®—ä¸€æ¬¡æ¢¯åº¦ $g = \\nabla L$\n",
    "- å–å®ƒçš„å¹³æ–¹ $g_i^2$\n",
    "- å†å¯¹ batch æ±‚æœŸæœ›ï¼ˆå¹³å‡ï¼‰\n",
    "\n",
    "å°±å¾—åˆ°äº†å¯¹è§’çº¿ Hessian çš„ä¼°è®¡ã€‚\n",
    "\n",
    "\n",
    "ğŸ“Œ ä¸ºä»€ä¹ˆæ¢¯åº¦å¹³æ–¹å¯ä»¥è¿‘ä¼¼ Hessian å¯¹è§’çº¿ï¼Ÿ\n",
    "åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨ **Cross-Entropy loss + Softmax** çš„æ¨¡å‹ï¼Œï¼ˆå…¶ä»–çš„ä¸ä¸€å®šå¯¹ï¼Œæ¯”å¦‚MSEï¼ˆå¹³æ–¹è¯¯å·®å›å½’ï¼‰ï¼‰é‡Œï¼š\n",
    "$$H_{ii} \\approx \\mathbb{E}[g_i^2]$$\n",
    "åŸå› æ˜¯ï¼š\n",
    "1. Hessian = äºŒé˜¶å˜åŒ–ç‡ï¼›Fisher ä¿¡æ¯ = æ¢¯åº¦æ–¹å·®\n",
    "å¯¹æ•°ä¼¼ç„¶æ¡ä»¶ä¸‹ï¼Œæœ‰æ•°å­¦å®šç†ï¼ˆCramerâ€“Rao + Information Equalityï¼‰ï¼š\n",
    "$$\\mathbb{E}[ \\nabla^2 L ] = \\mathbb{E}[ g g^\\top ]$$\n",
    "å–å¯¹è§’çº¿ï¼š\n",
    "$$H_{ii} \\approx \\mathbb{E}[g_i^2]$$\n",
    "æ¢å¥è¯è¯´ï¼š\n",
    "**æ¢¯åº¦çš„æ–¹å·® = äºŒé˜¶æ›²ç‡ï¼ˆHessian å¯¹è§’çº¿**\n",
    "\n",
    "ğŸ“Œ å·¥ç¨‹ä¸Šå¦‚ä½•è®¡ç®—$E[g_i^2]$?\n",
    "```code\n",
    "hessian_diag[i] += grad[i]**2\n",
    "count += 1\n",
    "...\n",
    "hessian_diag /= count\n",
    "```\n",
    "è¿™å°±æ˜¯ Hessian å¯¹è§’çº¿çš„æ— åä¼°è®¡ã€‚\n",
    "\n",
    "ğŸ“Œ ç›´è§‰çš„è§£é‡Š\n",
    "- æ¢¯åº¦æ˜¯â€œæŸå¤±å¯¹å‚æ•°çš„ä¸€é˜¶ååº”â€\n",
    "\n",
    "- æ¢¯åº¦å¹³æ–¹çš„æœŸæœ›å°±æ˜¯â€œæŸå¤±åœ¨è¿™ä¸ªæ–¹å‘ä¸Šçš„æ³¢åŠ¨å¼ºåº¦â€\n",
    "\n",
    "- æ³¢åŠ¨å¼ºè¯´æ˜â€œè¿™ä¸ªæ–¹å‘ä¸Š loss å¾ˆé™¡â€ â†’ Hessian å¤§\n",
    "\n",
    "- æ³¢åŠ¨å¼±è¯´æ˜â€œè¿™ä¸ªæ–¹å‘å¹³ç¼“â€ â†’ Hessian å°\n",
    "æ‰€ä»¥ï¼š\n",
    "$H_{ii} \\approx E[g_i^2]$\n",
    "---\n",
    "\n",
    "#### 2.1.2.1 äºŒé˜¶æ³°å‹’å±•å¼€è¯¦ç»†æ¨å¯¼\n",
    "ç°åœ¨å‚æ•°æ˜¯å‘é‡ï¼š $W = (w_1, w_2, \\dots, w_n)^\\top$\n",
    "loss æ˜¯ $L(W)$\n",
    "åœ¨ç‚¹ ğ‘Š é™„è¿‘åšäºŒé˜¶æ³°å‹’å±•å¼€ï¼š\n",
    "$$L(W + \\Delta W)\n",
    "\\approx\n",
    "L(W)\n",
    "+ \\nabla L(W)^\\top \\Delta W\n",
    "+ \\frac{1}{2} \\Delta W^\\top H \\Delta W$$\n",
    "\n",
    "è¿™é‡Œï¼š\n",
    "- $\\nabla L(W)$æ˜¯æ¢¯åº¦å‘é‡ï¼ˆé•¿åº¦ nï¼‰\n",
    "- $H = \\nabla^2 L(W)$ æ˜¯ Hessian çŸ©é˜µï¼ˆnÃ—n\n",
    "\n",
    "ç°åœ¨ **æˆ‘ä»¬åªåŠ¨ç¬¬ i ä¸ªå‚æ•°ï¼ŒæŠŠå®ƒå˜æˆ 0**\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "- åŸæ¥ï¼š$w_i$\n",
    "- å‰ªæåï¼š$w_i^{\\text{new}} = 0$\n",
    "æ‰€ä»¥ï¼š\n",
    "$$\\Delta W = W_{\\text{new}} - W_{\\text{old}}\n",
    "= (0,\\dots,0, -w_i, 0,\\dots,0)^\\top\n",
    "= -w_i e_i$$\n",
    "\n",
    "ä»£å…¥æ³°å‹’å±•å¼€å…¬å¼\n",
    "$$\\Delta L\n",
    "= L(W + \\Delta W) - L(W)\n",
    "\\approx \\nabla L(W)^\\top \\Delta W + \\frac{1}{2} \\Delta W^\\top H \\Delta W$$\n",
    "\n",
    "æ¢¯åº¦å‘é‡ï¼š\n",
    "$\\begin{array}{c} \\nabla L(W) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial w_1} \\\\\n",
    "\\frac{\\partial L}{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial w_n}\n",
    "\\end{bmatrix} \\end{array}$\n",
    "\n",
    "\n",
    "æŠŠ $\\Delta W = -w_i e_i$ä»£è¿›å»ï¼š\n",
    "- ä¸€é˜¶é¡¹ï¼š\n",
    "$$\\nabla L(W)^\\top \\Delta W\n",
    "= \\nabla L(W)^\\top (-w_i e_i)\n",
    "= -w_i \\frac{\\partial L}{\\partial w_i}$$\n",
    "\n",
    "- äºŒé˜¶é¡¹\n",
    "$$\\Delta W^\\top H \\Delta W\n",
    "= (-w_i e_i)^\\top H (-w_i e_i)\n",
    "= w_i^2 \\, e_i^\\top H e_i$$\n",
    "ä½†ï¼š\n",
    "$$e_i^\\top H e_i = H_{ii}$$\n",
    "æ‰€ä»¥ï¼š\n",
    "$$\\Delta W^\\top H \\Delta W = w_i^2 H_{ii}$$\n",
    "äºæ˜¯ï¼š\n",
    "$$\\Delta L \\approx -w_i \\frac{\\partial L}{\\partial w_i} + \\frac{1}{2} w_i^2 H_{ii}$$\n",
    "\n",
    "**å†æ¬¡ä½¿ç”¨â€œåœ¨æ”¶æ•›ç‚¹æ¢¯åº¦ â‰ˆ 0â€**, åœ¨è®­ç»ƒå¥½çš„å‚æ•°$W^\\star$é™„è¿‘ï¼š\n",
    "$$\\frac{\\partial L}{\\partial w_i}(W^\\star) \\approx 0$$\n",
    "äºæ˜¯ï¼š\n",
    "$$\\Delta L \\approx \\frac{1}{2} H_{ii} w_i^2$$\n",
    "è¿™å°±æ˜¯**OBDï¼ˆOptimal Brain Damageï¼‰ä¸­çš„ saliency**: $\\text{saliency}_i = \\frac{1}{2} H_{ii} w_i^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b402c",
   "metadata": {},
   "source": [
    "#### 2.1.2.2 pytorch CrossEntropy ä¸‹çš„ Hessian å¯¹è§’çº¿ vs E[gÂ²]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996091e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained loss: 0.33923059701919556\n",
      "Trained theta: [ 1.3182789 -1.9975102  0.4114626]\n",
      "\n",
      "True Hessian diagonal H_ii:\n",
      "tensor([0.0965, 0.0759, 0.1324])\n",
      "\n",
      "Estimated Hessian diagonal via E[g^2]:\n",
      "tensor([0.0015, 0.0012, 0.0014])\n",
      "\n",
      "Absolute diff |H_ii - E[g_i^2]|:\n",
      "tensor([0.0949, 0.0747, 0.1310])\n",
      "\n",
      "Relative diff |H_ii - E[g_i^2]| / |H_ii|:\n",
      "tensor([0.9839, 0.9840, 0.9891])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import hessian\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ===========================\n",
    "# 1. æ„é€ ä¸€ä¸ªå°çš„ logistic å›å½’æ¨¡å‹\n",
    "#    y âˆˆ {0,1}, p = sigmoid(xW + b)\n",
    "# ===========================\n",
    "\n",
    "in_dim = 2\n",
    "\n",
    "N = 500\n",
    "X = torch.randn(N, in_dim)\n",
    "\n",
    "# çœŸå®å‚æ•°ï¼ˆåªç”¨äºç”Ÿæˆæ•°æ®ï¼‰\n",
    "true_theta = torch.tensor([2.0, -3.0, 0.5])  # [w1, w2, b]\n",
    "true_W = true_theta[:in_dim].view(in_dim, 1) # [2,1]\n",
    "true_b = true_theta[in_dim:].view(1)         # [1]\n",
    "\n",
    "logits = X @ true_W + true_b            # [N,1]\n",
    "probs = torch.sigmoid(logits)           # [N,1]\n",
    "y = torch.bernoulli(probs).view(-1).long()  # è½¬æˆ int labels [N]\n",
    "\n",
    "# æˆ‘ä»¬åŒæ ·ç”¨ä¸€ç»´å‚æ•°å‘é‡ theta = [w1, w2, b] æ¥ä¼˜åŒ–\n",
    "theta = torch.randn(in_dim + 1, requires_grad=True)\n",
    "\n",
    "def unpack_theta(th):\n",
    "    W = th[:in_dim].view(in_dim, 1)   # [2,1]\n",
    "    b = th[in_dim:].view(1)           # [1]\n",
    "    return W, b\n",
    "\n",
    "def loss_fn(theta, X, y):\n",
    "    W, b = unpack_theta(theta)        # W: [2,1], b: [1]\n",
    "    logits = X @ W + b                # [N,1]\n",
    "    logits = logits.view(-1, 1)\n",
    "    # Binary cross entropy with logits\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, y.float().view(-1, 1))\n",
    "    return loss\n",
    "\n",
    "# ç®€å•è®­ç»ƒä¸€ä¸‹ï¼Œè®© theta æ¥è¿‘â€œæ”¶æ•›ç‚¹â€\n",
    "optim = torch.optim.SGD([theta], lr=0.1)\n",
    "\n",
    "for step in range(300):\n",
    "    optim.zero_grad()\n",
    "    loss = loss_fn(theta, X, y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "print(\"Trained loss:\", loss_fn(theta, X, y).item())\n",
    "print(\"Trained theta:\", theta.detach().numpy())\n",
    "\n",
    "# ==========================================\n",
    "# 2. è®¡ç®—â€œçœŸÂ·Hessian å¯¹è§’çº¿â€ï¼šH_ii\n",
    "# ==========================================\n",
    "\n",
    "def loss_only_theta(th):\n",
    "    return loss_fn(th, X, y)\n",
    "\n",
    "H = hessian(loss_only_theta, theta)   # [P,P], P = in_dim+1=3\n",
    "H_diag_true = torch.diag(H).detach()\n",
    "\n",
    "print(\"\\nTrue Hessian diagonal H_ii:\")\n",
    "print(H_diag_true)\n",
    "\n",
    "# ==========================================\n",
    "# 3. ç”¨å¤š batch çš„æ¢¯åº¦å¹³æ–¹å¹³å‡ E[g^2] è¿‘ä¼¼ H_ii\n",
    "# ==========================================\n",
    "\n",
    "num_batches = 100\n",
    "batch_size = 64\n",
    "\n",
    "g2_accum = torch.zeros_like(theta)\n",
    "\n",
    "for _ in range(num_batches):\n",
    "    idx = torch.randint(0, N, (batch_size,))\n",
    "    Xb = X[idx]\n",
    "    yb = y[idx]\n",
    "\n",
    "    loss_b = loss_fn(theta, Xb, yb)\n",
    "    grad_theta = torch.autograd.grad(loss_b, theta, retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "    g2_accum += grad_theta.detach() ** 2\n",
    "\n",
    "H_diag_est = g2_accum / num_batches\n",
    "\n",
    "print(\"\\nEstimated Hessian diagonal via E[g^2]:\")\n",
    "print(H_diag_est)\n",
    "\n",
    "# ==========================================\n",
    "# 4. å¯¹æ¯”ä¸¤è€…\n",
    "# ==========================================\n",
    "\n",
    "abs_diff = (H_diag_true - H_diag_est).abs()\n",
    "rel_diff = abs_diff / (H_diag_true.abs() + 1e-8)\n",
    "\n",
    "print(\"\\nAbsolute diff |H_ii - E[g_i^2]|:\")\n",
    "print(abs_diff)\n",
    "\n",
    "print(\"\\nRelative diff |H_ii - E[g_i^2]| / |H_ii|:\")\n",
    "print(rel_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492a2d7",
   "metadata": {},
   "source": [
    "### 2.1.3 è¡¥ï¼šFisher ä¿¡æ¯çŸ©é˜µ\n",
    "Fisher ä¿¡æ¯çŸ©é˜µï¼ˆFisher Information Matrix, FIMï¼‰ è¡¡é‡çš„æ˜¯ï¼š\n",
    "\n",
    "æ¨¡å‹å¯¹å‚æ•°å˜åŒ–çš„æ•æ„Ÿåº¦\n",
    "\n",
    "æˆ–\n",
    "\n",
    "ç”¨å‚æ•°å»è§£é‡Šæ•°æ®æ—¶ï¼Œä¸ç¡®å®šæ€§æœ‰å¤šå°\n",
    "\n",
    "â€œä¿¡æ¯è¶Šå¤§ â†’ å‚æ•°è¶Šé‡è¦ï¼Œä¸èƒ½åŠ¨â€\n",
    "â€œä¿¡æ¯è¶Šå° â†’ å‚æ•°ä¸é‡è¦ï¼Œå¯ä»¥å‰ª / é‡åŒ–å¾—æ›´æ¿€è¿›â€\n",
    "\n",
    "---\n",
    "1. Fisher ä¿¡æ¯çŸ©é˜µçš„æ­£å¼å®šä¹‰\n",
    "ç»™å®šæ¨¡å‹çš„å¯¹æ•°ä¼¼ç„¶ï¼ˆlog-likelihoodï¼‰ï¼š$\\log p_\\theta(x)$\n",
    "\n",
    "Fisher ä¿¡æ¯çŸ©é˜µå®šä¹‰ä¸ºï¼š\n",
    "$$F(\\theta) = \\mathbb{E}\\left[ \\left( \\nabla_\\theta \\log p_\\theta(x) \\right)\n",
    "       \\left( \\nabla_\\theta \\log p_\\theta(x) \\right)^\\top \\right]$$\n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "\n",
    "- å…ˆå¯¹ log-likelihood æ±‚æ¢¯åº¦ï¼ˆscore functionï¼‰\n",
    "\n",
    "- å†æ±‚å®ƒä¸è‡ªèº«çš„å¤–ç§¯\n",
    "\n",
    "- å†åœ¨æ•°æ®åˆ†å¸ƒä¸Šå–æœŸæœ›\n",
    "\n",
    "---\n",
    "2. æ·±åº¦å­¦ä¹ é‡Œæ›´å¸¸ç”¨çš„è¡¨è¾¾\n",
    "å½“æˆ‘ä»¬ç”¨ CrossEntropy Loss æ—¶ï¼š\n",
    "$$L(\\theta) = -\\log p_\\theta(y|x)$$\n",
    "æ¢¯åº¦å°±æ˜¯ï¼š\n",
    "$$g = \\nabla_\\theta L = - \\nabla_\\theta \\log p_\\theta(y|x)$$\n",
    "äºæ˜¯ Fisher ä¿¡æ¯çŸ©é˜µç­‰ä»·äºï¼š\n",
    "$$F(\\theta) = \\mathbb{E}[g\\, g^\\top]$$\n",
    "\n",
    "å¯¹è§’çº¿å°±æ˜¯ï¼š\n",
    "$$F_{ii} = \\mathbb{E}[g_i^2]$$\n",
    "ä¹Ÿå°±æ˜¯\n",
    "$$H_{ii} \\approx \\mathbb{E}[g_i^2]$$\n",
    "\n",
    "---\n",
    "3. ä¸ºä»€ä¹ˆ Fisher ä¿¡æ¯çŸ©é˜µåœ¨æ·±åº¦å­¦ä¹ ä¸­å¾ˆé‡è¦ï¼Ÿ\n",
    "Fisher ä¿¡æ¯å‘Šè¯‰æˆ‘ä»¬ï¼š\n",
    "\n",
    "å¦‚æœå‚æ•° Î¸ å‘ç”Ÿå¾®å°å˜åŒ–ï¼Œæ¨¡å‹é¢„æµ‹ä¼šæœ‰å¤šå¤§æ³¢åŠ¨ï¼Ÿ\n",
    "\n",
    "æ¢å¥è¯è¯´ï¼š\n",
    "- $F_{ii}$ å¤§ â†’ å‚æ•°éå¸¸â€œæ•æ„Ÿâ€ â†’ å¾ˆé‡è¦ â†’ ä¸å®œå‰ª\n",
    "- $F_{ii}$å° â†’ å‚æ•°åŸºæœ¬ä¸èµ·ä½œç”¨ â†’ å¯ä»¥å‰ª / å¯ä»¥ä½ bit é‡åŒ– \n",
    "\n",
    "æ‰€ä»¥ï¼š\n",
    "\n",
    "âœ” Fisher ä¿¡æ¯å¯ç”¨äºå‰ªæï¼ˆpruning saliencyï¼‰\n",
    "\n",
    "âœ” å¯ç”¨äºé‡åŒ–ï¼ˆquantization sensitivityï¼‰\n",
    "\n",
    "âœ” å¯ç”¨äºç¡®å®šå“ªäº›å±‚çš„ rank è¦ä¸‹è°ƒï¼ˆlow-rank SVDï¼‰\n",
    "\n",
    "âœ” å¯ç”¨äº LayerNorm / Whitening çš„ç¨³å®šæ€§åˆ†æ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349799b",
   "metadata": {},
   "source": [
    "### 2.1.4 ç»“æ„åŒ–å‰ªæï¼ˆchannel/head pruningï¼‰\n",
    "ç»“æ„åŒ–å‰ªæå…³æ³¨â€œæ•´ä¸ªé€šé“/æ•´ä¸ª head/æ•´ä¸ª filter æ˜¯å¦é‡è¦â€ï¼Œè€Œä¸æ˜¯å•ä¸ª weightã€‚\n",
    "å¯ä»¥æŠŠé€šé“å‰ªæå†™æˆï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\theta, m} \\ L(\\theta \\odot m) + \\lambda \\|m\\|_0\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\theta$ï¼šåŸå§‹å‚æ•°\n",
    "- $m$ï¼šmaskï¼ŒæŒ‰é€šé“/å—ä¸ºå•ä½å– 0 æˆ– 1\n",
    "- $\\odot$ï¼šé€å…ƒç´ æˆ–é€é€šé“ä¹˜æ³•\n",
    "\n",
    "é€šå¸¸ä¼šï¼š\n",
    "\n",
    "1. è®­ç»ƒä¸€ä¸ªå¸¦æœ‰å¯å¾®è¿‘ä¼¼ mask çš„æ¨¡å‹ï¼ˆå¦‚ç”¨ sigmoid/Concrete distributionï¼‰\n",
    "2. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å‹ç¼©æŸäº›é€šé“çš„æƒé‡\n",
    "3. æœ€åå°†æ¥è¿‘ 0 çš„é€šé“ç¡¬å‰ªæ‰\n",
    "\n",
    "> æœ¬è´¨ä»ç„¶æ˜¯â€œå¸¦ç¨€ç–çº¦æŸçš„ä¼˜åŒ–é—®é¢˜â€ï¼Œåªæ˜¯ä½œç”¨å¯¹è±¡ä»å•ä¸ªå…ƒç´ æå‡åˆ°äº†â€œç»“æ„åŒ–å—â€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 2.1.5 å„ç§å‰ªææ–¹æ³•å¯¹æ¯”\n",
    "| æ–¹æ³•                           | ä½¿ç”¨çš„æŒ‡æ ‡                         | æŒ‡æ ‡æ„ä¹‰                     | æ˜¯å¦è¡¡é‡é‡è¦æ€§ï¼Ÿ        | æ˜¯å¦ç§‘å­¦ï¼Ÿ |\n",
    "| ------------------------------ | ----------------------------------- | ----------------------------- | ------------------------- | ---------- |\n",
    "| **L1 å‰ªæ**                    | $\\lvert w_i \\rvert$                | æ˜¯å¦æ¥è¿‘ 0ï¼ˆè‡ªç„¶ç¨€ç–ï¼‰         | âŒ å¦                     | ä½         |\n",
    "| **Hessian å‰ªæï¼ˆOBD/OBSï¼‰**    | $H_{ii} \\, w_i^2$                  | åˆ æ‰è¯¥æƒé‡é€ æˆçš„ loss å¢åŠ é‡  | âœ” æ˜¯ï¼ˆæ•°å­¦ä¸¥æ ¼ï¼‰         | é«˜         |\n",
    "| **Fisher å‰ªæ**                | \\mathbb{E}[g_i^2] \\, w_i^2$        | ç”¨ Fisher è¿‘ä¼¼ Hessian       | âœ” æ˜¯                     | é«˜         |\n",
    "| **Magnitude å‰ªæï¼ˆstructuredï¼‰** | é€šé“ $\\ell_1$ / $\\ell_2$ èŒƒæ•°       | é€šé“èƒ½é‡å¤§å°                  | éƒ¨åˆ†                      | ä¸­         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b611d7",
   "metadata": {},
   "source": [
    "## 2.2 é‡åŒ–ï¼ˆQuantizationï¼‰çš„ä¼˜åŒ–è§†è§’\n",
    "\n",
    "é‡åŒ–æœ¬è´¨ä¸æ˜¯ç®€å•åœ°â€œæŠŠæµ®ç‚¹å˜æˆ int8/int4â€ï¼Œ\n",
    "è€Œæ˜¯ï¼š\n",
    "\n",
    "**åœ¨ bit-budgetï¼ˆä½å®½é¢„ç®—ï¼‰å—é™çš„çº¦æŸä¸‹ï¼Œæ±‚ä¸€ä¸ªé€¼è¿‘åŸæ¨¡å‹çš„æœ€ä¼˜ç¦»æ•£è¡¨ç¤ºï¼Œä½¿å¾— inference çš„è¯¯å·®æœ€å°ã€‚**\n",
    "\n",
    "æ•°å­¦å½¢å¼ä¸Šå°±æ˜¯ï¼š\n",
    "$$\\min_{s,z, Q(\\cdot)} \\; L\\big(f_{Q,s,z}(x), f(x)\\big)$$\n",
    "å…¶ä¸­ï¼š\n",
    "- $Q(\\cdot)$é‡åŒ–æ˜ å°„ï¼ˆround, clipï¼‰\n",
    "- $s$: scale ï¼ˆæ­¥é•¿ï¼‰\n",
    "- $z$: zero pointï¼ˆéå¯¹ç§°é‡åŒ–çš„åç½®ï¼‰\n",
    "- $f_{Q,s,z}$: é‡åŒ–åçš„æ¨¡å‹\n",
    "- $f$: æ¨¡å‹\n",
    "\n",
    "---\n",
    "\n",
    "#1 é‡åŒ–çš„ä¸‰ä¸ªæ ¸å¿ƒä¼˜åŒ–ç›®æ ‡\n",
    "ğŸ“Œ ç›®æ ‡ 1ï¼šæœ€å°åŒ–é‡åŒ–è¯¯å·®\n",
    "æœ€å¸¸è§ï¼š\n",
    "$$\\min_{s,z} \\sum_i (w_i - \\hat{w}_i)^2$$\n",
    "å…¶ä¸­ï¼š\n",
    "$$\\hat{w}_i = s (Q(w_i/s) - z)$$\n",
    "è¿™æ˜¯ æœ€å°äºŒä¹˜æ„ä¹‰ä¸‹ çš„æœ€ä¼˜ scale / zero pointã€‚\n",
    "\n",
    "\n",
    "ğŸ“Œ ç›®æ ‡ 2ï¼šæ»¡è¶³ bit-budgetï¼ˆçº¦æŸä¼˜åŒ–ï¼‰\n",
    "$q_i \\in \\{-Q, ..., Q\\}, \\quad Q = 2^{b-1} - 1$\n",
    "bit è¶Šå°‘ â†’ é‡åŒ–å™ªå£°è¶Šå¤§ â†’ æ¨ç†è¯¯å·®è¶Šé«˜\n",
    "æœ¬è´¨æ˜¯ï¼š**åœ¨ bit=4/8 çš„çº¦æŸä¸‹ï¼ŒæŠŠè¯¯å·®å‹åˆ°æœ€ä½ã€‚**\n",
    "\n",
    "ğŸ“Œ ç›®æ ‡ 3ï¼šç»´æŒæ¨ç†è§£ç ç¨³å®šæ€§ï¼ˆæ¢¯åº¦æ— å…³ï¼‰\n",
    "- runtime inference ä¸­æœ€é‡è¦ï¼š\n",
    "\n",
    "- ä¸çˆ†å€¼\n",
    "\n",
    "- ä¸æº¢å‡º\n",
    "\n",
    "- ä¸ç§¯ç´¯æ•°å€¼æ¼‚ç§»\n",
    "\n",
    "- softmax ä¸å´©\n",
    "\n",
    "- LayerNorm ä¿æŒç¨³å®š\n",
    "æ•°å­¦ä¸Šï¼Œè¿™è¦æ±‚ï¼š\n",
    "\n",
    "$$\\|f_{Q}(x) - f(x)\\| \\text{ å°ä¸”å¯æ§}$$\n",
    "ç‰¹åˆ«æ˜¯åœ¨ Transformer ä¸­ï¼Œé‡åŒ–è¯¯å·®ä¼šåœ¨æ³¨æ„åŠ›è·¯å¾„ç´¯è®¡ï¼Œéœ€è¦é’ˆå¯¹ä¸åŒå±‚è®¾è®¡ä¸åŒä¼˜åŒ–ç­–ç•¥\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f318349",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ada21244",
   "metadata": {},
   "source": [
    "### 2.2.1 ç®€å•çš„ uniform quantization - ä¼˜åŒ–scale\n",
    "\n",
    "è€ƒè™‘å¯¹æƒé‡é›†åˆ $\\{w_i\\}$ åšå¯¹ç§°å‡åŒ€é‡åŒ–ï¼Œbit å®½ä¸º bï¼š\n",
    "\n",
    "- é‡åŒ–çº§åˆ«ï¼š$q_i \\in \\{-Q,\\dots,Q\\}$ï¼Œå…¶ä¸­ $Q = 2^{b-1}-1$\n",
    "- ç¼©æ”¾å› å­ï¼ˆscaleï¼‰ï¼š$s > 0$\n",
    "- é‡åŒ–/åé‡åŒ–è¿‡ç¨‹ï¼š\n",
    "\n",
    "$$\n",
    "q_i = \\text{round}(w_i / s), \\quad\n",
    "  \\hat{w}_i = s \\cdot q_i\n",
    "$$\n",
    "\n",
    "\n",
    "å…¸å‹ç›®æ ‡æ˜¯æœ€å°åŒ–é‡åŒ–è¯¯å·®ï¼š\n",
    "\n",
    "$$\n",
    "\\min_s \\sum_i (w_i - \\hat{w}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "è¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªä¸€ç»´å‡¸ä¼˜åŒ–é—®é¢˜ã€‚å¯ä»¥è¯æ˜ï¼š\n",
    "\n",
    "- å¯¹ç§°é‡åŒ–ï¼ˆzero-point = 0ï¼‰\n",
    "- é‡åŒ–åŒºé—´å›ºå®šåœ¨$[-Q, Q]$\n",
    "åˆ™æœ€ä¼˜ scale è§£ä¸ºï¼š$$s^* = \\frac{\\max |w_i|}{Q}$$\n",
    "\n",
    "å·¥ç¨‹ä¸Šå¸¸è§ heuristicsï¼š\n",
    "\n",
    "- ç›´æ¥å–ï¼š\n",
    "\n",
    "$$\n",
    "s = \\frac{\\max_i |w_i|}{Q}\n",
    "$$\n",
    "\n",
    "\n",
    "- æˆ–å–æŸä¸ªåˆ†ä½æ•°ï¼ˆå¦‚ 99.9%ï¼‰æ›¿ä»£ maxï¼Œé˜²æ­¢ outlier è¿‡å¤§ï¼š\n",
    "\n",
    "$$\n",
    "s = \\frac{\\text{quantile}_{p}(|w_i|)}{Q}\n",
    "$$\n",
    "\n",
    "\n",
    "**ã€è¦ç‚¹ã€‘**  \n",
    "ä½ éœ€è¦çŸ¥é“ï¼š\n",
    "\n",
    "- é‡åŒ–å‚æ•°ï¼ˆscale/zero-pointï¼‰å¯ä»¥é€šè¿‡æœ€å°äºŒä¹˜æ„ä¹‰ä¸‹çš„ä¼˜åŒ–æ±‚å¾—\n",
    "- å·¥ç¨‹å®ç°ä¸­ç”¨ histogram + æœç´¢ / heuristics åšè¿‘ä¼¼æ±‚è§£ï¼š åŸå› çœŸå®æƒé‡/æ¿€æ´» ä¸æ˜¯å‡åŒ€åˆ†å¸ƒï¼Œå¾ˆå¤š heavy-tailï¼ˆé•¿å°¾ï¼‰çš„ã€‚æ‰€ä»¥æ›´å¥½çš„ä¼˜åŒ–æ–¹æ³•æ˜¯**ä¼˜åŒ–åˆ†å¸ƒé€¼è¿‘ï¼šKL æœ€å°åŒ–**\n",
    "$$s^* = \\arg\\min_s \\text{KL}(P(w) \\| P(\\hat{w}))$$\n",
    "æ‰€ä»¥å¾ˆå¤šå·¥å…·ï¼ˆTensorRTã€TF-Liteã€MKLDNN etcï¼‰éƒ½ç”¨ï¼š\n",
    "\n",
    " - histogram åˆ†å¸ƒ\n",
    "\n",
    " - quantization reconstruction\n",
    "\n",
    " - KL divergence\n",
    "\n",
    "æ¥é€‰æœ€ä¼˜ clip rangeã€‚\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2.2 LSQï¼ˆLearned Step Size Quantizationï¼‰æ€æƒ³\n",
    "\n",
    "æ›´è¿›ä¸€æ­¥ï¼Œå¯ä»¥æŠŠ scale $s$ å½“æˆå¯å­¦ä¹ å‚æ•°ï¼Œè®©åå‘ä¼ æ’­ç›´æ¥ä¼˜åŒ–ï¼š\n",
    "\n",
    "- å®šä¹‰ä¸€ä¸ªâ€œä¼ªé‡åŒ–â€ç®—å­ï¼ˆç”¨ STE Straight-Through Estimator é€¼è¿‘æ¢¯åº¦ï¼‰\n",
    "- åœ¨è®­ç»ƒ/å¾®è°ƒæ—¶ joint optimize $W$ å’Œ $s$\n",
    "æŠŠ quantizer å†™æˆï¼ˆå¸¦ straight-through æ¢¯åº¦ï¼‰ï¼š\n",
    "$$\\hat{w} = \\mathrm{clip}\\Big(\\mathrm{round}(\\frac{w}{s})\\Big) \\cdot s$$\n",
    "ç„¶åæŠŠ s å½“æˆæ¨¡å‹å‚æ•°ä¸€èµ·è®­ç»ƒï¼š\n",
    "$$\\min_{s,W} L(\\hat{W}(s), y)$$\n",
    "è¿™å°±ä»æ•°å­¦ä¸Šå˜æˆï¼š\n",
    "\n",
    "- è”åˆä¼˜åŒ–ï¼ˆjoint optimizationï¼‰\n",
    "\n",
    "- å¸¦ä¸å¯å¯¼ç®—å­çš„è¿‘ä¼¼æ¢¯åº¦ï¼ˆSTEï¼‰\n",
    "\n",
    "- å‚æ•°åŒ–çš„çº¦æŸä¼˜åŒ–\n",
    "\n",
    "è¿™ç§æ–¹æ³•ä½¿å¾—é‡åŒ–å‚æ•°é’ˆå¯¹å½“å‰ä»»åŠ¡/æ•°æ®é›†è‡ªé€‚åº”åœ°æ”¶æ•›åˆ°è¾ƒå¥½çš„å€¼ï¼Œä»è€Œæå‡ä½ bit é‡åŒ–çš„ç²¾åº¦ã€‚\n",
    "\n",
    "#### 2.2.2.1 STE Straight-Through Estimator\n",
    "å½“é‡åŒ–ã€å–æ•´ã€ç¬¦å·å‡½æ•°ç­‰ ä¸å¯å¯¼ï¼ˆæ¢¯åº¦ä¸º 0 æˆ–ä¸å­˜åœ¨ï¼‰æ—¶ï¼Œ\n",
    "æ— æ³•ç›´æ¥åšåå‘ä¼ æ’­ã€‚\n",
    "\n",
    "STE çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**å‰å‘ä½¿ç”¨çœŸå®çš„éå¯å¯¼ç®—å­ï¼ˆå¦‚ roundã€signï¼‰ï¼Œåå‘æ—¶â€œå‡è£…å®ƒæ˜¯æ’ç­‰æ˜ å°„â€ï¼ŒæŠŠæ¢¯åº¦ç›´æ¥ä¼ è¿‡å»ã€‚**\n",
    "æ•°å­¦å†™æ³•ï¼š\n",
    "å‰å‘ï¼š$$\\hat{w} = \\text{round}(w)$$\n",
    "åå‘ï¼š$$\\frac{\\partial \\hat{w}}{\\partial w} \\approx 1$$\n",
    "æˆ–è€…æ ¹æ®ä¸åŒå˜ä½“ï¼š\n",
    "$$\\begin{array}{c} \\frac{\\partial \\hat{w}}{\\partial w} = \n",
    "\\begin{cases}\n",
    "1, & |w|<1 \\\\\n",
    "0, & \\text{else}\n",
    "\\end{cases} \\end{array}$$\n",
    "\n",
    "---\n",
    "ğŸ”· ä¸ºä»€ä¹ˆå« \"Straight-Through\"ï¼Ÿ\n",
    "å› ä¸ºåœ¨åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦ä¸è¢«é˜»æ–­ï¼Œè€Œæ˜¯â€œç›´æ¥ç©¿è¿‡ï¼ˆgo straight throughï¼‰â€ä¸å¯å¯¼ç®—å­ã€‚\n",
    "```block\n",
    "   w ---->[ round ]----> \\hat{w}\n",
    "           (non-diff)\n",
    "\n",
    "   backward: gradient just passes straight through\n",
    "\n",
    "```\n",
    "- å‰å‘ç”¨ç¦»æ•£å€¼, $\\hat{w} = s \\cdot \\text{round}(w/s)$\n",
    "- åå‘æŠŠ round å½“æˆ identity. $\\frac{\\partial \\hat{w}}{\\partial w} \\approx 1$\n",
    "å› æ­¤é‡åŒ–æ¨¡å‹å¯ä»¥ç»§ç»­è®­ç»ƒï¼ˆQATï¼‰ã€å­¦ä¹ æœ€ä¼˜ scaleï¼ˆLSQï¼‰ã€‚\n",
    "\n",
    "ğŸ”· STE ç‰¹åˆ«é€‚ç”¨äºï¼š\n",
    "\n",
    "- QATï¼ˆQuantization-Aware Trainingï¼‰\n",
    "\n",
    "- LSQï¼ˆLearned Step Size Quantizationï¼‰\n",
    "\n",
    "- äºŒå€¼åŒ–ç½‘ç»œï¼ˆBNNï¼‰\n",
    "\n",
    "- hard-sigmoid/ç¡¬é—¨æ§çš„æ¢¯åº¦è¿‘ä¼¼\n",
    "\n",
    "- ä»»ä½•æ¶‰åŠ â€œä¸å¯å¯¼å‡½æ•° + æƒ³è®­ç»ƒå®ƒâ€ çš„åœºæ™¯\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fb8de",
   "metadata": {},
   "source": [
    "## 2.3 çŸ¥è¯†è’¸é¦ï¼ˆDistillationï¼‰çš„ä¼˜åŒ–è§†è§’\n",
    "\n",
    "### 2.3.1 æ ¸å¿ƒæ€æƒ³ï¼šè’¸é¦æ˜¯ä¸€ä¸ªè”åˆæœ€ä¼˜åŒ–é—®é¢˜\n",
    "\n",
    "Teacherï¼ˆå¤§æ¨¡å‹ï¼‰å‘ Studentï¼ˆå°æ¨¡å‹ï¼‰ä¼ é€’è½¯çŸ¥è¯†ï¼ˆsoft knowledgeï¼‰ã€‚\n",
    "\n",
    "- Teacher logitsï¼š  \n",
    "  $$ z^T $$\n",
    "\n",
    "- Student logitsï¼š  \n",
    "  $$ z^S $$\n",
    "\n",
    "åŠ å…¥æ¸©åº¦ $ T $ çš„ softmaxï¼š\n",
    "\n",
    "$$\n",
    "p^T_i = \\frac{e^{z_i^T / T}}{\\sum_j e^{z_j^T / T}}, \n",
    "\\qquad\n",
    "p^S_i = \\frac{e^{z_i^S / T}}{\\sum_j e^{z_j^S / T}}\n",
    "$$\n",
    "\n",
    "Student çš„ç›®æ ‡æ˜¯æœ€å°åŒ–ä»¥ä¸‹è”åˆæŸå¤±ï¼š\n",
    "\n",
    "$$\n",
    "L = \\alpha L_{\\text{CE}} + (1 - \\alpha)L_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.2 Hard Lossï¼ˆäº¤å‰ç†µ CEï¼‰\n",
    "\n",
    "å­¦ç”Ÿå¯¹çœŸå®æ ‡ç­¾çš„äº¤å‰ç†µæŸå¤±ï¼š\n",
    "\n",
    "$$\n",
    "L_{\\text{CE}} = -\\sum_i y_i \\log p^S_i\n",
    "$$\n",
    "\n",
    "- $y$ï¼šone-hot æˆ– label smoothing å¤„ç†åçš„æ ‡ç­¾  \n",
    "- è¿™ä¸€é¡¹ä¿è¯ Student è‡³å°‘èƒ½å¯¹â€œæ ‡å‡†ç­”æ¡ˆâ€åšå¯¹åˆ†ç±»\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.3 Soft Lossï¼ˆKL è’¸é¦æŸå¤±ï¼‰\n",
    "\n",
    "å­¦ç”Ÿæ¨¡ä»¿ Teacher çš„è¾“å‡ºåˆ†å¸ƒï¼š\n",
    "\n",
    "$$\n",
    "L_{\\text{KL}} = T^2 \\sum_i p^T_i \\log \\frac{p^T_i}{p^S_i}\n",
    "$$\n",
    "\n",
    "- ä½¿ç”¨ $ T^2 $ æ¥æŠµæ¶ˆæ¸©åº¦å¯¹æ¢¯åº¦çš„ç¼©æ”¾  \n",
    "- Teacher çš„ soft labels æä¾›â€œæš—çŸ¥è¯†â€ï¼ˆdark knowledgeï¼‰  \n",
    "- ä¸ä»…å‘Šè¯‰ Student â€œè°æ˜¯å¯¹çš„â€ï¼Œè¿˜å‘Šè¯‰å®ƒ â€œè°è·Ÿè°æ›´åƒâ€\n",
    "\n",
    "è”åˆæŸå¤±ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "L = \\alpha L_{\\text{CE}} + (1-\\alpha)L_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $ \\alpha $ï¼šæ§åˆ¶å¯¹çœŸå®æ ‡ç­¾ï¼ˆhard labelï¼‰å’Œ Teacherï¼ˆsoft labelï¼‰çš„ä¿¡ä»»åº¦  \n",
    "- å¤§æ¨¡å‹è¶Šå¼ºï¼Œä¸€èˆ¬å¯ä»¥è®© $ \\alpha $ ç¨å¾®å°ä¸€ç‚¹ï¼ˆæ›´ä¾èµ– Teacherï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.4 KL è’¸é¦æ¢¯åº¦çš„ç›´è§‚æ¨å¯¼\n",
    "\n",
    "KL éƒ¨åˆ†å¯¹ Student logits çš„æ¢¯åº¦ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{KL}}}{\\partial z_k^S} \n",
    "= p_k^S - p_k^T\n",
    "$$\n",
    "\n",
    "ç›´è§‚è§£é‡Šï¼š\n",
    "\n",
    "- å­¦ç”Ÿè¾“å‡ºæ¯” Teacher å¤§ï¼ˆ$ p_k^S > p_k^T $ï¼‰ â†’ æ¢¯åº¦ä¸ºæ­£ â†’ ä¸‹é™æ—¶å¾€ä¸‹æ‹‰  \n",
    "- å­¦ç”Ÿè¾“å‡ºæ¯” Teacher å°ï¼ˆ$ p_k^S < p_k^T $ï¼‰ â†’ æ¢¯åº¦ä¸ºè´Ÿ â†’ ä¸‹é™æ—¶å¾€ä¸Šæ¨  \n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "\n",
    "> Student è°ƒæ•´è‡ªå·±çš„è¾“å‡ºæ¦‚ç‡ï¼Œä½¿å…¶é€æ­¥é€¼è¿‘ Teacher çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "\n",
    "ä¸äº¤å‰ç†µ CE çš„æ¢¯åº¦å¯¹æ¯”ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{CE}}}{\\partial z_k^S} = p_k^S - y_k\n",
    "$$\n",
    "\n",
    "- å¯¹ CE æ¥è¯´ï¼Œtarget æ˜¯ one-hot çš„ $y_k$\n",
    "- å¯¹ KL è’¸é¦æ¥è¯´ï¼Œtarget æ˜¯ soft çš„ $p_k^T$\n",
    "\n",
    "å› æ­¤ï¼š\n",
    "\n",
    "> è’¸é¦ = ç”¨ Teacher çš„è½¯åˆ†å¸ƒ $p^T$ æ›¿ä»£ç¡¬åˆ†å¸ƒ $y$ æ¥â€œç›‘ç£â€ Studentã€‚\n",
    "\n",
    "---\n",
    "#### 2.3.4.x KLæ¢¯åº¦æ¨å¯¼\n",
    "\n",
    "Student æ¦‚ç‡ï¼ˆsoftmaxï¼‰ï¼š \n",
    "$$p^S_i = \\frac{e^{z^S_i}}{\\sum_j e^{z^S_j}}$$\n",
    "\n",
    "KL è’¸é¦æŸå¤±ï¼ˆå¿½ç•¥å‰é¢çš„ $T^2$ï¼‰ï¼š\n",
    "$$L_{\\mathrm{KL}} = \\sum_i p^T_i \\log \\frac{p^T_i}{p^S_i}\n",
    "               = \\sum_i p^T_i \\big(\\log p^T_i - \\log p^S_i\\big)$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $\\log p^T_i$éƒ¨åˆ†ä¸ Student æ— å…³ï¼ˆå¸¸æ•°ï¼‰\n",
    "- çœŸæ­£å’Œ student æœ‰å…³çš„æ˜¯è¿™ä¸€é¡¹\n",
    "$$L' = - \\sum_i p^T_i \\log p^S_i$$\n",
    "è¿™å…¶å®å°±æ˜¯ä»¥ **$p^T$ä¸º label çš„äº¤å‰ç†µ**\n",
    "$$L' = H(p^T, p^S)$$\n",
    "\n",
    "æ‰€ä»¥æœ¬è´¨å°±æ˜¯ï¼š **KL çš„æ¢¯åº¦ = äº¤å‰ç†µ w.r.t. logits çš„æ¢¯åº¦ã€‚**\n",
    "\n",
    "å¯¹ logits æ±‚å¯¼ï¼š$\\partial L' / \\partial z_k^S$\n",
    "$$L' = - \\sum_i p^T_i \\log p^S_i$$\n",
    "- ç¬¬ä¸€æ­¥ï¼šå¯¹ $p^S_i$ æ±‚åå¯¼ï¼ˆé“¾å¼æ³•åˆ™ï¼‰\n",
    "\n",
    "$$\\frac{\\partial L'}{\\partial z_k^S} = \\sum_i \\frac{\\partial L'}{\\partial p^S_i} \\cdot \\frac{\\partial p^S_i}{\\partial z_k^S}$$\n",
    "å…ˆç®—ç¬¬ä¸€éƒ¨åˆ†ï¼š\n",
    "$$\\frac{\\partial L'}{\\partial p^S_i} = - p^T_i \\cdot \\frac{1}{p^S_i}$$\n",
    "\n",
    "ç¬¬äºŒæ­¥ï¼šsoftmax çš„å¯¼æ•°\n",
    "\n",
    "å¯¹ softmaxï¼š\n",
    "$$p^S_i = \\frac{e^{z^S_i}}{\\sum_j e^{z^S_j}}$$\n",
    "è€Œ\n",
    "$$\\begin{array}{c} \\frac{\\partial p^S_i}{\\partial z_k^S}\n",
    "=\\begin{cases}\n",
    "p^S_i (1 - p^S_i), & i = k \\\\\n",
    "- p^S_i p^S_k, & i \\ne k\n",
    "\\end{cases} \\end{array}$$\n",
    "\n",
    "\n",
    "- ç¬¬äºŒæ­¥ï¼šæŠŠä¸¤éƒ¨åˆ†ä¹˜èµ·æ¥å¹¶æ±‚å’Œ\n",
    "$$\\frac{\\partial L'}{\\partial z_k^S}\n",
    "= \\sum_i \\left(- \\frac{p^T_i}{p^S_i}\\right) \\cdot \\frac{\\partial p^S_i}{\\partial z_k^S}$$\n",
    "- i = kï¼š\n",
    "$$\\left(- \\frac{p^T_k}{p^S_k}\\right) \\cdot p^S_k(1 - p^S_k)\n",
    "= - p^T_k (1 - p^S_k)\n",
    "= -p^T_k + p^T_k p^S_k$$\n",
    "- i â‰  kï¼š\n",
    "$$\\sum_{i \\ne k} \\left(- \\frac{p^T_i}{p^S_i}\\right) \\cdot (-p^S_i p^S_k)\n",
    "= \\sum_{i \\ne k} p^T_i p^S_k\n",
    "= p^S_k \\sum_{i \\ne k} p^T_i$$\n",
    "\n",
    "==>\n",
    "$$\\frac{\\partial L'}{\\partial z_k^S}\n",
    "= \\big(-p^T_k + p^T_k p^S_k\\big) + p^S_k \\sum_{i \\ne k} p^T_i$$\n",
    "\n",
    "æ³¨æ„åˆ° $\\sum_{i \\ne k} p^T_i = 1 - p^T_k$\n",
    "\n",
    "æ‰€ä»¥ï¼š\n",
    "$$\\frac{\\partial L'}{\\partial z_k^S}\n",
    "= -p^T_k + p^T_k p^S_k + p^S_k (1 - p^T_k)\n",
    "= -p^T_k + p^T_k p^S_k + p^S_k - p^S_k p^T_k\n",
    "= p^S_k - p^T_k$$\n",
    "\n",
    "$$\\boxed{\n",
    "\\frac{\\partial L_{\\mathrm{KL}}}{\\partial z_k^S} \n",
    "= p^S_k - p^T_k\n",
    "}$$\n",
    "\n",
    "--- \n",
    "\n",
    "### 2.3.5 Temperature $ T $ çš„ä½œç”¨ï¼ˆæ•°å­¦è§£é‡Šï¼‰\n",
    "\n",
    "å¸¦æ¸©åº¦çš„ softmaxï¼š\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}}\n",
    "$$\n",
    "\n",
    "- $ T > 1 $ï¼šåˆ†å¸ƒæ›´å¹³æ»‘ï¼ˆå„ç±»æ¦‚ç‡æ›´å‡åŒ€ï¼Œç±»é—´å…³ç³»æ›´æ˜æ˜¾ï¼‰  \n",
    "- $ T < 1 $ï¼šåˆ†å¸ƒæ›´å°–é”ï¼ˆæ›´æ¥è¿‘ argmaxï¼‰  \n",
    "\n",
    "è’¸é¦ä¸€èˆ¬ä½¿ç”¨ $T = 2 ï½ 4$ ï¼ŒåŸå› æ˜¯ï¼š\n",
    "\n",
    "- é«˜æ¸©ä½¿ softmax è¾“å‡ºå¯¹ logits çš„å˜åŒ–æ›´åŠ â€œæ•æ„Ÿä¸”å¹³æ»‘â€  \n",
    "- KL-loss å¯¹æ‰€æœ‰ç±»åˆ«éƒ½äº§ç”Ÿæ¢¯åº¦ï¼Œè€Œä¸ä»…ä»…æ˜¯ top-1  \n",
    "- Student èƒ½â€œçœ‹è§â€ Teacher å¯¹æ‰€æœ‰ç±»åˆ«çš„ç›¸å¯¹åå¥½ï¼Œå­¦åˆ°æ›´ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.6 å®é™…å·¥ç¨‹ä¸­çš„è’¸é¦æµç¨‹ï¼ˆTeacher â†’ Studentï¼‰\n",
    "\n",
    "ä¸€ä¸ªå…¸å‹çš„è’¸é¦ pipelineï¼š\n",
    "\n",
    "1. **è®­ç»ƒ Teacherï¼ˆå¤§æ¨¡å‹ï¼‰**  \n",
    "   - ç”¨å¸¸è§„ FP32 æˆ–æ··åˆç²¾åº¦è®­ç»ƒ  \n",
    "   - å¾—åˆ°æ€§èƒ½è¾ƒå¥½çš„å¤§æ¨¡å‹ $f_T$\n",
    "\n",
    "2. **å‡†å¤‡ Teacher è¾“å‡º**  \n",
    "   - å¯¹è®­ç»ƒé›†æˆ–é¢å¤–çš„è’¸é¦æ•°æ®é›†è·‘ Teacher  \n",
    "   - ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„ Teacher logits $z^T$ æˆ– soft probabilities $p^T$\n",
    "\n",
    "3. **æ„å»º Studentï¼ˆå°æ¨¡å‹ï¼‰**  \n",
    "   - æ›´å°çš„å®½åº¦/æ·±åº¦  \n",
    "   - æ›´æ˜“é‡åŒ–/å‰ªæçš„ç»“æ„ï¼ˆä¾‹å¦‚æ›´çª„çš„ MLPã€éƒ¨åˆ† head å‰ªæï¼‰\n",
    "\n",
    "4. **è”åˆè®­ç»ƒ Student**  \n",
    "   - å‰å‘ï¼šStudent è¾“å‡º logits $z^S$ã€æ¦‚ç‡ $p^S$  \n",
    "   - è®¡ç®—ï¼š\n",
    "     - hard lossï¼š$ L_{\\text{CE}}(y, p^S) $\n",
    "     - soft lossï¼š$ L_{\\text{KL}}(p^T \\| p^S) $\n",
    "   - ç»„åˆï¼š\n",
    "     $$L = \\alpha L_{\\text{CE}} + (1-\\alpha)L_{\\text{KL}}$$\n",
    "   - åå‘ä¼ æ’­ï¼Œæ›´æ–° Student å‚æ•°\n",
    "\n",
    "5. **ï¼ˆå¯é€‰ï¼‰å¢åŠ ç‰¹å¾è’¸é¦ï¼ˆFeature Distillationï¼‰**  \n",
    "   - å¼ºåˆ¶ Student çš„ä¸­é—´å±‚ç‰¹å¾æ¥è¿‘ Teacherï¼š\n",
    "     $$\n",
    "     L_{\\text{feat}} = \\| h^S - P(h^T) \\|_2^2\n",
    "     $$\n",
    "   - å…¶ä¸­ $P(\\cdot)$ æ˜¯æŠ•å½±å±‚ï¼Œç”¨äºå¯¹é½ç»´åº¦ã€‚\n",
    "\n",
    "6. **ï¼ˆå¯é€‰ï¼‰å¢åŠ æ³¨æ„åŠ›è’¸é¦ï¼ˆAttention Distillationï¼‰**  \n",
    "   - å¯¹ Transformerï¼š\n",
    "     $$\n",
    "     L_{\\text{attn}} = \\| A^S - A^T \\|_2^2\n",
    "     $$\n",
    "   - å…¶ä¸­ $A$ æ˜¯ Attention mapï¼ˆå¦‚ softmax(QK^\\top / \\sqrt{d_k})ï¼‰\n",
    "\n",
    "7. **å¾—åˆ°æœ€ç»ˆçš„ Student**  \n",
    "   - æ›´è½»é‡ã€æ›´å¿«ã€æ›´é€‚åˆé‡åŒ–ä¸å‰ªæ  \n",
    "   - å¸¸ç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼ˆè½¦ç«¯ã€ç§»åŠ¨ç«¯ã€robot ç­‰ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.7 ä¸ Runtime Inference åœºæ™¯çš„å…³ç³»\n",
    "\n",
    "åœ¨éƒ¨ç½²åœºæ™¯ä¸­ï¼Œä½ å¯ä»¥ï¼š\n",
    "\n",
    "- å…ˆè®­ç»ƒä¸€ä¸ªè¾ƒå¤§çš„ Teacherï¼ˆå¯èƒ½åœ¨äº‘ç«¯/æ•°æ®ä¸­å¿ƒï¼‰  \n",
    "- ç„¶åè®¾è®¡ä¸€ä¸ªä¸º **è½¦ç«¯/ECU/NPU ä¼˜åŒ–** çš„ Studentï¼š\n",
    "  - æ›´å°çš„å®½åº¦/æ·±åº¦  \n",
    "  - æ›´å‹å¥½çš„å¼ é‡å½¢çŠ¶å’Œ kernel æ˜ å°„  \n",
    "  - ä¾¿äº int8 / int4 é‡åŒ–ã€ä½ç§©åˆ†è§£ã€ç»“æ„åŒ–å‰ªæ\n",
    "\n",
    "ç„¶åï¼š\n",
    "\n",
    "1. ç”¨çŸ¥è¯†è’¸é¦ä¿è¯ Student åœ¨ç²¾åº¦ä¸Šå°½é‡é€¼è¿‘ Teacher  \n",
    "2. å†å¯¹ Student åšï¼š\n",
    "   - é‡åŒ–ï¼ˆPTQ/QATï¼‰\n",
    "   - å‰ªæï¼ˆç»“æ„åŒ–é€šé“/å¤´å‰ªæï¼‰\n",
    "   - low-rank/SVD åˆ†è§£\n",
    "3. æœ€ç»ˆå¯¼å‡ºä¸€ä¸ª **â€œè’¸é¦ + å‹ç¼© + é‡åŒ–â€ ä¸€ä½“åŒ–çš„éƒ¨ç½²ç‰ˆæœ¬**\n",
    "\n",
    "> æ¢å¥è¯è¯´ï¼š  \n",
    "> **è’¸é¦ä¸æ˜¯â€œå‹æ¨¡å‹â€ï¼Œè€Œæ˜¯â€œæŠŠå¤§æ¨¡å‹çš„å‡½æ•°ç©ºé—´æŠ•å½±åˆ°å°æ¨¡å‹çš„å‡½æ•°å­ç©ºé—´â€ã€‚**  \n",
    "> è¿™å°±æ˜¯å®ƒåœ¨æ•°å­¦å’Œå·¥ç¨‹ä¸Šçš„æ ¸å¿ƒæ„ä¹‰ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285e30a",
   "metadata": {},
   "source": [
    "\n",
    "## 2.4 å¤šç›®æ ‡ä¼˜åŒ–ï¼šç²¾åº¦ vs å»¶è¿Ÿ vs å†…å­˜\n",
    "\n",
    "åœ¨åšéƒ¨ç½²æ—¶ï¼Œä½ ä¸ä¼šåªå…³å¿ƒ lossï¼Œè¿˜ä¼šå…³å¿ƒï¼š\n",
    "\n",
    "- latencyï¼ˆæ¨ç†å»¶è¿Ÿï¼‰\n",
    "- memoryï¼ˆæ˜¾å­˜ / DRAM å ç”¨ï¼‰\n",
    "- throughputï¼ˆQPSï¼‰\n",
    "\n",
    "å¯ä»¥å°†è¿™äº›çº³å…¥ä¼˜åŒ–ç›®æ ‡ï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\quad \\alpha \\cdot \\text{Error}(\\theta)\n",
    "+ \\beta \\cdot \\text{Latency}(\\theta)\n",
    "+ \\gamma \\cdot \\text{Memory}(\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $\\theta$ï¼šæ¨¡å‹ç»“æ„ + é‡åŒ–é…ç½® + å‰ªæç­–ç•¥ ç­‰\n",
    "- $\\text{Latency}(\\theta)$ï¼šé€šè¿‡ profile æˆ– analytical model ä¼°è®¡\n",
    "- $\\text{Memory}(\\theta)$ï¼šç”±å‚æ•°é‡ã€activationã€KV cache å†³å®š\n",
    "\n",
    "å·¥ç¨‹å®ç°ä¸­å¸¸è§ç®€åŒ–ï¼š\n",
    "\n",
    "- å›ºå®šæŸä¸ª latency/memory ä¸Šé™ä½œä¸ºçº¦æŸï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\text{Error}(\\theta)\n",
    "  \\quad \\text{s.t.} \\quad \\text{Latency}(\\theta) \\le L_{\\max}, \\\n",
    "  \\text{Memory}(\\theta) \\le M_{\\max}\n",
    "$$\n",
    "\n",
    "\n",
    "- æˆ–æŠŠ latency/memory è½¬æ¢ä¸ºæ­£åˆ™é¡¹åŠ å…¥ loss\n",
    "\n",
    "> é‡è¦çš„æ˜¯ï¼šä½ è¦èƒ½æŠŠâ€œéƒ¨ç½²éœ€æ±‚â€ç¿»è¯‘æˆæ•°å­¦ä¸Šçš„â€œç›®æ ‡ + çº¦æŸâ€ï¼Œè¿™æ ·æ‰èƒ½ç”¨ä¼˜åŒ–å·¥å…·ç³»ç»Ÿåœ°è®¾è®¡ç®—æ³•ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 2.5 æœ¬ç« å°ç»“\n",
    "\n",
    "æœ¬ç« é‡ç‚¹ï¼š\n",
    "\n",
    "1. å‰ªæå¯ä»¥çœ‹ä½œå¸¦ $L_0/L_1$ ç¨€ç–çº¦æŸçš„ä¼˜åŒ–é—®é¢˜\n",
    "2. é‡åŒ–å¯ä»¥é€šè¿‡æœ€å°åŒ–é‡åŒ–è¯¯å·®çš„ä¼˜åŒ–é—®é¢˜ç¡®å®š scale/zero-pointï¼Œè¿›ä¸€æ­¥å¯é€šè¿‡è®­ç»ƒ joint optimize\n",
    "3. è’¸é¦æ˜¯ä¸€ä¸ªè”åˆæœ€å°åŒ– CE å’Œ KL çš„å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜\n",
    "4. éƒ¨ç½²æ—¶çš„ç²¾åº¦ã€å»¶è¿Ÿã€å†…å­˜å¯ä»¥ç»Ÿä¸€æœ¬ä¸ºå¤šç›®æ ‡/å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜\n",
    "\n",
    "> åœ¨åç»­ç« èŠ‚ä¸­ï¼Œè¿™äº›ä¼˜åŒ–ç›®æ ‡ä¼šä¸è¿‘ä¼¼ç†è®ºã€æ¦‚ç‡ç»Ÿè®¡ã€ç¡¬ä»¶æ¨¡å‹ç»“åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€å¥—å®Œæ•´çš„æ¨ç†åŠ é€Ÿæ€ç»´ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde561f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0591187",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use pretrained ResNet-18 as teacher\n",
    "teacher = resnet18(weights=\"IMAGENET1K_V1\")\n",
    "# fine tune last layer for CIFAR-10\n",
    "teacher.fc = nn.Linear(512, 10)\n",
    "teacher = teacher.to(device)\n",
    "opt_teacher = torch.optim.Adam(teacher.parameters(), lr=1e-4)\n",
    "\n",
    "def train_teacher(epochs, train_loader):\n",
    "    teacher.train()\n",
    "    for _ in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = teacher(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            opt_teacher.zero_grad()\n",
    "            loss.backward()\n",
    "            opt_teacher.step()\n",
    "\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate student\n",
    "student = SmallCNN(num_classes=10).to(device)\n",
    "opt_student = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
    "alpha = 0.3   # weight for CE loss\n",
    "T = 4.0       # distillation temperature\n",
    "\n",
    "def kd_loss(student_logits, teacher_logits, T=4.0):\n",
    "    # Softmax with temperature\n",
    "    teacher_probs = F.softmax(teacher_logits / T, dim=1)\n",
    "    student_log_probs = F.log_softmax(student_logits / T, dim=1)\n",
    "\n",
    "    # KL divergence, aggregated over batch \n",
    "    loss_kl = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')\n",
    "\n",
    "    return (T * T) * loss_kl\n",
    "\n",
    "\n",
    "\n",
    "# 1. æ•°æ®å¢å¼º / é¢„å¤„ç†\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 2. åŠ è½½ CIFAR-10\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 3. DataLoader\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "train_teacher(epochs=1, train_loader=dataloader)\n",
    "teacher.eval()  # freeze teacher\n",
    "# train_student_with_distillation\n",
    "for x, y in dataloader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    # Teacher forward (ä¸éœ€è¦è®¡ç®—æ¢¯åº¦)\n",
    "    with torch.no_grad():\n",
    "        teacher_logits = teacher(x)\n",
    "\n",
    "    # Student forward\n",
    "    student_logits = student(x)\n",
    "\n",
    "    # Hard label loss\n",
    "    loss_ce = F.cross_entropy(student_logits, y)\n",
    "\n",
    "    # Distillation loss\n",
    "    loss_kd = kd_loss(student_logits, teacher_logits, T)\n",
    "\n",
    "    # Combine\n",
    "    loss = alpha * loss_ce + (1 - alpha) * loss_kd\n",
    "\n",
    "    opt_student.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_student.step()\n",
    "\n",
    "    print(\"loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e01b9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.805099228668213, 'accuracy': 0.1379}\n"
     ]
    }
   ],
   "source": [
    "# Check student performance on test set\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    return {\"loss\": avg_loss, \"accuracy\": acc}\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student.to(device)\n",
    "metrics = evaluate(student, test_loader, device)\n",
    "print(metrics)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.28790156097412, 'accuracy': 0.0009}\n"
     ]
    }
   ],
   "source": [
    "# Check original teacher performance on test set\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "teacher.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "metrics = evaluate(teacher, test_loader, device)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f98e8",
   "metadata": {},
   "source": [
    "| åå­—           | æ¨¡å—                      | ä½œç”¨     |\n",
    "| ------------ | ------------------------------ | ------ |\n",
    "| `teacher`    | `teacher = resnet18(...)`      | å¤§æ¨¡å‹ï¼Œå†»ç»“ |\n",
    "| `student`    | `student = SmallCNN(...)`      | å°æ¨¡å‹ï¼Œè®­ç»ƒ |\n",
    "| `dataloader` | `dataloader = DataLoader(...)` | æä¾›è®­ç»ƒæ ·æœ¬ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df53884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
