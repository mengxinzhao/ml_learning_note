{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ecd59b",
   "metadata": {},
   "source": [
    "# ç¬¬ 2 ç« ï¼šæ•°å€¼ä¼˜åŒ–ï¼ˆOptimization for Pruning, Quantization & Distillationï¼‰\n",
    "\n",
    "æœ¬ç« ç›®æ ‡ï¼šç†è§£å‰ªæã€é‡åŒ–ã€è’¸é¦ç­‰æ“ä½œèƒŒåçš„â€œä¼˜åŒ–é—®é¢˜â€è§†è§’ã€‚  \n",
    "ä½ ä¸éœ€è¦æˆä¸ºä¼˜åŒ–ç†è®ºä¸“å®¶ï¼Œä½†éœ€è¦ï¼š\n",
    "\n",
    "- çœ‹æ‡‚å¸¸è§æŸå¤±å‡½æ•°ä¸æ­£åˆ™é¡¹çš„å½¢å¼\n",
    "- ç†è§£ L0/L1 ç¨€ç–åŒ–ã€é‡åŒ–å‚æ•°ä¼˜åŒ–çš„å¤§è‡´æ€è·¯\n",
    "- çŸ¥é“å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆç²¾åº¦ vs å»¶è¿Ÿï¼‰çš„å…¸å‹å†™æ³•\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 å‰ªæï¼ˆPruningï¼‰çš„ä¼˜åŒ–è§†è§’\n",
    "\n",
    "### 2.1.1 åŸºæœ¬ç›®æ ‡\n",
    "\n",
    "ç»™å®šè®­ç»ƒå¥½çš„æƒé‡ $W$ã€æŸå¤±å‡½æ•° $L(W)$ï¼Œå‰ªæå¸Œæœ›ï¼š\n",
    "\n",
    "- å¤§é‡å…ƒç´ å˜ä¸º 0ï¼ˆç¨€ç–ï¼‰\n",
    "- æŸå¤±/ç²¾åº¦å˜åŒ–å°½é‡å°\n",
    "\n",
    "å¯ä»¥å½¢å¼åŒ–ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\hat{W}} L(\\hat{W}) + \\lambda \\|\\hat{W}\\|_0\n",
    "$$\n",
    "\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- è¿™é‡Œ $L$ æ˜¯æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼ˆè®­ç»ƒæ—¶ç”¨çš„ç›®æ ‡å‡½æ•°ï¼‰ï¼Œåœ¨æƒé‡ä¸º $\\hat{W}$ æ—¶çš„æŸå¤±å€¼\n",
    "- $\\|\\hat{W}\\|_0$ï¼šéé›¶å…ƒç´ ä¸ªæ•°ï¼ˆç¨€ç–æ€§åº¦é‡ï¼‰\n",
    "- $\\lambda$ï¼šæƒè¡¡â€œç²¾åº¦ vs ç¨€ç–åº¦â€çš„è¶…å‚æ•°\n",
    "\n",
    "è¿™ä¸ªé—®é¢˜ä¸€èˆ¬æ˜¯ NP-hardï¼Œå¸¸è§è¿‘ä¼¼æ–¹å¼æœ‰ï¼š\n",
    "\n",
    "1. ç”¨ $L_1$ èŒƒæ•°æ›¿ä»£ $L_0$ï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\hat{W}} L(\\hat{W}) + \\lambda \\|\\hat{W}\\|_1\n",
    "$$\n",
    "\n",
    "\n",
    "2. å…ˆæ­£å¸¸è®­ç»ƒï¼Œå†åšåŸºäºæŸç§ saliency çš„åå¤„ç†å‰ªæï¼š\n",
    "   - æŒ‰æƒé‡ç»å¯¹å€¼å¤§å°å‰ªæ‰å°å€¼\n",
    "   - æŒ‰ Hessian è¿‘ä¼¼ï¼ˆå¦‚ OBS/OBDï¼‰è®¡ç®—æ•æ„Ÿåº¦\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2  HessiançŸ©é˜µ & saliency score\n",
    "åè®­ç»ƒå‰ªæï¼ˆpost-training pruningï¼‰æ ¸å¿ƒæ˜¯\n",
    "\n",
    "- è¿™æ¡è¾¹ï¼ˆæŸä¸ª weightï¼‰åˆ æ‰ä¼šä¸ä¼šä¼¤å®³ lossï¼Ÿ\n",
    "\n",
    "- å“ªä¸ªé€šé“æ›´é‡è¦ï¼Ÿ\n",
    "\n",
    "- å“ªå±‚è¯¥å¤šå‰ªï¼Ÿå“ªå±‚ä¸èƒ½ä¹±åŠ¨ï¼Ÿj z\n",
    "\n",
    "ğŸ”· 1. ä»€ä¹ˆæ˜¯ Hessianï¼Ÿ\n",
    "Hessian æ˜¯ loss å¯¹å‚æ•°çš„äºŒé˜¶å¯¼æ•°çŸ©é˜µï¼š\n",
    "$$H = \\nabla^2_{W} L(W)$$\n",
    "æ›´ç²¾ç¡®åœ°ï¼š\n",
    "- å¦‚æœæ¨¡å‹å‚æ•°æ˜¯$$W \\in \\mathbb{R}^n$$ï¼Œ åˆ™ Hessian æ˜¯ï¼š$$H \\in \\mathbb{R}^{n \\times n}$$\n",
    "å…¶ä¸­æ¯ä¸ªå…ƒç´ ï¼š $$H_{ij} = \\frac{\\partial^2 L}{\\partial W_i \\partial W_j}$$\n",
    "ä»£è¡¨ loss å¯¹ä¸¤ä¸ª weight çš„äºŒé˜¶ç›¸äº’å½±å“ã€‚\n",
    "\n",
    "ğŸ”· 2. ä¸ºä»€ä¹ˆ Hessian ä¸å‰ªææœ‰å…³ï¼Ÿ\n",
    "\n",
    "é—®é¢˜ï¼š\n",
    "å¦‚æœæŠŠæŸä¸ª weightä» $w_i$->0, loss ä¼šå˜å¤šå°‘ï¼Ÿ\n",
    "å¦‚æœä½ ç›´æ¥çœ‹$|w_i|$,åªèƒ½çŸ¥é“å®ƒâ€œå¤§å°â€ï¼Œä¸çŸ¥é“ï¼š\n",
    "- è¿™ä¸ªæƒé‡å¯¹ loss çš„æ•æ„Ÿåº¦å¦‚ä½•\n",
    "\n",
    "- æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯ä¸æ˜¯â€œå…³é”®è·¯å¾„â€ä¸Šçš„é‡è¦å‚æ•°ï¼Ÿ\n",
    "\n",
    "ç®€å•ç»å¯¹å€¼æ˜¯**ç²—ç³™ä»£ç†**ã€‚Hessian åˆ™è¡¡é‡äº†ï¼š**æ”¹å˜ä¸€ä¸ªæƒé‡ï¼Œä¼šè®© loss å¢å¤§å¤šå°‘**, æ›´ç²¾ç¡®ã€‚\n",
    "\n",
    "ğŸ”¥ 3. ç”¨äºŒé˜¶æ³°å‹’å±•å¼€ä¼°è®¡â€œå‰ªæ‰æŸä¸ª weight ä¼šå¸¦æ¥å¤šå°‘æŸå¤±å¢åŠ â€ï¼Ÿ\n",
    "å‡è®¾æˆ‘ä»¬è¦æŠŠç¬¬$i$ä¸ª weightä»å½“å‰$w_i$è®¾ç½®æˆ 0ï¼š\n",
    "$$\\Delta w_i = -w_i$$\n",
    "loss çš„å˜åŒ–ï¼ˆç”¨äºŒé˜¶æ³°å‹’å±•å¼€ï¼‰ï¼š\n",
    "$$\\Delta L \\approx \n",
    "\\frac{\\partial L}{\\partial w_i}\\Delta w_i + \\frac{1}{2} H_{ii} (\\Delta w_i)^2$$\n",
    "ä½†ï¼š\n",
    "- åœ¨è®­ç»ƒå®Œçš„ç‚¹, æ¢¯åº¦$\\partial L/\\partial w_i \\approx 0$ï¼ˆå› ä¸ºè®­ç»ƒæ”¶æ•›ï¼‰\n",
    "- æ‰€ä»¥å‰©ä¸‹ï¼š$$\\Delta L \\approx \\frac{1}{2} H_{ii} w_i^2$$\n",
    "\n",
    "è‡³æ­¤ **saliency score** = $H_{ii} w_i^2$\n",
    "è¿™æ¯”å•çœ‹$|w_i|$æ›´æœ‰æ„ä¹‰\n",
    "\n",
    "ğŸ”· ç›´è§‰çš„è§£é‡Š\n",
    "- æ³°å‹’å±•å¼€è¯´ï¼š\n",
    "â€œåœ¨ä¸€ä¸ªç‚¹é™„è¿‘ï¼Œå‡½æ•°çš„å˜åŒ– â‰ˆ ä¸€é˜¶ï¼ˆæ–œç‡ï¼‰ + äºŒé˜¶ï¼ˆå¼¯æ›²ç¨‹åº¦ï¼‰ã€‚â€\n",
    "\n",
    "- åœ¨å·²ç»è®­ç»ƒå¥½çš„ç‚¹ï¼š\n",
    "ä¸€é˜¶å¯¼ï¼ˆæ–œç‡ï¼‰æ¥è¿‘ 0ï¼Œæ‰€ä»¥å˜åŒ–ä¸»è¦é äºŒé˜¶å¯¼å†³å®šã€‚\n",
    "\n",
    "- å¯¹äºå‰ªæï¼š\n",
    "ä½ æŠŠ$w_i$æ”¹æˆ 0ï¼Œç›¸å½“äºåœ¨å‚æ•°ç©ºé—´é‡Œæ²¿ç€ç¬¬ i ä¸ªåæ ‡æ–¹å‘ï¼Œèµ°äº†ä¸€ä¸ªæ­¥é•¿ $-w_i$\n",
    "æŸå¤±å¢åŠ  â‰ˆ å¼¯æ›²ç¨‹åº¦ Ã— ä½ èµ°çš„è·ç¦»çš„å¹³æ–¹ã€‚\n",
    "\tâ€‹\n",
    "- â€œå¼¯æ›²ç¨‹åº¦â€åœ¨è¿™ä¸ªæ–¹å‘å°±æ˜¯ Hessian çš„å¯¹è§’çº¿å…ƒç´  $H_{ii}$\n",
    " - å¦‚æœè¿™ä¸ªæ–¹å‘ä¸Šå¼¯å¾—å¾ˆå‰å®³ï¼ˆH å¤§ï¼‰ï¼Œè¯´æ˜å¯¹è¿™ä¸ªå‚æ•°å¾ˆæ•æ„Ÿï¼Œä¸èƒ½ä¹±å‰ªã€‚\n",
    " - å¦‚æœè¿™ä¸ªæ–¹å‘å‡ ä¹æ˜¯å¹³çš„ï¼ˆH å°ï¼‰ï¼Œå³ä½¿ $w_i$ä¸ä¸ºé›¶ï¼Œä¹Ÿå¯ä»¥å¤§èƒ†å‰ª\n",
    "\n",
    "æ‰€ä»¥ï¼š\n",
    "- åªçœ‹$|w_i|$ â†’ åªæ˜¯çœ‹è¿™ä¸ªå‚æ•°â€œå¤§ä¸å¤§â€ã€‚\n",
    "- çœ‹$H_{ii} w_i^2$ â†’ çœ‹â€œåˆ æ‰å®ƒé€ æˆ loss ä¸Šå‡æœ‰å¤šå¤§â€ï¼Œè¿™æ˜¯æ›´åˆç†çš„â€œé‡è¦æ€§â€åº¦é‡ã€‚\n",
    "\n",
    "ğŸ”· 4. OBSï¼ˆOptimal Brain Surgeonï¼‰ä¸ OBDï¼ˆOptimal Brain Damageï¼‰\n",
    "è¿™æ˜¯ 1990 å¹´ä»£çš„ç»å…¸å‰ªææ–¹æ³•ï¼š\n",
    "\n",
    "ğŸŸ¦ OBDï¼ˆOptimal Brain Damageï¼‰\n",
    "\n",
    "å®ƒä½¿ç”¨å¯¹è§’ Hessianï¼š\n",
    "$$\\text{saliency}_i = \\frac{1}{2} H_{ii} w_i^2$$\n",
    "ç›´è§‰ï¼š\n",
    "* å¦‚æœ $H_{ii}$å¾ˆå¤§ â†’ è¯¥weight éå¸¸æ•æ„Ÿ\n",
    "* å°±ç®—å®ƒå¾ˆå°ï¼Œä¹Ÿä¸èƒ½è½»æ˜“åˆ \n",
    "* å¦‚æœ $H_{ii}$ å¾ˆå° â†’ è¿™ä¸ª weight ä¸é‡è¦\n",
    "* å¯ä»¥å®‰å…¨å‰ªæ‰\n",
    "\n",
    "ç¼ºç‚¹ï¼šå¿½ç•¥ Hessian çš„ off-diagonalï¼ˆè·¨ weight ç›¸å…³æ€§ï¼‰ã€‚\n",
    "\n",
    "ğŸŸ© OBSï¼ˆOptimal Brain Surgeonï¼‰\n",
    "OBS æ›´ç²¾ç¡®ï¼š\n",
    "$$\\text{saliency}_i = \n",
    "\\frac{1}{2} \\frac{w_i^2}{(H^{-1})_{ii}}$$\n",
    "\n",
    "OBS ä½¿ç”¨é€† Hessianï¼Œè€ƒè™‘æ‰€æœ‰ weight ä¹‹é—´çš„ç›¸äº’å½±å“ï¼Œç†è®ºä¸Šæ›´å‡†ç¡®ï¼š\n",
    "\n",
    "- å¦‚æœ weight èƒ½è¢«å…¶ä»–å‚æ•°è¡¥å¿ï¼ˆHessian inverse åæ˜ ç›¸å…³æ€§ï¼‰ï¼Œå®ƒæ›´é€‚åˆåˆ \n",
    "- ä½†è®¡ç®— Hessian inverse å¤ªè´µï¼Œç°ä»£å¾ˆå°‘ç”¨å…¨é‡ OBSã€‚\n",
    "\n",
    "ğŸ”¥ 5. å·¥ç¨‹åšæ³•ï¼ˆPruning-aware methodsï¼‰\n",
    "çœŸå®å·¥ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸å¯èƒ½ç®—çœŸæ­£çš„ Hessianï¼ˆå¤ªå¤§ï¼‰ï¼Œä½†å¯ä»¥ï¼š\n",
    "\n",
    "âœ” è¿‘ä¼¼å¯¹è§’çº¿ï¼š\n",
    "- ç”¨ Fisher ä¿¡æ¯çŸ©é˜µå¯¹è§’çº¿\n",
    "- ç”¨ Hutchinson estimator\n",
    "- ç”¨æ¢¯åº¦å¹³æ–¹çš„ moving average , ç±»ä¼¼ Adam çš„ $v_t$ ï¼Œè¿™å°±æŠŠ Hessian ä¼°è®¡å˜æˆå¯è®­ç»ƒã€å¯éƒ¨ç½²çš„ä¸œè¥¿ã€‚\n",
    "æ­¤å¤„\n",
    "$H_{ii} \\approx \\mathbb{E}[g_i^2]$ï¼Œ \n",
    "\n",
    "$g_i = \\frac{\\partial L}{\\partial w_i}$\n",
    "è¿™ä¸ªæ¢¯åº¦éšç€ batch è€Œå˜åŒ–ï¼Œå› ä¸º loss æ˜¯åŸºäº mini-batch æ±‚å‡ºæ¥çš„ï¼Œæ‰€ä»¥é€šå¸¸ç”¨ï¼š\n",
    "$$\\mathbb{E}[g_i^2] = \\text{over minibatches }$$\n",
    "æ¢å¥è¯è¯´ï¼š\n",
    "\n",
    "- æ¯ä¸ª batch ä¼šç®—ä¸€æ¬¡æ¢¯åº¦ $g = \\nabla L$\n",
    "- å–å®ƒçš„å¹³æ–¹ $g_i^2$\n",
    "- å†å¯¹ batch æ±‚æœŸæœ›ï¼ˆå¹³å‡ï¼‰\n",
    "\n",
    "å°±å¾—åˆ°äº†å¯¹è§’çº¿ Hessian çš„ä¼°è®¡ã€‚\n",
    "\n",
    "\n",
    "ğŸ“Œ ä¸ºä»€ä¹ˆæ¢¯åº¦å¹³æ–¹å¯ä»¥è¿‘ä¼¼ Hessian å¯¹è§’çº¿ï¼Ÿ\n",
    "åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨ **Cross-Entropy loss + Softmax** çš„æ¨¡å‹ï¼Œï¼ˆå…¶ä»–çš„ä¸ä¸€å®šå¯¹ï¼Œæ¯”å¦‚MSEï¼ˆå¹³æ–¹è¯¯å·®å›å½’ï¼‰ï¼‰é‡Œï¼š\n",
    "$$H_{ii} \\approx \\mathbb{E}[g_i^2]$$\n",
    "åŸå› æ˜¯ï¼š\n",
    "1. Hessian = äºŒé˜¶å˜åŒ–ç‡ï¼›Fisher ä¿¡æ¯ = æ¢¯åº¦æ–¹å·®\n",
    "å¯¹æ•°ä¼¼ç„¶æ¡ä»¶ä¸‹ï¼Œæœ‰æ•°å­¦å®šç†ï¼ˆCramerâ€“Rao + Information Equalityï¼‰ï¼š\n",
    "$$\\mathbb{E}[ \\nabla^2 L ] = \\mathbb{E}[ g g^\\top ]$$\n",
    "å–å¯¹è§’çº¿ï¼š\n",
    "$$H_{ii} \\approx \\mathbb{E}[g_i^2]$$\n",
    "æ¢å¥è¯è¯´ï¼š\n",
    "**æ¢¯åº¦çš„æ–¹å·® = äºŒé˜¶æ›²ç‡ï¼ˆHessian å¯¹è§’çº¿**\n",
    "\n",
    "ğŸ“Œ å·¥ç¨‹ä¸Šå¦‚ä½•è®¡ç®—$E[g_i^2]$?\n",
    "```code\n",
    "hessian_diag[i] += grad[i]**2\n",
    "count += 1\n",
    "...\n",
    "hessian_diag /= count\n",
    "```\n",
    "è¿™å°±æ˜¯ Hessian å¯¹è§’çº¿çš„æ— åä¼°è®¡ã€‚\n",
    "\n",
    "ğŸ“Œ ç›´è§‰çš„è§£é‡Š\n",
    "- æ¢¯åº¦æ˜¯â€œæŸå¤±å¯¹å‚æ•°çš„ä¸€é˜¶ååº”â€\n",
    "\n",
    "- æ¢¯åº¦å¹³æ–¹çš„æœŸæœ›å°±æ˜¯â€œæŸå¤±åœ¨è¿™ä¸ªæ–¹å‘ä¸Šçš„æ³¢åŠ¨å¼ºåº¦â€\n",
    "\n",
    "- æ³¢åŠ¨å¼ºè¯´æ˜â€œè¿™ä¸ªæ–¹å‘ä¸Š loss å¾ˆé™¡â€ â†’ Hessian å¤§\n",
    "\n",
    "- æ³¢åŠ¨å¼±è¯´æ˜â€œè¿™ä¸ªæ–¹å‘å¹³ç¼“â€ â†’ Hessian å°\n",
    "æ‰€ä»¥ï¼š\n",
    "$H_{ii} \\approx E[g_i^2]$\n",
    "---\n",
    "\n",
    "#### 2.1.2.1 äºŒé˜¶æ³°å‹’å±•å¼€è¯¦ç»†æ¨å¯¼\n",
    "ç°åœ¨å‚æ•°æ˜¯å‘é‡ï¼š $W = (w_1, w_2, \\dots, w_n)^\\top$\n",
    "loss æ˜¯ $L(W)$\n",
    "åœ¨ç‚¹ ğ‘Š é™„è¿‘åšäºŒé˜¶æ³°å‹’å±•å¼€ï¼š\n",
    "$$L(W + \\Delta W)\n",
    "\\approx\n",
    "L(W)\n",
    "+ \\nabla L(W)^\\top \\Delta W\n",
    "+ \\frac{1}{2} \\Delta W^\\top H \\Delta W$$\n",
    "\n",
    "è¿™é‡Œï¼š\n",
    "- $\\nabla L(W)$æ˜¯æ¢¯åº¦å‘é‡ï¼ˆé•¿åº¦ nï¼‰\n",
    "- $H = \\nabla^2 L(W)$ æ˜¯ Hessian çŸ©é˜µï¼ˆnÃ—n\n",
    "\n",
    "ç°åœ¨ **æˆ‘ä»¬åªåŠ¨ç¬¬ i ä¸ªå‚æ•°ï¼ŒæŠŠå®ƒå˜æˆ 0**\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "- åŸæ¥ï¼š$w_i$\n",
    "- å‰ªæåï¼š$w_i^{\\text{new}} = 0$\n",
    "æ‰€ä»¥ï¼š\n",
    "$$\\Delta W = W_{\\text{new}} - W_{\\text{old}}\n",
    "= (0,\\dots,0, -w_i, 0,\\dots,0)^\\top\n",
    "= -w_i e_i$$\n",
    "\n",
    "ä»£å…¥æ³°å‹’å±•å¼€å…¬å¼\n",
    "$$\\Delta L\n",
    "= L(W + \\Delta W) - L(W)\n",
    "\\approx \\nabla L(W)^\\top \\Delta W + \\frac{1}{2} \\Delta W^\\top H \\Delta W$$\n",
    "\n",
    "æ¢¯åº¦å‘é‡ï¼š\n",
    "$\\begin{array}{c} \\nabla L(W) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial w_1} \\\\\n",
    "\\frac{\\partial L}{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial w_n}\n",
    "\\end{bmatrix} \\end{array}$\n",
    "\n",
    "\n",
    "æŠŠ $\\Delta W = -w_i e_i$ä»£è¿›å»ï¼š\n",
    "- ä¸€é˜¶é¡¹ï¼š\n",
    "$$\\nabla L(W)^\\top \\Delta W\n",
    "= \\nabla L(W)^\\top (-w_i e_i)\n",
    "= -w_i \\frac{\\partial L}{\\partial w_i}$$\n",
    "\n",
    "- äºŒé˜¶é¡¹\n",
    "$$\\Delta W^\\top H \\Delta W\n",
    "= (-w_i e_i)^\\top H (-w_i e_i)\n",
    "= w_i^2 \\, e_i^\\top H e_i$$\n",
    "ä½†ï¼š\n",
    "$$e_i^\\top H e_i = H_{ii}$$\n",
    "æ‰€ä»¥ï¼š\n",
    "$$\\Delta W^\\top H \\Delta W = w_i^2 H_{ii}$$\n",
    "äºæ˜¯ï¼š\n",
    "$$\\Delta L \\approx -w_i \\frac{\\partial L}{\\partial w_i} + \\frac{1}{2} w_i^2 H_{ii}$$\n",
    "\n",
    "**å†æ¬¡ä½¿ç”¨â€œåœ¨æ”¶æ•›ç‚¹æ¢¯åº¦ â‰ˆ 0â€**, åœ¨è®­ç»ƒå¥½çš„å‚æ•°$W^\\star$é™„è¿‘ï¼š\n",
    "$$\\frac{\\partial L}{\\partial w_i}(W^\\star) \\approx 0$$\n",
    "äºæ˜¯ï¼š\n",
    "$$\\Delta L \\approx \\frac{1}{2} H_{ii} w_i^2$$\n",
    "è¿™å°±æ˜¯**OBDï¼ˆOptimal Brain Damageï¼‰ä¸­çš„ saliency**: $\\text{saliency}_i = \\frac{1}{2} H_{ii} w_i^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b402c",
   "metadata": {},
   "source": [
    "#### 2.1.2.2 pytorch CrossEntropy ä¸‹çš„ Hessian å¯¹è§’çº¿ vs E[gÂ²]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996091e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained loss: 0.33923059701919556\n",
      "Trained theta: [ 1.3182789 -1.9975102  0.4114626]\n",
      "\n",
      "True Hessian diagonal H_ii:\n",
      "tensor([0.0965, 0.0759, 0.1324])\n",
      "\n",
      "Estimated Hessian diagonal via E[g^2]:\n",
      "tensor([0.0015, 0.0012, 0.0014])\n",
      "\n",
      "Absolute diff |H_ii - E[g_i^2]|:\n",
      "tensor([0.0949, 0.0747, 0.1310])\n",
      "\n",
      "Relative diff |H_ii - E[g_i^2]| / |H_ii|:\n",
      "tensor([0.9839, 0.9840, 0.9891])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import hessian\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ===========================\n",
    "# 1. æ„é€ ä¸€ä¸ªå°çš„ logistic å›å½’æ¨¡å‹\n",
    "#    y âˆˆ {0,1}, p = sigmoid(xW + b)\n",
    "# ===========================\n",
    "\n",
    "in_dim = 2\n",
    "\n",
    "N = 500\n",
    "X = torch.randn(N, in_dim)\n",
    "\n",
    "# çœŸå®å‚æ•°ï¼ˆåªç”¨äºç”Ÿæˆæ•°æ®ï¼‰\n",
    "true_theta = torch.tensor([2.0, -3.0, 0.5])  # [w1, w2, b]\n",
    "true_W = true_theta[:in_dim].view(in_dim, 1) # [2,1]\n",
    "true_b = true_theta[in_dim:].view(1)         # [1]\n",
    "\n",
    "logits = X @ true_W + true_b            # [N,1]\n",
    "probs = torch.sigmoid(logits)           # [N,1]\n",
    "y = torch.bernoulli(probs).view(-1).long()  # è½¬æˆ int labels [N]\n",
    "\n",
    "# æˆ‘ä»¬åŒæ ·ç”¨ä¸€ç»´å‚æ•°å‘é‡ theta = [w1, w2, b] æ¥ä¼˜åŒ–\n",
    "theta = torch.randn(in_dim + 1, requires_grad=True)\n",
    "\n",
    "def unpack_theta(th):\n",
    "    W = th[:in_dim].view(in_dim, 1)   # [2,1]\n",
    "    b = th[in_dim:].view(1)           # [1]\n",
    "    return W, b\n",
    "\n",
    "def loss_fn(theta, X, y):\n",
    "    W, b = unpack_theta(theta)        # W: [2,1], b: [1]\n",
    "    logits = X @ W + b                # [N,1]\n",
    "    logits = logits.view(-1, 1)\n",
    "    # Binary cross entropy with logits\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, y.float().view(-1, 1))\n",
    "    return loss\n",
    "\n",
    "# ç®€å•è®­ç»ƒä¸€ä¸‹ï¼Œè®© theta æ¥è¿‘â€œæ”¶æ•›ç‚¹â€\n",
    "optim = torch.optim.SGD([theta], lr=0.1)\n",
    "\n",
    "for step in range(300):\n",
    "    optim.zero_grad()\n",
    "    loss = loss_fn(theta, X, y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "print(\"Trained loss:\", loss_fn(theta, X, y).item())\n",
    "print(\"Trained theta:\", theta.detach().numpy())\n",
    "\n",
    "# ==========================================\n",
    "# 2. è®¡ç®—â€œçœŸÂ·Hessian å¯¹è§’çº¿â€ï¼šH_ii\n",
    "# ==========================================\n",
    "\n",
    "def loss_only_theta(th):\n",
    "    return loss_fn(th, X, y)\n",
    "\n",
    "H = hessian(loss_only_theta, theta)   # [P,P], P = in_dim+1=3\n",
    "H_diag_true = torch.diag(H).detach()\n",
    "\n",
    "print(\"\\nTrue Hessian diagonal H_ii:\")\n",
    "print(H_diag_true)\n",
    "\n",
    "# ==========================================\n",
    "# 3. ç”¨å¤š batch çš„æ¢¯åº¦å¹³æ–¹å¹³å‡ E[g^2] è¿‘ä¼¼ H_ii\n",
    "# ==========================================\n",
    "\n",
    "num_batches = 100\n",
    "batch_size = 64\n",
    "\n",
    "g2_accum = torch.zeros_like(theta)\n",
    "\n",
    "for _ in range(num_batches):\n",
    "    idx = torch.randint(0, N, (batch_size,))\n",
    "    Xb = X[idx]\n",
    "    yb = y[idx]\n",
    "\n",
    "    loss_b = loss_fn(theta, Xb, yb)\n",
    "    grad_theta = torch.autograd.grad(loss_b, theta, retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "    g2_accum += grad_theta.detach() ** 2\n",
    "\n",
    "H_diag_est = g2_accum / num_batches\n",
    "\n",
    "print(\"\\nEstimated Hessian diagonal via E[g^2]:\")\n",
    "print(H_diag_est)\n",
    "\n",
    "# ==========================================\n",
    "# 4. å¯¹æ¯”ä¸¤è€…\n",
    "# ==========================================\n",
    "\n",
    "abs_diff = (H_diag_true - H_diag_est).abs()\n",
    "rel_diff = abs_diff / (H_diag_true.abs() + 1e-8)\n",
    "\n",
    "print(\"\\nAbsolute diff |H_ii - E[g_i^2]|:\")\n",
    "print(abs_diff)\n",
    "\n",
    "print(\"\\nRelative diff |H_ii - E[g_i^2]| / |H_ii|:\")\n",
    "print(rel_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492a2d7",
   "metadata": {},
   "source": [
    "### 2.1.3 è¡¥ï¼šFisher ä¿¡æ¯çŸ©é˜µ\n",
    "Fisher ä¿¡æ¯çŸ©é˜µï¼ˆFisher Information Matrix, FIMï¼‰ è¡¡é‡çš„æ˜¯ï¼š\n",
    "\n",
    "æ¨¡å‹å¯¹å‚æ•°å˜åŒ–çš„æ•æ„Ÿåº¦\n",
    "\n",
    "æˆ–\n",
    "\n",
    "ç”¨å‚æ•°å»è§£é‡Šæ•°æ®æ—¶ï¼Œä¸ç¡®å®šæ€§æœ‰å¤šå°\n",
    "\n",
    "â€œä¿¡æ¯è¶Šå¤§ â†’ å‚æ•°è¶Šé‡è¦ï¼Œä¸èƒ½åŠ¨â€\n",
    "â€œä¿¡æ¯è¶Šå° â†’ å‚æ•°ä¸é‡è¦ï¼Œå¯ä»¥å‰ª / é‡åŒ–å¾—æ›´æ¿€è¿›â€\n",
    "\n",
    "---\n",
    "1. Fisher ä¿¡æ¯çŸ©é˜µçš„æ­£å¼å®šä¹‰\n",
    "ç»™å®šæ¨¡å‹çš„å¯¹æ•°ä¼¼ç„¶ï¼ˆlog-likelihoodï¼‰ï¼š$\\log p_\\theta(x)$\n",
    "\n",
    "Fisher ä¿¡æ¯çŸ©é˜µå®šä¹‰ä¸ºï¼š\n",
    "$$F(\\theta) = \\mathbb{E}\\left[ \\left( \\nabla_\\theta \\log p_\\theta(x) \\right)\n",
    "       \\left( \\nabla_\\theta \\log p_\\theta(x) \\right)^\\top \\right]$$\n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "\n",
    "- å…ˆå¯¹ log-likelihood æ±‚æ¢¯åº¦ï¼ˆscore functionï¼‰\n",
    "\n",
    "- å†æ±‚å®ƒä¸è‡ªèº«çš„å¤–ç§¯\n",
    "\n",
    "- å†åœ¨æ•°æ®åˆ†å¸ƒä¸Šå–æœŸæœ›\n",
    "\n",
    "---\n",
    "2. æ·±åº¦å­¦ä¹ é‡Œæ›´å¸¸ç”¨çš„è¡¨è¾¾\n",
    "å½“æˆ‘ä»¬ç”¨ CrossEntropy Loss æ—¶ï¼š\n",
    "$$L(\\theta) = -\\log p_\\theta(y|x)$$\n",
    "æ¢¯åº¦å°±æ˜¯ï¼š\n",
    "$$g = \\nabla_\\theta L = - \\nabla_\\theta \\log p_\\theta(y|x)$$\n",
    "äºæ˜¯ Fisher ä¿¡æ¯çŸ©é˜µç­‰ä»·äºï¼š\n",
    "$$F(\\theta) = \\mathbb{E}[g\\, g^\\top]$$\n",
    "\n",
    "å¯¹è§’çº¿å°±æ˜¯ï¼š\n",
    "$$F_{ii} = \\mathbb{E}[g_i^2]$$\n",
    "ä¹Ÿå°±æ˜¯\n",
    "$$H_{ii} \\approx \\mathbb{E}[g_i^2]$$\n",
    "\n",
    "---\n",
    "3. ä¸ºä»€ä¹ˆ Fisher ä¿¡æ¯çŸ©é˜µåœ¨æ·±åº¦å­¦ä¹ ä¸­å¾ˆé‡è¦ï¼Ÿ\n",
    "Fisher ä¿¡æ¯å‘Šè¯‰æˆ‘ä»¬ï¼š\n",
    "\n",
    "å¦‚æœå‚æ•° Î¸ å‘ç”Ÿå¾®å°å˜åŒ–ï¼Œæ¨¡å‹é¢„æµ‹ä¼šæœ‰å¤šå¤§æ³¢åŠ¨ï¼Ÿ\n",
    "\n",
    "æ¢å¥è¯è¯´ï¼š\n",
    "- $F_{ii}$ å¤§ â†’ å‚æ•°éå¸¸â€œæ•æ„Ÿâ€ â†’ å¾ˆé‡è¦ â†’ ä¸å®œå‰ª\n",
    "- $F_{ii}$å° â†’ å‚æ•°åŸºæœ¬ä¸èµ·ä½œç”¨ â†’ å¯ä»¥å‰ª / å¯ä»¥ä½ bit é‡åŒ– \n",
    "\n",
    "æ‰€ä»¥ï¼š\n",
    "\n",
    "âœ” Fisher ä¿¡æ¯å¯ç”¨äºå‰ªæï¼ˆpruning saliencyï¼‰\n",
    "\n",
    "âœ” å¯ç”¨äºé‡åŒ–ï¼ˆquantization sensitivityï¼‰\n",
    "\n",
    "âœ” å¯ç”¨äºç¡®å®šå“ªäº›å±‚çš„ rank è¦ä¸‹è°ƒï¼ˆlow-rank SVDï¼‰\n",
    "\n",
    "âœ” å¯ç”¨äº LayerNorm / Whitening çš„ç¨³å®šæ€§åˆ†æ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349799b",
   "metadata": {},
   "source": [
    "### 2.1.4 ç»“æ„åŒ–å‰ªæï¼ˆchannel/head pruningï¼‰\n",
    "ç»“æ„åŒ–å‰ªæå…³æ³¨â€œæ•´ä¸ªé€šé“/æ•´ä¸ª head/æ•´ä¸ª filter æ˜¯å¦é‡è¦â€ï¼Œè€Œä¸æ˜¯å•ä¸ª weightã€‚\n",
    "å¯ä»¥æŠŠé€šé“å‰ªæå†™æˆï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\theta, m} \\ L(\\theta \\odot m) + \\lambda \\|m\\|_0\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\theta$ï¼šåŸå§‹å‚æ•°\n",
    "- $m$ï¼šmaskï¼ŒæŒ‰é€šé“/å—ä¸ºå•ä½å– 0 æˆ– 1\n",
    "- $\\odot$ï¼šé€å…ƒç´ æˆ–é€é€šé“ä¹˜æ³•\n",
    "\n",
    "é€šå¸¸ä¼šï¼š\n",
    "\n",
    "1. è®­ç»ƒä¸€ä¸ªå¸¦æœ‰å¯å¾®è¿‘ä¼¼ mask çš„æ¨¡å‹ï¼ˆå¦‚ç”¨ sigmoid/Concrete distributionï¼‰\n",
    "2. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å‹ç¼©æŸäº›é€šé“çš„æƒé‡\n",
    "3. æœ€åå°†æ¥è¿‘ 0 çš„é€šé“ç¡¬å‰ªæ‰\n",
    "\n",
    "> æœ¬è´¨ä»ç„¶æ˜¯â€œå¸¦ç¨€ç–çº¦æŸçš„ä¼˜åŒ–é—®é¢˜â€ï¼Œåªæ˜¯ä½œç”¨å¯¹è±¡ä»å•ä¸ªå…ƒç´ æå‡åˆ°äº†â€œç»“æ„åŒ–å—â€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 2.1.5 å„ç§å‰ªææ–¹æ³•å¯¹æ¯”\n",
    "| æ–¹æ³•                           | ä½¿ç”¨çš„æŒ‡æ ‡                         | æŒ‡æ ‡æ„ä¹‰                     | æ˜¯å¦è¡¡é‡é‡è¦æ€§ï¼Ÿ        | æ˜¯å¦ç§‘å­¦ï¼Ÿ |\n",
    "| ------------------------------ | ----------------------------------- | ----------------------------- | ------------------------- | ---------- |\n",
    "| **L1 å‰ªæ**                    | $\\lvert w_i \\rvert$                | æ˜¯å¦æ¥è¿‘ 0ï¼ˆè‡ªç„¶ç¨€ç–ï¼‰         | âŒ å¦                     | ä½         |\n",
    "| **Hessian å‰ªæï¼ˆOBD/OBSï¼‰**    | $H_{ii} \\, w_i^2$                  | åˆ æ‰è¯¥æƒé‡é€ æˆçš„ loss å¢åŠ é‡  | âœ” æ˜¯ï¼ˆæ•°å­¦ä¸¥æ ¼ï¼‰         | é«˜         |\n",
    "| **Fisher å‰ªæ**                | \\mathbb{E}[g_i^2] \\, w_i^2$        | ç”¨ Fisher è¿‘ä¼¼ Hessian       | âœ” æ˜¯                     | é«˜         |\n",
    "| **Magnitude å‰ªæï¼ˆstructuredï¼‰** | é€šé“ $\\ell_1$ / $\\ell_2$ èŒƒæ•°       | é€šé“èƒ½é‡å¤§å°                  | éƒ¨åˆ†                      | ä¸­         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b611d7",
   "metadata": {},
   "source": [
    "## 2.2 é‡åŒ–ï¼ˆQuantizationï¼‰çš„ä¼˜åŒ–è§†è§’\n",
    "\n",
    "é‡åŒ–æœ¬è´¨ä¸æ˜¯ç®€å•åœ°â€œæŠŠæµ®ç‚¹å˜æˆ int8/int4â€ï¼Œ\n",
    "è€Œæ˜¯ï¼š\n",
    "\n",
    "**åœ¨ bit-budgetï¼ˆä½å®½é¢„ç®—ï¼‰å—é™çš„çº¦æŸä¸‹ï¼Œæ±‚ä¸€ä¸ªé€¼è¿‘åŸæ¨¡å‹çš„æœ€ä¼˜ç¦»æ•£è¡¨ç¤ºï¼Œä½¿å¾— inference çš„è¯¯å·®æœ€å°ã€‚**\n",
    "\n",
    "æ•°å­¦å½¢å¼ä¸Šå°±æ˜¯ï¼š\n",
    "$$\\min_{s,z, Q(\\cdot)} \\; L\\big(f_{Q,s,z}(x), f(x)\\big)$$\n",
    "å…¶ä¸­ï¼š\n",
    "- $Q(\\cdot)$é‡åŒ–æ˜ å°„ï¼ˆround, clipï¼‰\n",
    "- $s$: scale ï¼ˆæ­¥é•¿ï¼‰\n",
    "- $z$: zero pointï¼ˆéå¯¹ç§°é‡åŒ–çš„åç½®ï¼‰\n",
    "- $f_{Q,s,z}$: é‡åŒ–åçš„æ¨¡å‹\n",
    "- $f$: æ¨¡å‹\n",
    "\n",
    "---\n",
    "\n",
    "#1 é‡åŒ–çš„ä¸‰ä¸ªæ ¸å¿ƒä¼˜åŒ–ç›®æ ‡\n",
    "ğŸ“Œ ç›®æ ‡ 1ï¼šæœ€å°åŒ–é‡åŒ–è¯¯å·®\n",
    "æœ€å¸¸è§ï¼š\n",
    "$$\\min_{s,z} \\sum_i (w_i - \\hat{w}_i)^2$$\n",
    "å…¶ä¸­ï¼š\n",
    "$$\\hat{w}_i = s (Q(w_i/s) - z)$$\n",
    "è¿™æ˜¯ æœ€å°äºŒä¹˜æ„ä¹‰ä¸‹ çš„æœ€ä¼˜ scale / zero pointã€‚\n",
    "\n",
    "\n",
    "ğŸ“Œ ç›®æ ‡ 2ï¼šæ»¡è¶³ bit-budgetï¼ˆçº¦æŸä¼˜åŒ–ï¼‰\n",
    "$q_i \\in \\{-Q, ..., Q\\}, \\quad Q = 2^{b-1} - 1$\n",
    "bit è¶Šå°‘ â†’ é‡åŒ–å™ªå£°è¶Šå¤§ â†’ æ¨ç†è¯¯å·®è¶Šé«˜\n",
    "æœ¬è´¨æ˜¯ï¼š**åœ¨ bit=4/8 çš„çº¦æŸä¸‹ï¼ŒæŠŠè¯¯å·®å‹åˆ°æœ€ä½ã€‚**\n",
    "\n",
    "ğŸ“Œ ç›®æ ‡ 3ï¼šç»´æŒæ¨ç†è§£ç ç¨³å®šæ€§ï¼ˆæ¢¯åº¦æ— å…³ï¼‰\n",
    "- runtime inference ä¸­æœ€é‡è¦ï¼š\n",
    "\n",
    "- ä¸çˆ†å€¼\n",
    "\n",
    "- ä¸æº¢å‡º\n",
    "\n",
    "- ä¸ç§¯ç´¯æ•°å€¼æ¼‚ç§»\n",
    "\n",
    "- softmax ä¸å´©\n",
    "\n",
    "- LayerNorm ä¿æŒç¨³å®š\n",
    "æ•°å­¦ä¸Šï¼Œè¿™è¦æ±‚ï¼š\n",
    "\n",
    "$$\\|f_{Q}(x) - f(x)\\| \\text{ å°ä¸”å¯æ§}$$\n",
    "ç‰¹åˆ«æ˜¯åœ¨ Transformer ä¸­ï¼Œé‡åŒ–è¯¯å·®ä¼šåœ¨æ³¨æ„åŠ›è·¯å¾„ç´¯è®¡ï¼Œéœ€è¦é’ˆå¯¹ä¸åŒå±‚è®¾è®¡ä¸åŒä¼˜åŒ–ç­–ç•¥\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f318349",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ada21244",
   "metadata": {},
   "source": [
    "### 2.2.1 ç®€å•çš„ uniform quantization - ä¼˜åŒ–scale\n",
    "\n",
    "è€ƒè™‘å¯¹æƒé‡é›†åˆ $\\{w_i\\}$ åšå¯¹ç§°å‡åŒ€é‡åŒ–ï¼Œbit å®½ä¸º bï¼š\n",
    "\n",
    "- é‡åŒ–çº§åˆ«ï¼š$q_i \\in \\{-Q,\\dots,Q\\}$ï¼Œå…¶ä¸­ $Q = 2^{b-1}-1$\n",
    "- ç¼©æ”¾å› å­ï¼ˆscaleï¼‰ï¼š$s > 0$\n",
    "- é‡åŒ–/åé‡åŒ–è¿‡ç¨‹ï¼š\n",
    "\n",
    "$$\n",
    "q_i = \\text{round}(w_i / s), \\quad\n",
    "  \\hat{w}_i = s \\cdot q_i\n",
    "$$\n",
    "\n",
    "\n",
    "å…¸å‹ç›®æ ‡æ˜¯æœ€å°åŒ–é‡åŒ–è¯¯å·®ï¼š\n",
    "\n",
    "$$\n",
    "\\min_s \\sum_i (w_i - \\hat{w}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "è¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªä¸€ç»´å‡¸ä¼˜åŒ–é—®é¢˜ã€‚å¯ä»¥è¯æ˜ï¼š\n",
    "\n",
    "- å¯¹ç§°é‡åŒ–ï¼ˆzero-point = 0ï¼‰\n",
    "- é‡åŒ–åŒºé—´å›ºå®šåœ¨$[-Q, Q]$\n",
    "åˆ™æœ€ä¼˜ scale è§£ä¸ºï¼š$$s^* = \\frac{\\max |w_i|}{Q}$$\n",
    "\n",
    "å·¥ç¨‹ä¸Šå¸¸è§ heuristicsï¼š\n",
    "\n",
    "- ç›´æ¥å–ï¼š\n",
    "\n",
    "$$\n",
    "s = \\frac{\\max_i |w_i|}{Q}\n",
    "$$\n",
    "\n",
    "\n",
    "- æˆ–å–æŸä¸ªåˆ†ä½æ•°ï¼ˆå¦‚ 99.9%ï¼‰æ›¿ä»£ maxï¼Œé˜²æ­¢ outlier è¿‡å¤§ï¼š\n",
    "\n",
    "$$\n",
    "s = \\frac{\\text{quantile}_{p}(|w_i|)}{Q}\n",
    "$$\n",
    "\n",
    "\n",
    "**ã€è¦ç‚¹ã€‘**  \n",
    "ä½ éœ€è¦çŸ¥é“ï¼š\n",
    "\n",
    "- é‡åŒ–å‚æ•°ï¼ˆscale/zero-pointï¼‰å¯ä»¥é€šè¿‡æœ€å°äºŒä¹˜æ„ä¹‰ä¸‹çš„ä¼˜åŒ–æ±‚å¾—\n",
    "- å·¥ç¨‹å®ç°ä¸­ç”¨ histogram + æœç´¢ / heuristics åšè¿‘ä¼¼æ±‚è§£ï¼š åŸå› çœŸå®æƒé‡/æ¿€æ´» ä¸æ˜¯å‡åŒ€åˆ†å¸ƒï¼Œå¾ˆå¤š heavy-tailï¼ˆé•¿å°¾ï¼‰çš„ã€‚æ‰€ä»¥æ›´å¥½çš„ä¼˜åŒ–æ–¹æ³•æ˜¯**ä¼˜åŒ–åˆ†å¸ƒé€¼è¿‘ï¼šKL æœ€å°åŒ–**\n",
    "$$s^* = \\arg\\min_s \\text{KL}(P(w) \\| P(\\hat{w}))$$\n",
    "æ‰€ä»¥å¾ˆå¤šå·¥å…·ï¼ˆTensorRTã€TF-Liteã€MKLDNN etcï¼‰éƒ½ç”¨ï¼š\n",
    "\n",
    " - histogram åˆ†å¸ƒ\n",
    "\n",
    " - quantization reconstruction\n",
    "\n",
    " - KL divergence\n",
    "\n",
    "æ¥é€‰æœ€ä¼˜ clip rangeã€‚\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2.2 LSQï¼ˆLearned Step Size Quantizationï¼‰æ€æƒ³\n",
    "\n",
    "æ›´è¿›ä¸€æ­¥ï¼Œå¯ä»¥æŠŠ scale $s$ å½“æˆå¯å­¦ä¹ å‚æ•°ï¼Œè®©åå‘ä¼ æ’­ç›´æ¥ä¼˜åŒ–ï¼š\n",
    "\n",
    "- å®šä¹‰ä¸€ä¸ªâ€œä¼ªé‡åŒ–â€ç®—å­ï¼ˆç”¨ STE Straight-Through Estimator é€¼è¿‘æ¢¯åº¦ï¼‰\n",
    "- åœ¨è®­ç»ƒ/å¾®è°ƒæ—¶ joint optimize $W$ å’Œ $s$\n",
    "æŠŠ quantizer å†™æˆï¼ˆå¸¦ straight-through æ¢¯åº¦ï¼‰ï¼š\n",
    "$$\\hat{w} = \\mathrm{clip}\\Big(\\mathrm{round}(\\frac{w}{s})\\Big) \\cdot s$$\n",
    "ç„¶åæŠŠ s å½“æˆæ¨¡å‹å‚æ•°ä¸€èµ·è®­ç»ƒï¼š\n",
    "$$\\min_{s,W} L(\\hat{W}(s), y)$$\n",
    "è¿™å°±ä»æ•°å­¦ä¸Šå˜æˆï¼š\n",
    "\n",
    "- è”åˆä¼˜åŒ–ï¼ˆjoint optimizationï¼‰\n",
    "\n",
    "- å¸¦ä¸å¯å¯¼ç®—å­çš„è¿‘ä¼¼æ¢¯åº¦ï¼ˆSTEï¼‰\n",
    "\n",
    "- å‚æ•°åŒ–çš„çº¦æŸä¼˜åŒ–\n",
    "\n",
    "è¿™ç§æ–¹æ³•ä½¿å¾—é‡åŒ–å‚æ•°é’ˆå¯¹å½“å‰ä»»åŠ¡/æ•°æ®é›†è‡ªé€‚åº”åœ°æ”¶æ•›åˆ°è¾ƒå¥½çš„å€¼ï¼Œä»è€Œæå‡ä½ bit é‡åŒ–çš„ç²¾åº¦ã€‚\n",
    "\n",
    "#### 2.2.2.1 STE Straight-Through Estimator\n",
    "å½“é‡åŒ–ã€å–æ•´ã€ç¬¦å·å‡½æ•°ç­‰ ä¸å¯å¯¼ï¼ˆæ¢¯åº¦ä¸º 0 æˆ–ä¸å­˜åœ¨ï¼‰æ—¶ï¼Œ\n",
    "æ— æ³•ç›´æ¥åšåå‘ä¼ æ’­ã€‚\n",
    "\n",
    "STE çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**å‰å‘ä½¿ç”¨çœŸå®çš„éå¯å¯¼ç®—å­ï¼ˆå¦‚ roundã€signï¼‰ï¼Œåå‘æ—¶â€œå‡è£…å®ƒæ˜¯æ’ç­‰æ˜ å°„â€ï¼ŒæŠŠæ¢¯åº¦ç›´æ¥ä¼ è¿‡å»ã€‚**\n",
    "æ•°å­¦å†™æ³•ï¼š\n",
    "å‰å‘ï¼š$$\\hat{w} = \\text{round}(w)$$\n",
    "åå‘ï¼š$$\\frac{\\partial \\hat{w}}{\\partial w} \\approx 1$$\n",
    "æˆ–è€…æ ¹æ®ä¸åŒå˜ä½“ï¼š\n",
    "$$\\begin{array}{c} \\frac{\\partial \\hat{w}}{\\partial w} = \n",
    "\\begin{cases}\n",
    "1, & |w|<1 \\\\\n",
    "0, & \\text{else}\n",
    "\\end{cases} \\end{array}$$\n",
    "\n",
    "---\n",
    "ğŸ”· ä¸ºä»€ä¹ˆå« \"Straight-Through\"ï¼Ÿ\n",
    "å› ä¸ºåœ¨åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦ä¸è¢«é˜»æ–­ï¼Œè€Œæ˜¯â€œç›´æ¥ç©¿è¿‡ï¼ˆgo straight throughï¼‰â€ä¸å¯å¯¼ç®—å­ã€‚\n",
    "```block\n",
    "   w ---->[ round ]----> \\hat{w}\n",
    "           (non-diff)\n",
    "\n",
    "   backward: gradient just passes straight through\n",
    "\n",
    "```\n",
    "- å‰å‘ç”¨ç¦»æ•£å€¼, $\\hat{w} = s \\cdot \\text{round}(w/s)$\n",
    "- åå‘æŠŠ round å½“æˆ identity. $\\frac{\\partial \\hat{w}}{\\partial w} \\approx 1$\n",
    "å› æ­¤é‡åŒ–æ¨¡å‹å¯ä»¥ç»§ç»­è®­ç»ƒï¼ˆQATï¼‰ã€å­¦ä¹ æœ€ä¼˜ scaleï¼ˆLSQï¼‰ã€‚\n",
    "\n",
    "ğŸ”· STE ç‰¹åˆ«é€‚ç”¨äºï¼š\n",
    "\n",
    "- QATï¼ˆQuantization-Aware Trainingï¼‰\n",
    "\n",
    "- LSQï¼ˆLearned Step Size Quantizationï¼‰\n",
    "\n",
    "- äºŒå€¼åŒ–ç½‘ç»œï¼ˆBNNï¼‰\n",
    "\n",
    "- hard-sigmoid/ç¡¬é—¨æ§çš„æ¢¯åº¦è¿‘ä¼¼\n",
    "\n",
    "- ä»»ä½•æ¶‰åŠ â€œä¸å¯å¯¼å‡½æ•° + æƒ³è®­ç»ƒå®ƒâ€ çš„åœºæ™¯\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fb8de",
   "metadata": {},
   "source": [
    "## 2.3 çŸ¥è¯†è’¸é¦ï¼ˆDistillationï¼‰çš„ä¼˜åŒ–è§†è§’\n",
    "\n",
    "### 2.3.1 æ ¸å¿ƒæ€æƒ³ï¼šè’¸é¦æ˜¯ä¸€ä¸ªè”åˆæœ€ä¼˜åŒ–é—®é¢˜\n",
    "\n",
    "Teacherï¼ˆå¤§æ¨¡å‹ï¼‰å‘ Studentï¼ˆå°æ¨¡å‹ï¼‰ä¼ é€’è½¯çŸ¥è¯†ï¼ˆsoft knowledgeï¼‰ã€‚\n",
    "\n",
    "- Teacher logitsï¼š  \n",
    "  $$ z^T $$\n",
    "\n",
    "- Student logitsï¼š  \n",
    "  $$ z^S $$\n",
    "\n",
    "åŠ å…¥æ¸©åº¦ $ T $ çš„ softmaxï¼š\n",
    "\n",
    "$$\n",
    "p^T_i = \\frac{e^{z_i^T / T}}{\\sum_j e^{z_j^T / T}}, \n",
    "\\qquad\n",
    "p^S_i = \\frac{e^{z_i^S / T}}{\\sum_j e^{z_j^S / T}}\n",
    "$$\n",
    "\n",
    "Student çš„ç›®æ ‡æ˜¯æœ€å°åŒ–ä»¥ä¸‹è”åˆæŸå¤±ï¼š\n",
    "\n",
    "$$\n",
    "L = \\alpha L_{\\text{CE}} + (1 - \\alpha)L_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.2 Hard Lossï¼ˆäº¤å‰ç†µ CEï¼‰\n",
    "\n",
    "å­¦ç”Ÿå¯¹çœŸå®æ ‡ç­¾çš„äº¤å‰ç†µæŸå¤±ï¼š\n",
    "\n",
    "$$\n",
    "L_{\\text{CE}} = -\\sum_i y_i \\log p^S_i\n",
    "$$\n",
    "\n",
    "- $y$ï¼šone-hot æˆ– label smoothing å¤„ç†åçš„æ ‡ç­¾  \n",
    "- è¿™ä¸€é¡¹ä¿è¯ Student è‡³å°‘èƒ½å¯¹â€œæ ‡å‡†ç­”æ¡ˆâ€åšå¯¹åˆ†ç±»\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.3 Soft Lossï¼ˆKL è’¸é¦æŸå¤±ï¼‰\n",
    "\n",
    "å­¦ç”Ÿæ¨¡ä»¿ Teacher çš„è¾“å‡ºåˆ†å¸ƒï¼š\n",
    "\n",
    "$$\n",
    "L_{\\text{KL}} = T^2 \\sum_i p^T_i \\log \\frac{p^T_i}{p^S_i}\n",
    "$$\n",
    "\n",
    "- ä½¿ç”¨ $ T^2 $ æ¥æŠµæ¶ˆæ¸©åº¦å¯¹æ¢¯åº¦çš„ç¼©æ”¾  \n",
    "- Teacher çš„ soft labels æä¾›â€œæš—çŸ¥è¯†â€ï¼ˆdark knowledgeï¼‰  \n",
    "- ä¸ä»…å‘Šè¯‰ Student â€œè°æ˜¯å¯¹çš„â€ï¼Œè¿˜å‘Šè¯‰å®ƒ â€œè°è·Ÿè°æ›´åƒâ€\n",
    "\n",
    "è”åˆæŸå¤±ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "L = \\alpha L_{\\text{CE}} + (1-\\alpha)L_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $ \\alpha $ï¼šæ§åˆ¶å¯¹çœŸå®æ ‡ç­¾ï¼ˆhard labelï¼‰å’Œ Teacherï¼ˆsoft labelï¼‰çš„ä¿¡ä»»åº¦  \n",
    "- å¤§æ¨¡å‹è¶Šå¼ºï¼Œä¸€èˆ¬å¯ä»¥è®© $ \\alpha $ ç¨å¾®å°ä¸€ç‚¹ï¼ˆæ›´ä¾èµ– Teacherï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.4 KL è’¸é¦æ¢¯åº¦çš„ç›´è§‚æ¨å¯¼\n",
    "\n",
    "KL éƒ¨åˆ†å¯¹ Student logits çš„æ¢¯åº¦ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{KL}}}{\\partial z_k^S} \n",
    "= p_k^S - p_k^T\n",
    "$$\n",
    "\n",
    "ç›´è§‚è§£é‡Šï¼š\n",
    "\n",
    "- å­¦ç”Ÿè¾“å‡ºæ¯” Teacher å¤§ï¼ˆ$ p_k^S > p_k^T $ï¼‰ â†’ æ¢¯åº¦ä¸ºæ­£ â†’ ä¸‹é™æ—¶å¾€ä¸‹æ‹‰  \n",
    "- å­¦ç”Ÿè¾“å‡ºæ¯” Teacher å°ï¼ˆ$ p_k^S < p_k^T $ï¼‰ â†’ æ¢¯åº¦ä¸ºè´Ÿ â†’ ä¸‹é™æ—¶å¾€ä¸Šæ¨  \n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "\n",
    "> Student è°ƒæ•´è‡ªå·±çš„è¾“å‡ºæ¦‚ç‡ï¼Œä½¿å…¶é€æ­¥é€¼è¿‘ Teacher çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "\n",
    "ä¸äº¤å‰ç†µ CE çš„æ¢¯åº¦å¯¹æ¯”ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{CE}}}{\\partial z_k^S} = p_k^S - y_k\n",
    "$$\n",
    "\n",
    "- å¯¹ CE æ¥è¯´ï¼Œtarget æ˜¯ one-hot çš„ $y_k$\n",
    "- å¯¹ KL è’¸é¦æ¥è¯´ï¼Œtarget æ˜¯ soft çš„ $p_k^T$\n",
    "\n",
    "å› æ­¤ï¼š\n",
    "\n",
    "> è’¸é¦ = ç”¨ Teacher çš„è½¯åˆ†å¸ƒ $p^T$ æ›¿ä»£ç¡¬åˆ†å¸ƒ $y$ æ¥â€œç›‘ç£â€ Studentã€‚\n",
    "\n",
    "---\n",
    "#### 2.3.4.x KLæ¢¯åº¦æ¨å¯¼\n",
    "\n",
    "Student æ¦‚ç‡ï¼ˆsoftmaxï¼‰ï¼š \n",
    "$$p^S_i = \\frac{e^{z^S_i}}{\\sum_j e^{z^S_j}}$$\n",
    "\n",
    "KL è’¸é¦æŸå¤±ï¼ˆå¿½ç•¥å‰é¢çš„ $T^2$ï¼‰ï¼š\n",
    "$$L_{\\mathrm{KL}} = \\sum_i p^T_i \\log \\frac{p^T_i}{p^S_i}\n",
    "               = \\sum_i p^T_i \\big(\\log p^T_i - \\log p^S_i\\big)$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $\\log p^T_i$éƒ¨åˆ†ä¸ Student æ— å…³ï¼ˆå¸¸æ•°ï¼‰\n",
    "- çœŸæ­£å’Œ student æœ‰å…³çš„æ˜¯è¿™ä¸€é¡¹\n",
    "$$L' = - \\sum_i p^T_i \\log p^S_i$$\n",
    "è¿™å…¶å®å°±æ˜¯ä»¥ **$p^T$ä¸º label çš„äº¤å‰ç†µ**\n",
    "$$L' = H(p^T, p^S)$$\n",
    "\n",
    "æ‰€ä»¥æœ¬è´¨å°±æ˜¯ï¼š **KL çš„æ¢¯åº¦ = äº¤å‰ç†µ w.r.t. logits çš„æ¢¯åº¦ã€‚**\n",
    "\n",
    "å¯¹ logits æ±‚å¯¼ï¼š$\\partial L' / \\partial z_k^S$\n",
    "$$L' = - \\sum_i p^T_i \\log p^S_i$$\n",
    "- ç¬¬ä¸€æ­¥ï¼šå¯¹ $p^S_i$ æ±‚åå¯¼ï¼ˆé“¾å¼æ³•åˆ™ï¼‰\n",
    "\n",
    "$$\\frac{\\partial L'}{\\partial z_k^S} = \\sum_i \\frac{\\partial L'}{\\partial p^S_i} \\cdot \\frac{\\partial p^S_i}{\\partial z_k^S}$$\n",
    "å…ˆç®—ç¬¬ä¸€éƒ¨åˆ†ï¼š\n",
    "$$\\frac{\\partial L'}{\\partial p^S_i} = - p^T_i \\cdot \\frac{1}{p^S_i}$$\n",
    "\n",
    "ç¬¬äºŒæ­¥ï¼šsoftmax çš„å¯¼æ•°\n",
    "\n",
    "å¯¹ softmaxï¼š\n",
    "$$p^S_i = \\frac{e^{z^S_i}}{\\sum_j e^{z^S_j}}$$\n",
    "è€Œ\n",
    "$$\\begin{array}{c} \\frac{\\partial p^S_i}{\\partial z_k^S}\n",
    "=\\begin{cases}\n",
    "p^S_i (1 - p^S_i), & i = k \\\\\n",
    "- p^S_i p^S_k, & i \\ne k\n",
    "\\end{cases} \\end{array}$$\n",
    "\n",
    "\n",
    "- ç¬¬äºŒæ­¥ï¼šæŠŠä¸¤éƒ¨åˆ†ä¹˜èµ·æ¥å¹¶æ±‚å’Œ\n",
    "$$\\frac{\\partial L'}{\\partial z_k^S}\n",
    "= \\sum_i \\left(- \\frac{p^T_i}{p^S_i}\\right) \\cdot \\frac{\\partial p^S_i}{\\partial z_k^S}$$\n",
    "- i = kï¼š\n",
    "$$\\left(- \\frac{p^T_k}{p^S_k}\\right) \\cdot p^S_k(1 - p^S_k)\n",
    "= - p^T_k (1 - p^S_k)\n",
    "= -p^T_k + p^T_k p^S_k$$\n",
    "- i â‰  kï¼š\n",
    "$$\\sum_{i \\ne k} \\left(- \\frac{p^T_i}{p^S_i}\\right) \\cdot (-p^S_i p^S_k)\n",
    "= \\sum_{i \\ne k} p^T_i p^S_k\n",
    "= p^S_k \\sum_{i \\ne k} p^T_i$$\n",
    "\n",
    "==>\n",
    "$$\\frac{\\partial L'}{\\partial z_k^S}\n",
    "= \\big(-p^T_k + p^T_k p^S_k\\big) + p^S_k \\sum_{i \\ne k} p^T_i$$\n",
    "\n",
    "æ³¨æ„åˆ° $\\sum_{i \\ne k} p^T_i = 1 - p^T_k$\n",
    "\n",
    "æ‰€ä»¥ï¼š\n",
    "$$\\frac{\\partial L'}{\\partial z_k^S}\n",
    "= -p^T_k + p^T_k p^S_k + p^S_k (1 - p^T_k)\n",
    "= -p^T_k + p^T_k p^S_k + p^S_k - p^S_k p^T_k\n",
    "= p^S_k - p^T_k$$\n",
    "\n",
    "$$\\boxed{\n",
    "\\frac{\\partial L_{\\mathrm{KL}}}{\\partial z_k^S} \n",
    "= p^S_k - p^T_k\n",
    "}$$\n",
    "\n",
    "--- \n",
    "\n",
    "### 2.3.5 Temperature $ T $ çš„ä½œç”¨ï¼ˆæ•°å­¦è§£é‡Šï¼‰\n",
    "\n",
    "å¸¦æ¸©åº¦çš„ softmaxï¼š\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}}\n",
    "$$\n",
    "\n",
    "- $ T > 1 $ï¼šåˆ†å¸ƒæ›´å¹³æ»‘ï¼ˆå„ç±»æ¦‚ç‡æ›´å‡åŒ€ï¼Œç±»é—´å…³ç³»æ›´æ˜æ˜¾ï¼‰  \n",
    "- $ T < 1 $ï¼šåˆ†å¸ƒæ›´å°–é”ï¼ˆæ›´æ¥è¿‘ argmaxï¼‰  \n",
    "\n",
    "è’¸é¦ä¸€èˆ¬ä½¿ç”¨ $T = 2 ï½ 4$ ï¼ŒåŸå› æ˜¯ï¼š\n",
    "\n",
    "- é«˜æ¸©ä½¿ softmax è¾“å‡ºå¯¹ logits çš„å˜åŒ–æ›´åŠ â€œæ•æ„Ÿä¸”å¹³æ»‘â€  \n",
    "- KL-loss å¯¹æ‰€æœ‰ç±»åˆ«éƒ½äº§ç”Ÿæ¢¯åº¦ï¼Œè€Œä¸ä»…ä»…æ˜¯ top-1  \n",
    "- Student èƒ½â€œçœ‹è§â€ Teacher å¯¹æ‰€æœ‰ç±»åˆ«çš„ç›¸å¯¹åå¥½ï¼Œå­¦åˆ°æ›´ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.6 å®é™…å·¥ç¨‹ä¸­çš„è’¸é¦æµç¨‹ï¼ˆTeacher â†’ Studentï¼‰\n",
    "\n",
    "ä¸€ä¸ªå…¸å‹çš„è’¸é¦ pipelineï¼š\n",
    "\n",
    "1. **è®­ç»ƒ Teacherï¼ˆå¤§æ¨¡å‹ï¼‰**  \n",
    "   - ç”¨å¸¸è§„ FP32 æˆ–æ··åˆç²¾åº¦è®­ç»ƒ  \n",
    "   - å¾—åˆ°æ€§èƒ½è¾ƒå¥½çš„å¤§æ¨¡å‹ $f_T$\n",
    "\n",
    "2. **å‡†å¤‡ Teacher è¾“å‡º**  \n",
    "   - å¯¹è®­ç»ƒé›†æˆ–é¢å¤–çš„è’¸é¦æ•°æ®é›†è·‘ Teacher  \n",
    "   - ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„ Teacher logits $z^T$ æˆ– soft probabilities $p^T$\n",
    "\n",
    "3. **æ„å»º Studentï¼ˆå°æ¨¡å‹ï¼‰**  \n",
    "   - æ›´å°çš„å®½åº¦/æ·±åº¦  \n",
    "   - æ›´æ˜“é‡åŒ–/å‰ªæçš„ç»“æ„ï¼ˆä¾‹å¦‚æ›´çª„çš„ MLPã€éƒ¨åˆ† head å‰ªæï¼‰\n",
    "\n",
    "4. **è”åˆè®­ç»ƒ Student**  \n",
    "   - å‰å‘ï¼šStudent è¾“å‡º logits $z^S$ã€æ¦‚ç‡ $p^S$  \n",
    "   - è®¡ç®—ï¼š\n",
    "     - hard lossï¼š$ L_{\\text{CE}}(y, p^S) $\n",
    "     - soft lossï¼š$ L_{\\text{KL}}(p^T \\| p^S) $\n",
    "   - ç»„åˆï¼š\n",
    "     $$L = \\alpha L_{\\text{CE}} + (1-\\alpha)L_{\\text{KL}}$$\n",
    "   - åå‘ä¼ æ’­ï¼Œæ›´æ–° Student å‚æ•°\n",
    "\n",
    "5. **ï¼ˆå¯é€‰ï¼‰å¢åŠ ç‰¹å¾è’¸é¦ï¼ˆFeature Distillationï¼‰**  \n",
    "   - å¼ºåˆ¶ Student çš„ä¸­é—´å±‚ç‰¹å¾æ¥è¿‘ Teacherï¼š\n",
    "     $$\n",
    "     L_{\\text{feat}} = \\| h^S - P(h^T) \\|_2^2\n",
    "     $$\n",
    "   - å…¶ä¸­ $P(\\cdot)$ æ˜¯æŠ•å½±å±‚ï¼Œç”¨äºå¯¹é½ç»´åº¦ã€‚\n",
    "\n",
    "6. **ï¼ˆå¯é€‰ï¼‰å¢åŠ æ³¨æ„åŠ›è’¸é¦ï¼ˆAttention Distillationï¼‰**  \n",
    "   - å¯¹ Transformerï¼š\n",
    "     $$\n",
    "     L_{\\text{attn}} = \\| A^S - A^T \\|_2^2\n",
    "     $$\n",
    "   - å…¶ä¸­ $A$ æ˜¯ Attention mapï¼ˆå¦‚ softmax(QK^\\top / \\sqrt{d_k})ï¼‰\n",
    "\n",
    "7. **å¾—åˆ°æœ€ç»ˆçš„ Student**  \n",
    "   - æ›´è½»é‡ã€æ›´å¿«ã€æ›´é€‚åˆé‡åŒ–ä¸å‰ªæ  \n",
    "   - å¸¸ç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼ˆè½¦ç«¯ã€ç§»åŠ¨ç«¯ã€robot ç­‰ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3.7 ä¸ Runtime Inference åœºæ™¯çš„å…³ç³»\n",
    "\n",
    "åœ¨éƒ¨ç½²åœºæ™¯ä¸­ï¼Œä½ å¯ä»¥ï¼š\n",
    "\n",
    "- å…ˆè®­ç»ƒä¸€ä¸ªè¾ƒå¤§çš„ Teacherï¼ˆå¯èƒ½åœ¨äº‘ç«¯/æ•°æ®ä¸­å¿ƒï¼‰  \n",
    "- ç„¶åè®¾è®¡ä¸€ä¸ªä¸º **è½¦ç«¯/ECU/NPU ä¼˜åŒ–** çš„ Studentï¼š\n",
    "  - æ›´å°çš„å®½åº¦/æ·±åº¦  \n",
    "  - æ›´å‹å¥½çš„å¼ é‡å½¢çŠ¶å’Œ kernel æ˜ å°„  \n",
    "  - ä¾¿äº int8 / int4 é‡åŒ–ã€ä½ç§©åˆ†è§£ã€ç»“æ„åŒ–å‰ªæ\n",
    "\n",
    "ç„¶åï¼š\n",
    "\n",
    "1. ç”¨çŸ¥è¯†è’¸é¦ä¿è¯ Student åœ¨ç²¾åº¦ä¸Šå°½é‡é€¼è¿‘ Teacher  \n",
    "2. å†å¯¹ Student åšï¼š\n",
    "   - é‡åŒ–ï¼ˆPTQ/QATï¼‰\n",
    "   - å‰ªæï¼ˆç»“æ„åŒ–é€šé“/å¤´å‰ªæï¼‰\n",
    "   - low-rank/SVD åˆ†è§£\n",
    "3. æœ€ç»ˆå¯¼å‡ºä¸€ä¸ª **â€œè’¸é¦ + å‹ç¼© + é‡åŒ–â€ ä¸€ä½“åŒ–çš„éƒ¨ç½²ç‰ˆæœ¬**\n",
    "\n",
    "> æ¢å¥è¯è¯´ï¼š  \n",
    "> **è’¸é¦ä¸æ˜¯â€œå‹æ¨¡å‹â€ï¼Œè€Œæ˜¯â€œæŠŠå¤§æ¨¡å‹çš„å‡½æ•°ç©ºé—´æŠ•å½±åˆ°å°æ¨¡å‹çš„å‡½æ•°å­ç©ºé—´â€ã€‚**  \n",
    "> è¿™å°±æ˜¯å®ƒåœ¨æ•°å­¦å’Œå·¥ç¨‹ä¸Šçš„æ ¸å¿ƒæ„ä¹‰ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285e30a",
   "metadata": {},
   "source": [
    "\n",
    "## 2.4 å¤šç›®æ ‡ä¼˜åŒ–ï¼šç²¾åº¦ vs å»¶è¿Ÿ vs å†…å­˜\n",
    "\n",
    "åœ¨åšéƒ¨ç½²æ—¶ï¼Œä½ ä¸ä¼šåªå…³å¿ƒ lossï¼Œè¿˜ä¼šå…³å¿ƒï¼š\n",
    "\n",
    "- latencyï¼ˆæ¨ç†å»¶è¿Ÿï¼‰\n",
    "- memoryï¼ˆæ˜¾å­˜ / DRAM å ç”¨ï¼‰\n",
    "- throughputï¼ˆQPSï¼‰\n",
    "\n",
    "å¯ä»¥å°†è¿™äº›çº³å…¥ä¼˜åŒ–ç›®æ ‡ï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\quad \\alpha \\cdot \\text{Error}(\\theta)\n",
    "+ \\beta \\cdot \\text{Latency}(\\theta)\n",
    "+ \\gamma \\cdot \\text{Memory}(\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $\\theta$ï¼šæ¨¡å‹ç»“æ„ + é‡åŒ–é…ç½® + å‰ªæç­–ç•¥ ç­‰\n",
    "- $\\text{Latency}(\\theta)$ï¼šé€šè¿‡ profile æˆ– analytical model ä¼°è®¡\n",
    "- $\\text{Memory}(\\theta)$ï¼šç”±å‚æ•°é‡ã€activationã€KV cache å†³å®š\n",
    "\n",
    "å·¥ç¨‹å®ç°ä¸­å¸¸è§ç®€åŒ–ï¼š\n",
    "\n",
    "- å›ºå®šæŸä¸ª latency/memory ä¸Šé™ä½œä¸ºçº¦æŸï¼š\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\text{Error}(\\theta)\n",
    "  \\quad \\text{s.t.} \\quad \\text{Latency}(\\theta) \\le L_{\\max}, \\\n",
    "  \\text{Memory}(\\theta) \\le M_{\\max}\n",
    "$$\n",
    "\n",
    "\n",
    "- æˆ–æŠŠ latency/memory è½¬æ¢ä¸ºæ­£åˆ™é¡¹åŠ å…¥ loss\n",
    "\n",
    "> é‡è¦çš„æ˜¯ï¼šä½ è¦èƒ½æŠŠâ€œéƒ¨ç½²éœ€æ±‚â€ç¿»è¯‘æˆæ•°å­¦ä¸Šçš„â€œç›®æ ‡ + çº¦æŸâ€ï¼Œè¿™æ ·æ‰èƒ½ç”¨ä¼˜åŒ–å·¥å…·ç³»ç»Ÿåœ°è®¾è®¡ç®—æ³•ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 2.5 æœ¬ç« å°ç»“\n",
    "\n",
    "æœ¬ç« é‡ç‚¹ï¼š\n",
    "\n",
    "1. å‰ªæå¯ä»¥çœ‹ä½œå¸¦ $L_0/L_1$ ç¨€ç–çº¦æŸçš„ä¼˜åŒ–é—®é¢˜\n",
    "2. é‡åŒ–å¯ä»¥é€šè¿‡æœ€å°åŒ–é‡åŒ–è¯¯å·®çš„ä¼˜åŒ–é—®é¢˜ç¡®å®š scale/zero-pointï¼Œè¿›ä¸€æ­¥å¯é€šè¿‡è®­ç»ƒ joint optimize\n",
    "3. è’¸é¦æ˜¯ä¸€ä¸ªè”åˆæœ€å°åŒ– CE å’Œ KL çš„å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜\n",
    "4. éƒ¨ç½²æ—¶çš„ç²¾åº¦ã€å»¶è¿Ÿã€å†…å­˜å¯ä»¥ç»Ÿä¸€æœ¬ä¸ºå¤šç›®æ ‡/å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜\n",
    "\n",
    "> åœ¨åç»­ç« èŠ‚ä¸­ï¼Œè¿™äº›ä¼˜åŒ–ç›®æ ‡ä¼šä¸è¿‘ä¼¼ç†è®ºã€æ¦‚ç‡ç»Ÿè®¡ã€ç¡¬ä»¶æ¨¡å‹ç»“åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€å¥—å®Œæ•´çš„æ¨ç†åŠ é€Ÿæ€ç»´ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde561f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0591187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.706850051879883\n",
      "loss: 6.641338348388672\n",
      "loss: 6.852497100830078\n",
      "loss: 6.8808393478393555\n",
      "loss: 6.776559829711914\n",
      "loss: 7.481376647949219\n",
      "loss: 6.0222039222717285\n",
      "loss: 7.132743835449219\n",
      "loss: 7.319509983062744\n",
      "loss: 6.267420291900635\n",
      "loss: 6.981998920440674\n",
      "loss: 7.101173400878906\n",
      "loss: 7.005058288574219\n",
      "loss: 7.252947807312012\n",
      "loss: 6.8980913162231445\n",
      "loss: 7.536619663238525\n",
      "loss: 6.342973232269287\n",
      "loss: 6.782742500305176\n",
      "loss: 6.836009979248047\n",
      "loss: 7.472536087036133\n",
      "loss: 7.106359958648682\n",
      "loss: 6.843428134918213\n",
      "loss: 6.935361862182617\n",
      "loss: 6.460139274597168\n",
      "loss: 6.434418201446533\n",
      "loss: 7.590257167816162\n",
      "loss: 6.904282569885254\n",
      "loss: 7.575451850891113\n",
      "loss: 6.969733238220215\n",
      "loss: 7.346570014953613\n",
      "loss: 6.731241703033447\n",
      "loss: 6.119562149047852\n",
      "loss: 6.660286903381348\n",
      "loss: 6.891930103302002\n",
      "loss: 5.715048789978027\n",
      "loss: 6.798610687255859\n",
      "loss: 7.164504528045654\n",
      "loss: 6.8233866691589355\n",
      "loss: 6.830433368682861\n",
      "loss: 6.465048789978027\n",
      "loss: 6.511832237243652\n",
      "loss: 7.389891624450684\n",
      "loss: 6.8146071434021\n",
      "loss: 6.40390682220459\n",
      "loss: 6.609987735748291\n",
      "loss: 6.763182163238525\n",
      "loss: 6.72474479675293\n",
      "loss: 6.76747989654541\n",
      "loss: 7.239054203033447\n",
      "loss: 6.649162292480469\n",
      "loss: 7.091368675231934\n",
      "loss: 6.742522239685059\n",
      "loss: 6.685555934906006\n",
      "loss: 6.719648361206055\n",
      "loss: 6.681499004364014\n",
      "loss: 7.204469203948975\n",
      "loss: 7.168405532836914\n",
      "loss: 7.1225996017456055\n",
      "loss: 6.8901143074035645\n",
      "loss: 7.390513896942139\n",
      "loss: 7.265018939971924\n",
      "loss: 6.556756973266602\n",
      "loss: 6.907297134399414\n",
      "loss: 6.952666759490967\n",
      "loss: 7.076615333557129\n",
      "loss: 6.906044006347656\n",
      "loss: 6.818953514099121\n",
      "loss: 6.883358955383301\n",
      "loss: 6.862888336181641\n",
      "loss: 6.627162933349609\n",
      "loss: 6.573819160461426\n",
      "loss: 6.991433143615723\n",
      "loss: 7.29876708984375\n",
      "loss: 6.40640115737915\n",
      "loss: 6.829994201660156\n",
      "loss: 6.522035598754883\n",
      "loss: 6.916079521179199\n",
      "loss: 6.896795272827148\n",
      "loss: 7.603297233581543\n",
      "loss: 7.372061252593994\n",
      "loss: 7.0386834144592285\n",
      "loss: 6.870718955993652\n",
      "loss: 6.50913143157959\n",
      "loss: 7.52623176574707\n",
      "loss: 6.745406150817871\n",
      "loss: 6.851004123687744\n",
      "loss: 7.673297882080078\n",
      "loss: 6.708409309387207\n",
      "loss: 6.836457252502441\n",
      "loss: 6.777379035949707\n",
      "loss: 6.413236618041992\n",
      "loss: 6.92719030380249\n",
      "loss: 6.621286869049072\n",
      "loss: 6.2065019607543945\n",
      "loss: 6.665985107421875\n",
      "loss: 7.137915134429932\n",
      "loss: 6.767892360687256\n",
      "loss: 8.051656723022461\n",
      "loss: 6.625263214111328\n",
      "loss: 6.431797027587891\n",
      "loss: 7.207820892333984\n",
      "loss: 6.629262924194336\n",
      "loss: 6.858495712280273\n",
      "loss: 7.4415283203125\n",
      "loss: 6.482387542724609\n",
      "loss: 6.723425388336182\n",
      "loss: 7.056260585784912\n",
      "loss: 6.8283538818359375\n",
      "loss: 6.738766193389893\n",
      "loss: 6.516890525817871\n",
      "loss: 6.647271156311035\n",
      "loss: 6.484250068664551\n",
      "loss: 6.696055889129639\n",
      "loss: 7.111705780029297\n",
      "loss: 6.978602886199951\n",
      "loss: 6.721832275390625\n",
      "loss: 7.237315654754639\n",
      "loss: 7.294513702392578\n",
      "loss: 6.528671741485596\n",
      "loss: 6.996809005737305\n",
      "loss: 7.48117733001709\n",
      "loss: 6.604353904724121\n",
      "loss: 6.692049026489258\n",
      "loss: 6.430265426635742\n",
      "loss: 7.151003360748291\n",
      "loss: 6.996978759765625\n",
      "loss: 6.527896881103516\n",
      "loss: 7.109837055206299\n",
      "loss: 6.401959419250488\n",
      "loss: 6.6641435623168945\n",
      "loss: 7.432755470275879\n",
      "loss: 6.911662578582764\n",
      "loss: 6.624944686889648\n",
      "loss: 7.086079120635986\n",
      "loss: 6.848196506500244\n",
      "loss: 6.60008430480957\n",
      "loss: 5.979668140411377\n",
      "loss: 6.131065368652344\n",
      "loss: 6.532571315765381\n",
      "loss: 6.8620524406433105\n",
      "loss: 6.478692531585693\n",
      "loss: 6.048106670379639\n",
      "loss: 7.433409214019775\n",
      "loss: 7.592000484466553\n",
      "loss: 6.701013565063477\n",
      "loss: 6.5616302490234375\n",
      "loss: 6.528415679931641\n",
      "loss: 7.127872943878174\n",
      "loss: 6.286931991577148\n",
      "loss: 6.740148544311523\n",
      "loss: 6.673682689666748\n",
      "loss: 6.3059306144714355\n",
      "loss: 7.948846817016602\n",
      "loss: 7.174642086029053\n",
      "loss: 7.133371829986572\n",
      "loss: 6.572239398956299\n",
      "loss: 6.258087635040283\n",
      "loss: 6.381435394287109\n",
      "loss: 7.532124042510986\n",
      "loss: 6.0479817390441895\n",
      "loss: 6.921549320220947\n",
      "loss: 6.245826721191406\n",
      "loss: 7.18238639831543\n",
      "loss: 7.806329727172852\n",
      "loss: 6.561556339263916\n",
      "loss: 8.09016227722168\n",
      "loss: 7.130673408508301\n",
      "loss: 7.159051895141602\n",
      "loss: 6.686134338378906\n",
      "loss: 7.016738414764404\n",
      "loss: 6.90778923034668\n",
      "loss: 6.883511543273926\n",
      "loss: 6.8849568367004395\n",
      "loss: 6.291580677032471\n",
      "loss: 7.194637775421143\n",
      "loss: 6.566843509674072\n",
      "loss: 6.894926071166992\n",
      "loss: 6.606169700622559\n",
      "loss: 7.529797077178955\n",
      "loss: 7.566015243530273\n",
      "loss: 6.837937355041504\n",
      "loss: 7.378838062286377\n",
      "loss: 6.262255668640137\n",
      "loss: 6.97712516784668\n",
      "loss: 7.166830062866211\n",
      "loss: 6.668305397033691\n",
      "loss: 6.874044895172119\n",
      "loss: 6.555196285247803\n",
      "loss: 6.323087692260742\n",
      "loss: 6.908172130584717\n",
      "loss: 6.397244930267334\n",
      "loss: 7.178339004516602\n",
      "loss: 6.334286212921143\n",
      "loss: 7.124755382537842\n",
      "loss: 7.107276916503906\n",
      "loss: 6.584687232971191\n",
      "loss: 5.906883239746094\n",
      "loss: 6.894870758056641\n",
      "loss: 6.654548645019531\n",
      "loss: 6.89670467376709\n",
      "loss: 6.671645164489746\n",
      "loss: 6.748653411865234\n",
      "loss: 6.591020107269287\n",
      "loss: 6.8811187744140625\n",
      "loss: 5.972055912017822\n",
      "loss: 6.324606418609619\n",
      "loss: 6.7037553787231445\n",
      "loss: 6.778480529785156\n",
      "loss: 6.046205043792725\n",
      "loss: 6.864850997924805\n",
      "loss: 6.570070743560791\n",
      "loss: 7.082254409790039\n",
      "loss: 6.145928859710693\n",
      "loss: 7.018570423126221\n",
      "loss: 6.5012640953063965\n",
      "loss: 6.3907318115234375\n",
      "loss: 6.41400146484375\n",
      "loss: 6.276627063751221\n",
      "loss: 6.599930763244629\n",
      "loss: 6.468742370605469\n",
      "loss: 6.629279136657715\n",
      "loss: 7.225607872009277\n",
      "loss: 6.1480207443237305\n",
      "loss: 7.379115104675293\n",
      "loss: 6.774259567260742\n",
      "loss: 7.252009391784668\n",
      "loss: 6.959811210632324\n",
      "loss: 7.3246541023254395\n",
      "loss: 6.939969062805176\n",
      "loss: 6.25555944442749\n",
      "loss: 6.303403377532959\n",
      "loss: 7.63037109375\n",
      "loss: 6.374534606933594\n",
      "loss: 6.874504566192627\n",
      "loss: 7.061203956604004\n",
      "loss: 6.186196804046631\n",
      "loss: 6.762090682983398\n",
      "loss: 6.671528339385986\n",
      "loss: 6.831165313720703\n",
      "loss: 6.394855976104736\n",
      "loss: 7.091193199157715\n",
      "loss: 6.8645758628845215\n",
      "loss: 6.83671236038208\n",
      "loss: 6.749229907989502\n",
      "loss: 6.834411144256592\n",
      "loss: 6.574306488037109\n",
      "loss: 6.854681015014648\n",
      "loss: 6.522147178649902\n",
      "loss: 6.524106979370117\n",
      "loss: 6.7973175048828125\n",
      "loss: 7.613521575927734\n",
      "loss: 6.852322578430176\n",
      "loss: 6.526968002319336\n",
      "loss: 6.414401054382324\n",
      "loss: 6.881453514099121\n",
      "loss: 6.540518760681152\n",
      "loss: 6.804447650909424\n",
      "loss: 6.60041618347168\n",
      "loss: 6.715991020202637\n",
      "loss: 6.0435075759887695\n",
      "loss: 6.4368510246276855\n",
      "loss: 6.191517353057861\n",
      "loss: 6.7032670974731445\n",
      "loss: 6.733334064483643\n",
      "loss: 6.413013935089111\n",
      "loss: 6.86015510559082\n",
      "loss: 6.053689002990723\n",
      "loss: 7.20734167098999\n",
      "loss: 7.678032875061035\n",
      "loss: 6.059812545776367\n",
      "loss: 6.439192295074463\n",
      "loss: 6.474296569824219\n",
      "loss: 6.588459014892578\n",
      "loss: 6.922102451324463\n",
      "loss: 6.159167289733887\n",
      "loss: 7.157773494720459\n",
      "loss: 6.919376373291016\n",
      "loss: 6.074123382568359\n",
      "loss: 6.503050804138184\n",
      "loss: 6.452080726623535\n",
      "loss: 6.725259780883789\n",
      "loss: 6.362843990325928\n",
      "loss: 6.328734874725342\n",
      "loss: 7.047393798828125\n",
      "loss: 6.583359718322754\n",
      "loss: 7.153032302856445\n",
      "loss: 7.4308552742004395\n",
      "loss: 6.530285358428955\n",
      "loss: 7.382792949676514\n",
      "loss: 6.165316581726074\n",
      "loss: 6.713208198547363\n",
      "loss: 6.114446640014648\n",
      "loss: 7.0930681228637695\n",
      "loss: 6.510167121887207\n",
      "loss: 7.015711784362793\n",
      "loss: 6.543047904968262\n",
      "loss: 6.6632280349731445\n",
      "loss: 6.540656566619873\n",
      "loss: 6.455130100250244\n",
      "loss: 6.539009094238281\n",
      "loss: 6.264113426208496\n",
      "loss: 7.075157165527344\n",
      "loss: 6.668269157409668\n",
      "loss: 6.43468713760376\n",
      "loss: 6.823137283325195\n",
      "loss: 6.5142693519592285\n",
      "loss: 7.038346767425537\n",
      "loss: 6.464938163757324\n",
      "loss: 6.753921985626221\n",
      "loss: 7.504758834838867\n",
      "loss: 7.081350803375244\n",
      "loss: 7.276307106018066\n",
      "loss: 6.354090690612793\n",
      "loss: 6.7796630859375\n",
      "loss: 6.958858489990234\n",
      "loss: 6.734329700469971\n",
      "loss: 6.333681583404541\n",
      "loss: 7.044743537902832\n",
      "loss: 7.164883613586426\n",
      "loss: 6.218859672546387\n",
      "loss: 6.022533416748047\n",
      "loss: 6.329344749450684\n",
      "loss: 6.286465167999268\n",
      "loss: 6.28187370300293\n",
      "loss: 6.188319206237793\n",
      "loss: 6.794439315795898\n",
      "loss: 7.029144287109375\n",
      "loss: 6.487915992736816\n",
      "loss: 6.8270583152771\n",
      "loss: 6.762378692626953\n",
      "loss: 6.481950283050537\n",
      "loss: 6.605425834655762\n",
      "loss: 6.278705596923828\n",
      "loss: 6.215167999267578\n",
      "loss: 6.329497337341309\n",
      "loss: 6.303048133850098\n",
      "loss: 6.217254638671875\n",
      "loss: 6.140409469604492\n",
      "loss: 7.0440802574157715\n",
      "loss: 6.25902795791626\n",
      "loss: 6.895532608032227\n",
      "loss: 7.110992908477783\n",
      "loss: 7.257238388061523\n",
      "loss: 6.742395401000977\n",
      "loss: 6.548550605773926\n",
      "loss: 6.599635124206543\n",
      "loss: 6.713657855987549\n",
      "loss: 6.359561920166016\n",
      "loss: 6.19296407699585\n",
      "loss: 5.907092571258545\n",
      "loss: 6.765369892120361\n",
      "loss: 7.009969234466553\n",
      "loss: 6.269199371337891\n",
      "loss: 6.069506645202637\n",
      "loss: 6.56056022644043\n",
      "loss: 6.200866222381592\n",
      "loss: 6.5078020095825195\n",
      "loss: 6.457047462463379\n",
      "loss: 7.0951972007751465\n",
      "loss: 7.050428867340088\n",
      "loss: 6.856836795806885\n",
      "loss: 6.174586296081543\n",
      "loss: 6.4559173583984375\n",
      "loss: 6.469161033630371\n",
      "loss: 6.937126636505127\n",
      "loss: 6.312093734741211\n",
      "loss: 7.080557823181152\n",
      "loss: 7.758114814758301\n",
      "loss: 6.890656471252441\n",
      "loss: 6.575004577636719\n",
      "loss: 5.779879093170166\n",
      "loss: 6.146233558654785\n",
      "loss: 6.237594127655029\n",
      "loss: 6.536383152008057\n",
      "loss: 5.879183292388916\n",
      "loss: 6.433385372161865\n",
      "loss: 7.058742523193359\n",
      "loss: 5.808073043823242\n",
      "loss: 7.07791805267334\n",
      "loss: 7.00715446472168\n",
      "loss: 5.802290916442871\n",
      "loss: 6.889244079589844\n",
      "loss: 6.6735334396362305\n",
      "loss: 6.83665657043457\n",
      "loss: 5.998993396759033\n",
      "loss: 6.669722557067871\n",
      "loss: 6.29646110534668\n",
      "loss: 6.359573841094971\n",
      "loss: 6.0224714279174805\n",
      "loss: 6.516693592071533\n",
      "loss: 6.160609245300293\n",
      "loss: 6.088640213012695\n",
      "loss: 6.157589912414551\n",
      "loss: 6.953873157501221\n",
      "loss: 5.610256671905518\n",
      "loss: 7.380729675292969\n",
      "loss: 6.386970043182373\n",
      "loss: 6.884784698486328\n",
      "loss: 7.027744293212891\n",
      "loss: 6.39943265914917\n",
      "loss: 6.943734645843506\n",
      "loss: 6.849974632263184\n",
      "loss: 6.53791618347168\n",
      "loss: 6.115701198577881\n",
      "loss: 6.217510223388672\n",
      "loss: 6.320248603820801\n",
      "loss: 6.522283554077148\n",
      "loss: 6.4914870262146\n",
      "loss: 6.915092945098877\n",
      "loss: 6.235347747802734\n",
      "loss: 6.815883159637451\n",
      "loss: 6.691957473754883\n",
      "loss: 5.883729934692383\n",
      "loss: 7.39335298538208\n",
      "loss: 6.606777191162109\n",
      "loss: 6.639791011810303\n",
      "loss: 7.150055885314941\n",
      "loss: 6.168609619140625\n",
      "loss: 5.9827141761779785\n",
      "loss: 6.471479892730713\n",
      "loss: 6.008847236633301\n",
      "loss: 7.149232864379883\n",
      "loss: 6.325905799865723\n",
      "loss: 6.344097137451172\n",
      "loss: 6.441979885101318\n",
      "loss: 6.872888565063477\n",
      "loss: 6.463354110717773\n",
      "loss: 7.348794937133789\n",
      "loss: 6.072803020477295\n",
      "loss: 6.3555169105529785\n",
      "loss: 5.8875412940979\n",
      "loss: 6.127913951873779\n",
      "loss: 6.198395729064941\n",
      "loss: 6.179571151733398\n",
      "loss: 5.997127056121826\n",
      "loss: 6.251445293426514\n",
      "loss: 6.76671838760376\n",
      "loss: 6.726207733154297\n",
      "loss: 6.927957534790039\n",
      "loss: 6.631016731262207\n",
      "loss: 6.640542507171631\n",
      "loss: 6.427459239959717\n",
      "loss: 6.559637546539307\n",
      "loss: 6.446782112121582\n",
      "loss: 5.710960388183594\n",
      "loss: 6.943652153015137\n",
      "loss: 6.186283588409424\n",
      "loss: 6.102205276489258\n",
      "loss: 6.269074440002441\n",
      "loss: 6.582566261291504\n",
      "loss: 6.305881500244141\n",
      "loss: 6.634873867034912\n",
      "loss: 6.2209062576293945\n",
      "loss: 6.802491188049316\n",
      "loss: 5.7977190017700195\n",
      "loss: 6.39889669418335\n",
      "loss: 6.602626800537109\n",
      "loss: 6.127783298492432\n",
      "loss: 5.930144786834717\n",
      "loss: 5.847285747528076\n",
      "loss: 6.696409702301025\n",
      "loss: 5.8753132820129395\n",
      "loss: 6.446070671081543\n",
      "loss: 5.8828558921813965\n",
      "loss: 6.042029857635498\n",
      "loss: 6.512880325317383\n",
      "loss: 5.988066673278809\n",
      "loss: 6.557059288024902\n",
      "loss: 6.696460247039795\n",
      "loss: 6.4565324783325195\n",
      "loss: 6.058075904846191\n",
      "loss: 5.694813251495361\n",
      "loss: 5.969437122344971\n",
      "loss: 6.208247184753418\n",
      "loss: 6.437132835388184\n",
      "loss: 6.6466217041015625\n",
      "loss: 6.242061614990234\n",
      "loss: 6.500244617462158\n",
      "loss: 5.942705154418945\n",
      "loss: 6.834749698638916\n",
      "loss: 6.40257453918457\n",
      "loss: 6.2535200119018555\n",
      "loss: 5.97901725769043\n",
      "loss: 6.4911088943481445\n",
      "loss: 6.635720252990723\n",
      "loss: 5.838352203369141\n",
      "loss: 6.032252788543701\n",
      "loss: 6.645125865936279\n",
      "loss: 6.430397033691406\n",
      "loss: 6.616686820983887\n",
      "loss: 6.1136016845703125\n",
      "loss: 6.36483097076416\n",
      "loss: 6.227858066558838\n",
      "loss: 6.470073699951172\n",
      "loss: 6.32983922958374\n",
      "loss: 6.709038734436035\n",
      "loss: 6.431455135345459\n",
      "loss: 6.884154319763184\n",
      "loss: 6.886291980743408\n",
      "loss: 6.490385055541992\n",
      "loss: 6.087038516998291\n",
      "loss: 6.603250026702881\n",
      "loss: 6.174224853515625\n",
      "loss: 6.992161750793457\n",
      "loss: 6.607872486114502\n",
      "loss: 6.553086757659912\n",
      "loss: 5.732275485992432\n",
      "loss: 6.517051696777344\n",
      "loss: 6.425448894500732\n",
      "loss: 6.878845691680908\n",
      "loss: 6.014206409454346\n",
      "loss: 6.381026744842529\n",
      "loss: 6.232405185699463\n",
      "loss: 6.864507675170898\n",
      "loss: 6.075043678283691\n",
      "loss: 6.124028205871582\n",
      "loss: 6.078914165496826\n",
      "loss: 6.020209312438965\n",
      "loss: 6.499446868896484\n",
      "loss: 6.138288974761963\n",
      "loss: 6.680431365966797\n",
      "loss: 6.346085071563721\n",
      "loss: 5.984405994415283\n",
      "loss: 6.492869853973389\n",
      "loss: 6.306856632232666\n",
      "loss: 5.993915557861328\n",
      "loss: 6.798046112060547\n",
      "loss: 6.8260087966918945\n",
      "loss: 6.586174011230469\n",
      "loss: 6.389503479003906\n",
      "loss: 7.137331008911133\n",
      "loss: 6.275449752807617\n",
      "loss: 6.061335563659668\n",
      "loss: 5.952853202819824\n",
      "loss: 6.745673656463623\n",
      "loss: 6.559486389160156\n",
      "loss: 6.401480674743652\n",
      "loss: 6.441822528839111\n",
      "loss: 5.891444683074951\n",
      "loss: 6.673750400543213\n",
      "loss: 6.719797611236572\n",
      "loss: 6.2025065422058105\n",
      "loss: 7.058017730712891\n",
      "loss: 6.697820663452148\n",
      "loss: 6.340300559997559\n",
      "loss: 6.2939372062683105\n",
      "loss: 6.119968414306641\n",
      "loss: 6.124544143676758\n",
      "loss: 6.7037858963012695\n",
      "loss: 6.103659629821777\n",
      "loss: 6.974124431610107\n",
      "loss: 6.533123016357422\n",
      "loss: 6.0491943359375\n",
      "loss: 6.097867012023926\n",
      "loss: 6.339044094085693\n",
      "loss: 6.730330467224121\n",
      "loss: 6.164414405822754\n",
      "loss: 6.030762672424316\n",
      "loss: 6.03792667388916\n",
      "loss: 6.025484085083008\n",
      "loss: 6.059047698974609\n",
      "loss: 7.263533115386963\n",
      "loss: 6.475149631500244\n",
      "loss: 6.195003986358643\n",
      "loss: 6.166118621826172\n",
      "loss: 6.641723155975342\n",
      "loss: 5.868322849273682\n",
      "loss: 6.019104480743408\n",
      "loss: 6.558928489685059\n",
      "loss: 6.806930065155029\n",
      "loss: 5.5650105476379395\n",
      "loss: 7.260093688964844\n",
      "loss: 5.914600849151611\n",
      "loss: 5.680654048919678\n",
      "loss: 6.425334453582764\n",
      "loss: 6.174203395843506\n",
      "loss: 5.623807907104492\n",
      "loss: 6.07308292388916\n",
      "loss: 6.6459574699401855\n",
      "loss: 6.712681770324707\n",
      "loss: 6.243305683135986\n",
      "loss: 5.916617393493652\n",
      "loss: 6.544065952301025\n",
      "loss: 6.381461143493652\n",
      "loss: 6.305291652679443\n",
      "loss: 5.748234748840332\n",
      "loss: 6.272891998291016\n",
      "loss: 6.475611686706543\n",
      "loss: 6.031549453735352\n",
      "loss: 6.384974002838135\n",
      "loss: 6.628450393676758\n",
      "loss: 6.245976448059082\n",
      "loss: 5.707831859588623\n",
      "loss: 6.176517486572266\n",
      "loss: 6.5705366134643555\n",
      "loss: 6.934699535369873\n",
      "loss: 6.243838787078857\n",
      "loss: 6.233763217926025\n",
      "loss: 6.346552848815918\n",
      "loss: 6.985827922821045\n",
      "loss: 6.076910495758057\n",
      "loss: 5.962587833404541\n",
      "loss: 6.723020076751709\n",
      "loss: 6.465975761413574\n",
      "loss: 6.3504109382629395\n",
      "loss: 5.988117218017578\n",
      "loss: 6.132011890411377\n",
      "loss: 6.552639961242676\n",
      "loss: 6.807855129241943\n",
      "loss: 6.525279521942139\n",
      "loss: 5.931354999542236\n",
      "loss: 6.683190822601318\n",
      "loss: 5.838563442230225\n",
      "loss: 6.264512062072754\n",
      "loss: 6.666626453399658\n",
      "loss: 6.251558303833008\n",
      "loss: 6.525595188140869\n",
      "loss: 7.06513786315918\n",
      "loss: 6.242403984069824\n",
      "loss: 5.822991371154785\n",
      "loss: 6.2041473388671875\n",
      "loss: 6.035606384277344\n",
      "loss: 6.104320526123047\n",
      "loss: 6.4355878829956055\n",
      "loss: 6.125908374786377\n",
      "loss: 6.736806869506836\n",
      "loss: 6.509762287139893\n",
      "loss: 5.883366107940674\n",
      "loss: 6.253754615783691\n",
      "loss: 6.461881160736084\n",
      "loss: 5.9715352058410645\n",
      "loss: 5.527203559875488\n",
      "loss: 5.737614631652832\n",
      "loss: 6.24567174911499\n",
      "loss: 5.395791530609131\n",
      "loss: 5.916197299957275\n",
      "loss: 6.23659086227417\n",
      "loss: 6.515339374542236\n",
      "loss: 6.028141021728516\n",
      "loss: 6.054612159729004\n",
      "loss: 5.7307538986206055\n",
      "loss: 6.008411407470703\n",
      "loss: 5.881988525390625\n",
      "loss: 6.151642799377441\n",
      "loss: 6.063098430633545\n",
      "loss: 6.103738784790039\n",
      "loss: 5.717208385467529\n",
      "loss: 6.166741847991943\n",
      "loss: 5.535179615020752\n",
      "loss: 5.830902099609375\n",
      "loss: 6.557773590087891\n",
      "loss: 6.195363521575928\n",
      "loss: 6.131153106689453\n",
      "loss: 6.495815753936768\n",
      "loss: 5.729499340057373\n",
      "loss: 6.53269624710083\n",
      "loss: 6.696101665496826\n",
      "loss: 5.536377429962158\n",
      "loss: 7.040493011474609\n",
      "loss: 6.203788757324219\n",
      "loss: 6.542891502380371\n",
      "loss: 6.051088333129883\n",
      "loss: 6.437726974487305\n",
      "loss: 5.943519115447998\n",
      "loss: 6.276830673217773\n",
      "loss: 5.57048225402832\n",
      "loss: 6.473200798034668\n",
      "loss: 5.732968807220459\n",
      "loss: 6.616518020629883\n",
      "loss: 5.997669219970703\n",
      "loss: 5.844431400299072\n",
      "loss: 5.719852924346924\n",
      "loss: 6.048179626464844\n",
      "loss: 6.0104498863220215\n",
      "loss: 6.024691104888916\n",
      "loss: 5.18756103515625\n",
      "loss: 7.08920955657959\n",
      "loss: 6.6488752365112305\n",
      "loss: 6.688286781311035\n",
      "loss: 5.7672553062438965\n",
      "loss: 6.642668724060059\n",
      "loss: 6.108421325683594\n",
      "loss: 5.960382461547852\n",
      "loss: 6.08488130569458\n",
      "loss: 5.981443405151367\n",
      "loss: 5.955243110656738\n",
      "loss: 6.555909156799316\n",
      "loss: 5.881110668182373\n",
      "loss: 5.923843860626221\n",
      "loss: 7.107734203338623\n",
      "loss: 6.883937835693359\n",
      "loss: 6.081905841827393\n",
      "loss: 5.846317291259766\n",
      "loss: 6.051874160766602\n",
      "loss: 5.2082624435424805\n",
      "loss: 5.887549877166748\n",
      "loss: 6.33962345123291\n",
      "loss: 6.034541606903076\n",
      "loss: 5.6536641120910645\n",
      "loss: 6.132651329040527\n",
      "loss: 6.309074878692627\n",
      "loss: 6.092738628387451\n",
      "loss: 6.646914482116699\n",
      "loss: 6.99898624420166\n",
      "loss: 6.073125839233398\n",
      "loss: 6.807725429534912\n",
      "loss: 6.127124786376953\n",
      "loss: 6.376272678375244\n",
      "loss: 6.170276165008545\n",
      "loss: 6.448266983032227\n",
      "loss: 6.106289863586426\n",
      "loss: 6.357379913330078\n",
      "loss: 6.503909587860107\n",
      "loss: 5.687970161437988\n",
      "loss: 5.9307427406311035\n",
      "loss: 6.334779739379883\n",
      "loss: 6.123776435852051\n",
      "loss: 5.843665599822998\n",
      "loss: 5.835573673248291\n",
      "loss: 6.586599826812744\n",
      "loss: 6.336284160614014\n",
      "loss: 5.61054801940918\n",
      "loss: 6.3862762451171875\n",
      "loss: 6.093517303466797\n",
      "loss: 6.082355976104736\n",
      "loss: 5.935529708862305\n",
      "loss: 5.93927001953125\n",
      "loss: 6.5548996925354\n",
      "loss: 5.639522552490234\n",
      "loss: 5.432353496551514\n",
      "loss: 5.8193254470825195\n",
      "loss: 5.636142730712891\n",
      "loss: 5.602296352386475\n",
      "loss: 6.010869026184082\n",
      "loss: 5.786271095275879\n",
      "loss: 6.643828868865967\n",
      "loss: 5.928686618804932\n",
      "loss: 6.659941673278809\n",
      "loss: 5.871101379394531\n",
      "loss: 6.295968055725098\n",
      "loss: 6.143762111663818\n",
      "loss: 7.217662334442139\n",
      "loss: 6.57054328918457\n",
      "loss: 6.085968494415283\n",
      "loss: 6.379091739654541\n",
      "loss: 5.968247890472412\n",
      "loss: 6.518858909606934\n",
      "loss: 6.724470138549805\n",
      "loss: 6.394585609436035\n",
      "loss: 6.336903095245361\n",
      "loss: 6.3271284103393555\n",
      "loss: 6.234541416168213\n",
      "loss: 6.396357536315918\n",
      "loss: 5.825386047363281\n",
      "loss: 6.241199016571045\n",
      "loss: 6.195932388305664\n",
      "loss: 6.457680702209473\n",
      "loss: 5.889091491699219\n",
      "loss: 6.16498327255249\n",
      "loss: 5.759756088256836\n",
      "loss: 6.0360636711120605\n",
      "loss: 5.781589508056641\n",
      "loss: 6.43454647064209\n",
      "loss: 5.978590965270996\n",
      "loss: 6.941714286804199\n",
      "loss: 5.951913833618164\n",
      "loss: 6.524755477905273\n",
      "loss: 6.457669734954834\n",
      "loss: 5.581488609313965\n",
      "loss: 6.227980136871338\n",
      "loss: 5.836846828460693\n",
      "loss: 5.810550689697266\n",
      "loss: 5.82911491394043\n",
      "loss: 6.314847469329834\n",
      "loss: 6.1564812660217285\n",
      "loss: 6.416525363922119\n",
      "loss: 6.0841546058654785\n",
      "loss: 6.068796634674072\n",
      "loss: 6.110822677612305\n",
      "loss: 6.07769775390625\n",
      "loss: 6.136997699737549\n",
      "loss: 6.453009605407715\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use pretrained ResNet-18 as teacher\n",
    "teacher = resnet18(weights=\"IMAGENET1K_V1\")\n",
    "# fine tune last layer for CIFAR-10\n",
    "teacher.fc = nn.Linear(512, 10)\n",
    "teacher = teacher.to(device)\n",
    "opt_teacher = torch.optim.Adam(teacher.parameters(), lr=1e-4)\n",
    "\n",
    "def train_teacher(epochs, train_loader):\n",
    "    teacher.train()\n",
    "    for _ in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = teacher(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            opt_teacher.zero_grad()\n",
    "            loss.backward()\n",
    "            opt_teacher.step()\n",
    "\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate student\n",
    "student = SmallCNN(num_classes=10).to(device)\n",
    "opt_student = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
    "alpha = 0.3   # weight for CE loss\n",
    "T = 4.0       # distillation temperature\n",
    "\n",
    "def kd_loss(student_logits, teacher_logits, T=4.0):\n",
    "    # Softmax with temperature\n",
    "    teacher_probs = F.softmax(teacher_logits / T, dim=1)\n",
    "    student_log_probs = F.log_softmax(student_logits / T, dim=1)\n",
    "\n",
    "    # KL divergence, aggregated over batch \n",
    "    loss_kl = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')\n",
    "\n",
    "    return (T * T) * loss_kl\n",
    "\n",
    "\n",
    "\n",
    "# 1. æ•°æ®å¢å¼º / é¢„å¤„ç†\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 2. åŠ è½½ CIFAR-10\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 3. DataLoader\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "train_teacher(epochs=1, train_loader=dataloader)\n",
    "teacher.eval()  # freeze teacher\n",
    "# train_student_with_distillation\n",
    "for x, y in dataloader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    # Teacher forward (ä¸éœ€è¦è®¡ç®—æ¢¯åº¦)\n",
    "    with torch.no_grad():\n",
    "        teacher_logits = teacher(x)\n",
    "\n",
    "    # Student forward\n",
    "    student_logits = student(x)\n",
    "\n",
    "    # Hard label loss\n",
    "    loss_ce = F.cross_entropy(student_logits, y)\n",
    "\n",
    "    # Distillation loss\n",
    "    loss_kd = kd_loss(student_logits, teacher_logits, T)\n",
    "\n",
    "    # Combine\n",
    "    loss = alpha * loss_ce + (1 - alpha) * loss_kd\n",
    "\n",
    "    opt_student.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_student.step()\n",
    "\n",
    "    print(\"loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e01b9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1237823066711425, 'accuracy': 0.1957}\n"
     ]
    }
   ],
   "source": [
    "# Check student performance on test set\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    return {\"loss\": avg_loss, \"accuracy\": acc}\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student.to(device)\n",
    "metrics = evaluate(student, test_loader, device)\n",
    "print(metrics)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea50e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2054711022377014, 'accuracy': 0.9325}\n"
     ]
    }
   ],
   "source": [
    "# Check original teacher performance on test set\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "teacher.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "metrics = evaluate(teacher, test_loader, device)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd97e8",
   "metadata": {},
   "source": [
    "Well the student seems too low in performance. chatGPT suggested common fixes\n",
    "\n",
    "- Train longer: more epochs/steps on CIFAR-10 (both teacher finetune and student distill). One epoch isnâ€™t enough.\n",
    "- Match capacity/aug: consider a slightly larger student (wider convs) or more aug (RandomCrop/Flip). Current 2-layer conv is very small for CIFAR-10.\n",
    "- Hyperparams: try higher LR for student (e.g., 1e-3), adjust Î±/T (e.g., Î±=0.5, T=2â€“4), use weight decay.\n",
    "- Normalize inputs: ensure CIFAR-10 mean/std normalization (e.g., mean=[0.4914,0.4822,0.4465], std=[0.2470,0.2435,0.2616]) instead of only ToTensor.\n",
    "- Optimize loop: use teacher.eval() and student.train(); ensure both on the same device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f98e8",
   "metadata": {},
   "source": [
    "| åå­—           | æ¨¡å—                      | ä½œç”¨     |\n",
    "| ------------ | ------------------------------ | ------ |\n",
    "| `teacher`    | `teacher = resnet18(...)`      | å¤§æ¨¡å‹ï¼Œå†»ç»“ |\n",
    "| `student`    | `student = SmallCNN(...)`      | å°æ¨¡å‹ï¼Œè®­ç»ƒ |\n",
    "| `dataloader` | `dataloader = DataLoader(...)` | æä¾›è®­ç»ƒæ ·æœ¬ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df53884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
