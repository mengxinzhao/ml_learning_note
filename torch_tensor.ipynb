{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683e0f5b",
   "metadata": {},
   "source": [
    "# pytorch Tensor 操作总结\n",
    "## Indexing & Slicing\n",
    "### Basics\n",
    "- x[a:b]：第0维切片\n",
    "\n",
    "- x[:, a:b]：第1维切片\n",
    "\n",
    "- x[..., a:b]：最后一维切片（前面维度全保留）\n",
    "\n",
    "- x[:, -1]：取最后一个元素（降维）\n",
    "\n",
    "- x[:, -1:]：取最后一个元素（保维）\n",
    "\n",
    "**NOTE**: -1 会“降维”，-1: 会“保维”\n",
    "\n",
    "### Bool Mask\n",
    "- x[mask]：按 True 选元素（结果 shape 常不直观，容易变 1D）\n",
    "\n",
    "### Advanced index: select / narrow / index_select\n",
    "- torch.index_select, torch.masked_select, torch.where, torch.take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d0fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1487, 0.7390, 0.8095, 0.8403, 0.8885]])\n",
      "tensor([[0.4682],\n",
      "        [0.9542],\n",
      "        [0.8095],\n",
      "        [0.0101],\n",
      "        [0.4852]])\n",
      "tensor([[0.4790],\n",
      "        [0.9332],\n",
      "        [0.8885],\n",
      "        [0.2712],\n",
      "        [0.0032]])\n",
      "tensor([0.4790, 0.9332, 0.8885, 0.2712, 0.0032])\n",
      "tensor([0.5114, 0.6049, 0.8403, 0.9998, 0.3584])\n",
      "tensor([[0.4682, 0.5114],\n",
      "        [0.9542, 0.6049],\n",
      "        [0.8095, 0.8403],\n",
      "        [0.0101, 0.9998],\n",
      "        [0.4852, 0.3584]])\n",
      "tensor([[0.8249, 0.5114, 0.4790],\n",
      "        [0.0872, 0.6049, 0.9332],\n",
      "        [0.7390, 0.8403, 0.8885],\n",
      "        [0.8078, 0.9998, 0.2712],\n",
      "        [0.5394, 0.3584, 0.0032]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# generate 2d  5 x 5 random tensor\n",
    "t = torch.rand(5, 5)\n",
    "\n",
    "print(t[2:3, :])  # 3rd row\n",
    "print(t[:, 2:3])  # 3rd column\n",
    "print(t[:, -1:])  # last column\n",
    "print(t[:, -1])  # last column, squeezed\n",
    "\n",
    "print(t.select(dim=1, index=3) )         # 等价 t[:, 3]（会降维）\n",
    "print(t.narrow(dim=1, start=2, length=2)) # 等价 t[:, 2:4]\n",
    "idx = torch.tensor([1, 3, 4])  # 1D LongTensor\n",
    "print(t.index_select(dim=1, index=idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390858f",
   "metadata": {},
   "source": [
    "## Shape / View ops（形状变换）\n",
    "### reshape / view / flatten\n",
    "- x.view(...)：不拷贝，要求 memory contiguous（不满足会报错）\n",
    "\n",
    "- x.reshape(...)：尽量不拷贝，不行就拷贝（更稳）\n",
    "\n",
    "- x.flatten(start_dim=1)：从某维起压平（常用在 [B,C,H,W]->[B,CHW]）\n",
    "\n",
    "### unsqueeze / squeeze（加/删长度为1的维）\n",
    "\n",
    "- x.unsqueeze(dim) 或 x[:, None]：插一个 1 维（为 broadcast/gather 做准备）\n",
    "\n",
    "- x.squeeze(dim)：删掉 dim 上长度为1的维（dim 不写会删所有 1 维，容易坑）\n",
    "\n",
    "**NOTE**: `squeeze` removes all dimensions that have a size of 1\n",
    "\n",
    "### transpose / permute / movedim（换维度顺序）\n",
    "\n",
    "- x.transpose(i,j)：交换两个维\n",
    "\n",
    "- x.permute(...)：任意重排维度顺序\n",
    "\n",
    "- x.movedim(src, dst)：把某个维挪到新位置（读起来更语义化）\n",
    "\n",
    "**NOTE**: x = x.permute(...).contiguous().view(...) 因为 permute 往往让 tensor 非连续，要 .contiguous() 才能 .view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b9847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten: torch.Size([8, 4608])\n",
      "view: torch.Size([8, 4608])\n",
      "reshape: torch.Size([8, 4608])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, 3, 32, 48)   # [B, C, H, W] = [8, 3, 32, 48]\n",
    "x_flat = x.flatten(start_dim=1)  # [8, 4608]\n",
    "print(\"flatten:\", x_flat.shape)\n",
    "\n",
    "x_view = x.view(x.size(0), -1)   # [8, 4608]\n",
    "print(\"view:\", x_view.shape)\n",
    "\n",
    "# reshape to same shape trying to avoid copy if possible\n",
    "x_reshape = x.reshape(8, -1)     # [8, 4608]\n",
    "print(\"reshape:\", x_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ea9fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsqueeze: torch.Size([8, 1, 1])\n",
      "squeeze dim1: torch.Size([8, 1])\n",
      "squeeze all: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "v = torch.randn(8, 1)    # [B, 1]\n",
    "\n",
    "v2 = v.unsqueeze(dim=2)      # [8, 1, 1]\n",
    "print(\"unsqueeze:\", v2.shape)\n",
    "\n",
    "v3 = v2.squeeze(dim=1)       # [8, 1]\n",
    "print(\"squeeze dim1:\", v3.shape)\n",
    "\n",
    "v4 = v2.squeeze()        # 删除所有长度为1的轴 -> [8]\n",
    "print(\"squeeze all:\", v4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76215207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permute: torch.Size([8, 32, 48, 3])\n",
      "movedim: torch.Size([8, 32, 48, 3])\n",
      "transpose: torch.Size([8, 3, 48, 32])\n",
      "combo: torch.Size([8, 4608])\n"
     ]
    }
   ],
   "source": [
    "# [B, C, H, W] -> [B, H, W, C]\n",
    "y1 = x.permute(0, 2, 3, 1)\n",
    "print(\"permute:\", y1.shape)\n",
    "\n",
    "y2 = torch.movedim(x, 1, -1)     # same as permute\n",
    "print(\"movedim:\", y2.shape)\n",
    "\n",
    "y3 = x.transpose(2, 3)           # [8, 3, 32, 48] -> [8, 3, 48, 32]\n",
    "print(\"transpose:\", y3.shape)\n",
    "\n",
    "# 典型 CNN flatten 到 linear 的写法：\n",
    "# 非连续: permute 打乱 memory layout\n",
    "y = x.permute(0, 2, 3, 1)        # [8, 32, 48, 3]\n",
    "\n",
    "# 需要 contiguous 让内存重新排布\n",
    "y = y.contiguous()\n",
    "\n",
    "# 再 flatten\n",
    "y = y.view(8, -1)                # [8, 4608]\n",
    "print(\"combo:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915bb39",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "规则：从右往左对齐维度；维度相等或其中一个为 1 才能广播。\n",
    "- x[..., None]：在最后插 1 维\n",
    "\n",
    "- x[None, ...]：在最前插 1 维\n",
    "\n",
    "- x.expand(...)：不拷贝，只是“看起来变大”（stride trick）\n",
    "\n",
    "- x.repeat(...)：真拷贝（更耗内存）\n",
    "\n",
    "看到 None/unsqueeze + expand → 99% 是在做 broadcast 对齐维度。\n",
    "\n",
    "expand is cheap but requires the dimension to be 1 (or added via unsqueeze). If you need real copies, use repeat.\n",
    "\n",
    "- broadcast rules\n",
    "    - 从最右侧维度开始向左比较。\n",
    "\n",
    "    - 两个维度兼容的条件：要么相等，要么其中一个为 1，要么某个张量缺少该维（视为前面补 1）。\n",
    "\n",
    "    - 结果维度取每个位置上的较大值，维度为 1 的那一方会被“拉伸”（不复制数据）。\n",
    "\n",
    "    - 如果某一维既不相等又不为 1，则无法广播，报错。\n",
    "\n",
    "    - 例子：\n",
    "\n",
    "a: (2, 1, 4)\n",
    "\n",
    "b: (1, 3, 4)\n",
    "从右往左比较：4 vs 4（ok），1 vs 3（1 可广播），2 vs 1（1 可广播），所以结果形状是 (2, 3, 4)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be320684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer via None:\n",
      " tensor([[11, 21, 31],\n",
      "        [12, 22, 32],\n",
      "        [13, 23, 33]])\n",
      "outer shape: torch.Size([3, 3])\n",
      "expanded (no copy):\n",
      " tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]])\n",
      "expanded shape: torch.Size([4, 3])\n",
      "expanded shape: torch.Size([2, 5])\n",
      "repeated (copy):\n",
      " tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]])\n",
      "repeated shape: torch.Size([4, 3])\n",
      "a+b shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])          # shape (3,)\n",
    "y = torch.tensor([10, 20, 30])       # shape (3,)\n",
    "\n",
    "# 1) x[..., None]: insert a trailing dim\n",
    "x_col = x[..., None]                 # shape (3,1)\n",
    "# 2) x[None, ...]: insert a leading dim\n",
    "x_row = x[None, ...]                 # shape (1,3)\n",
    "\n",
    "# Broadcast to outer sum via None/unsqueeze\n",
    "outer = x_col + y                    # shapes (3,1) + (3,) -> (3,3)\n",
    "print(\"outer via None:\\n\", outer)\n",
    "print(f\"outer shape: {outer.shape}\")\n",
    "\n",
    "# 3) expand: no copy, just view with stride trick\n",
    "x2 = torch.tensor([1, 2, 3])          # shape (3,)\n",
    "# Make a leading singleton dim, then expand to 4 rows\n",
    "x2_expanded = x2.unsqueeze(0).expand(4, -1)  # shape (4,3), no data copy\n",
    "print(\"expanded (no copy):\\n\", x2_expanded)\n",
    "print(f\"expanded shape: {x2_expanded.shape}\")\n",
    "\n",
    "# Expand a column vector to a full matrix\n",
    "col = torch.tensor([[10], [20]])     # shape (2,1)\n",
    "mat = col.expand(-1, 5)              # shape (2,5)\n",
    "print(f\"expanded shape: {mat.shape}\")\n",
    "\n",
    "# 4) repeat: real copy\n",
    "x2_repeated = x2.unsqueeze(0).repeat(4, 1)   # shape (4,3), data copied\n",
    "print(\"repeated (copy):\\n\", x2_repeated)\n",
    "print(f\"repeated shape: {x2_repeated.shape}\")\n",
    "\n",
    "# Broadcasting with mismatched dims\n",
    "a = torch.randn(2, 1, 4)   # shape (2,1,4)\n",
    "b = torch.randn(1, 3, 4)   # shape (1,3,4)\n",
    "c = a + b                  # broadcast to (2,3,4)\n",
    "print(\"a+b shape:\", c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab39779",
   "metadata": {},
   "source": [
    "## Grid generation（meshgrid：坐标网格/位置）\n",
    "\n",
    "- yy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")， yy.shape == xx.shape == [H, W]\n",
    "\n",
    "- grid = torch.stack([xx, yy], dim=-1) → [H, W, 2]\n",
    "\n",
    "- grid.reshape(-1, 2) → 点集 [H*W, 2]\n",
    "\n",
    "看到 meshgrid/stack → 在造坐标、位置编码、采样网格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ef4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx shape: torch.Size([3, 4])\n",
      "yy shape: torch.Size([3, 4])\n",
      "grid shape: torch.Size([3, 4, 2])\n",
      "grid:\n",
      " tensor([[[-1.0000, -2.0000],\n",
      "         [-0.3333, -2.0000],\n",
      "         [ 0.3333, -2.0000],\n",
      "         [ 1.0000, -2.0000]],\n",
      "\n",
      "        [[-1.0000,  0.0000],\n",
      "         [-0.3333,  0.0000],\n",
      "         [ 0.3333,  0.0000],\n",
      "         [ 1.0000,  0.0000]],\n",
      "\n",
      "        [[-1.0000,  2.0000],\n",
      "         [-0.3333,  2.0000],\n",
      "         [ 0.3333,  2.0000],\n",
      "         [ 1.0000,  2.0000]]])\n",
      "points shape: torch.Size([12, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# coordinate ranges\n",
    "xs = torch.linspace(-1.0, 1.0, steps=4)  # W = 4\n",
    "ys = torch.linspace(-2.0, 2.0, steps=3)  # H = 3\n",
    "\n",
    "# meshgrid with ij indexing (row = y, col = x)\n",
    "yy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n",
    "print(\"xx shape:\", xx.shape)  # (H, W)\n",
    "print(\"yy shape:\", yy.shape)  # (H, W)\n",
    "\n",
    "# stack into a grid of (x,y) pairs: [H, W, 2]\n",
    "# dim=-1 表示在最后一个维度上堆叠\n",
    "grid = torch.stack([xx, yy], dim=-1)\n",
    "print(\"grid shape:\", grid.shape)\n",
    "print(\"grid:\\n\", grid)\n",
    "\n",
    "# flatten to point set: [H*W, 2]\n",
    "points = grid.reshape(-1, 2)\n",
    "print(\"points shape:\", points.shape)\n",
    "# print(\"points:\\n\", points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c258ba",
   "metadata": {},
   "source": [
    "## Gather / Scatter（索引驱动的取值/写回）\n",
    "### gather：按 index “取值”\n",
    "- 沿着某个维度 dim，用 index 给出的坐标，把 input 里的元素“捞”出来；输出的形状由 index 决定。\n",
    "- index 必须是 LongTensor（int64）。\n",
    "- index 和 x 维数相同（rank 相同）。\n",
    "- index 在非 dim 的各维大小要能和输出对齐（需要expand）。\n",
    "- index 的取值必须在 [0, x.size(dim)-1]。\n",
    "\n",
    "例子 tensor =(B, N, C), B=2, N=5, C=3 在 dim=1（N维度） 取K（K=2）个 （每次取都会把一整条C维向量取出来，C=c0, c01, c2）\n",
    "取法 index=[[0, 2], [4, 1]] 也就是说对\n",
    "- batch0 选 n=0 和 n=2\n",
    "- batch1 选 n=4 和 n=1\n",
    "\n",
    "index实际上是（B，K） k代表N维上的坐标。\n",
    "\n",
    "实际取值如下图 batch=0 为例，输出的shape = (B， K， C) = (2, 2, 3)\n",
    "\n",
    "\n",
    "![gather](./image/gather_indices.png)\n",
    "\n",
    "实际做的就是`y[b, k, :] = x[b, idx[b, k], :]`\n",
    "\n",
    "### scatter：按 index “写回/覆盖”\n",
    "\n",
    "out.scatter_(dim, index, src)：把 src 写到 out 指定位置（覆盖）\n",
    "\n",
    "out.scatter_add_(dim, index, src)：写回时做累加（处理重复 index 更自然）\n",
    "\n",
    "直觉\n",
    "\n",
    "- gather：从大张量按索引“捞出来”\n",
    "\n",
    "- scatter：把小张量按索引“塞回去”\n",
    "\n",
    "- scatter_add：塞回去还要“加账”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e66a87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94cd297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([2, 5, 3])\n",
      "idx shape: torch.Size([2, 2])\n",
      "idx.unsqueeze(-1) shape: torch.Size([2, 2, 1])\n",
      "idx_exp shape: torch.Size([2, 2, 3])\n",
      "idx_exp values:\n",
      "tensor([[[0, 0, 0],\n",
      "         [2, 2, 2]],\n",
      "\n",
      "        [[4, 4, 4],\n",
      "         [1, 1, 1]]])\n",
      "y shape: torch.Size([2, 2, 3])\n",
      "y:\n",
      " tensor([[[ 0,  1,  2],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[27, 28, 29],\n",
      "         [18, 19, 20]]])\n",
      "scatter result (cover):\n",
      " tensor([[[1, 1, 1],\n",
      "         [0, 0, 0],\n",
      "         [1, 1, 1],\n",
      "         [0, 0, 0],\n",
      "         [0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0],\n",
      "         [1, 1, 1],\n",
      "         [0, 0, 0],\n",
      "         [0, 0, 0],\n",
      "         [1, 1, 1]]])\n",
      "scatter_add result (add on duplicates):\n",
      " tensor([[[2, 2, 2],\n",
      "         [0, 0, 0],\n",
      "         [2, 2, 2],\n",
      "         [0, 0, 0],\n",
      "         [0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0],\n",
      "         [2, 2, 2],\n",
      "         [0, 0, 0],\n",
      "         [0, 0, 0],\n",
      "         [2, 2, 2]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# gather\n",
    "# x: [B, N, C]  -> 在 dim=1 按 index 取值\n",
    "B, N, C = 2, 5, 3\n",
    "x = torch.arange(B*N*C).reshape(B, N, C)\n",
    "# x.shape = (2, 5, 3)\n",
    "\n",
    "# idx: [B, K]，想在 token 维 (dim=1) 选 K 个\n",
    "K = 2\n",
    "idx = torch.tensor([[0, 2],\n",
    "                    [4, 1]])  # shape (2,2)\n",
    "# 表达的是\n",
    "# batch0：k=0 选 n=0（对 c=0,1,2 都一样），k=1 选 n=2（对 c=0,1,2 都一样）\n",
    "# batch1：k=0 选 n=4（对 c=0,1,2 都一样），k=1 选 n=1（对 c=0,1,2 都一样）\n",
    "\n",
    "# 先扩成和 x 同步的形状，在 dim=1 上用 gather\n",
    "idx_exp = idx.unsqueeze(-1).expand(B, K, C)  # shape  (2,2,1) --> (2,2,3)\n",
    "y = x.gather(dim=1, index=idx_exp)           # y.shape = (2,2,3)\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"idx shape:\", idx.shape)\n",
    "print(f\"idx.unsqueeze(-1) shape:\", idx.unsqueeze(-1).shape)\n",
    "print(\"idx_exp shape:\", idx_exp.shape)\n",
    "print(f\"idx_exp values:\\n{idx_exp}\")\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"y:\\n\", y)\n",
    "\n",
    "# 说明：结果 y 的形状等于 index 的形状 (2,2,3)，其中 dim=1 的长度来自 idx 的 K=2，\n",
    "# 其他维度 (B, C) 与原 x 对齐。\n",
    "\n",
    "# scatter\n",
    "# 将 src 写回 out 的指定位置（覆盖）\n",
    "out = torch.zeros_like(x)  # shape (2,5,3)\n",
    "src = torch.ones_like(y)   # 形状要与 index 对齐 (2,2,3)\n",
    "out.scatter_(dim=1, index=idx_exp, src=src)\n",
    "print(\"scatter result (cover):\\n\", out)\n",
    "\n",
    "# 若 index 有重复，覆盖可能丢信息，可以用 scatter_add_ 做累加\n",
    "out2 = torch.zeros_like(x)\n",
    "src2 = torch.full_like(y, 2.0)\n",
    "out2.scatter_add_(dim=1, index=idx_exp, src=src2)\n",
    "print(\"scatter_add result (add on duplicates):\\n\", out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb266b",
   "metadata": {},
   "source": [
    "idx_exp[b, k, c] 表示：为了得到 y[b,k,c]，沿 dim=1 要去取哪个 n。\n",
    "```text\n",
    "tensor([\n",
    "  [[0, 0, 0],\n",
    "   [2, 2, 2]],\n",
    "\n",
    "  [[4, 4, 4],\n",
    "   [1, 1, 1]]\n",
    "])\n",
    "```\n",
    "- batch 0\n",
    "\n",
    "    - idx_exp[0,0,:] = [0,0,0]\n",
    "\n",
    "    意思是：当 b=0,k=0 时，不管 c=0/1/2，都取 n=0\n",
    "\n",
    "    所以：\n",
    "\n",
    "    y[0,0,0] = x[0,0,0]\n",
    "\n",
    "    y[0,0,1] = x[0,0,1]\n",
    "\n",
    "    y[0,0,2] = x[0,0,2]\n",
    "\n",
    "    - idx_exp[0,1,:] = [2,2,2]\n",
    "\n",
    "    意思是：当 b=0,k=1 时，所有 channel 都取 n=2\n",
    "\n",
    "    所以 y[0,1,:] = x[0,2,:]\n",
    "\n",
    "- batch 1\n",
    "\n",
    "    - idx_exp[1,0,:] = [4,4,4] → y[1,0,:] = x[1,4,:]\n",
    "\n",
    "    - idx_exp[1,1,:] = [1,1,1] → y[1,1,:] = x[1,1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f36acd",
   "metadata": {},
   "source": [
    "## Reduction / Selection（常和 gather/scatter 一起出现）\n",
    "\n",
    "- topk：values, indices = x.topk(k, dim=...)\n",
    "\n",
    "- argmax / max：vals, idx = x.max(dim=...)\n",
    "\n",
    "- sum/mean/min/max：降维聚合（注意 keepdim=True 是否保维）\n",
    "\n",
    "常见链路：\n",
    "\n",
    "topk -> indices -> gather（取对应向量/特征）\n",
    "\n",
    "indices -> scatter/scatter_add（写回 one-hot / routing 权重）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da0d035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:\n",
      " tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519],\n",
      "        [-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]])\n",
      "top_vals:\n",
      " tensor([[ 0.6614,  0.6213],\n",
      "        [ 0.3817, -0.1661]])\n",
      "top_idx:\n",
      " tensor([[0, 3],\n",
      "        [2, 0]])\n",
      "picked_features.shape: torch.Size([2, 2, 3])\n",
      "max_vals: tensor([0.6614, 0.3817])\n",
      "max_idx: tensor([0, 2])\n",
      "max_vals_kd (keepdim):\n",
      " tensor([[0.6614],\n",
      "        [0.3817]])\n",
      "one_hot:\n",
      " tensor([[1., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 0.]])\n",
      "routing (after scatter_add):\n",
      " tensor([[0.4903, 0.0000, 0.0000, 0.5730, 0.0000],\n",
      "        [0.1452, 0.0000, 0.1205, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "batch, items, feat_dim = 2, 5, 3\n",
    "scores = torch.randn(batch, items)            # e.g., logits per item\n",
    "features = torch.randn(batch, items, feat_dim)  # feature vector per item\n",
    "\n",
    "# topk -> indices -> gather (pick top-k features)\n",
    "k = 2\n",
    "# within each batch row, pick the top‑k items\n",
    "top_vals, top_idx = scores.topk(k, dim=1)\n",
    "# selects along the items dimension using those indices, pulling out the feature vectors for the top‑k items.\n",
    "# top_idx.unsqueeze(-1) → [batch, k, 1], so each index has a slot for the feature dimension.\n",
    "# .expand(-1, -1, feat_dim) → [batch, k, feat_dim], replicating each index across the feature dimension (no extra memory).\n",
    "# picked_features has shape [batch, k, feat_dim] containing the feature vectors corresponding to the top‑k items for each batch row.\n",
    "picked_features = features.gather(\n",
    "    1, top_idx.unsqueeze(-1).expand(-1, -1, feat_dim)\n",
    ")\n",
    "\n",
    "# argmax / max (with and without keepdim)\n",
    "max_vals, max_idx = scores.max(dim=1)              # shape: [batch]\n",
    "max_vals_kd, max_idx_kd = scores.max(dim=1, keepdim=True)  # shape: [batch, 1]\n",
    "\n",
    "# indices -> scatter (one-hot) and scatter_add (accumulate weights)\n",
    "one_hot = torch.zeros_like(scores)\n",
    "one_hot.scatter_(1, top_idx, 1.0)  # write ones at top-k positions\n",
    "\n",
    "weights = torch.rand(batch, k)     # routing weights for each top-k index\n",
    "routing = torch.zeros_like(scores)\n",
    "routing.scatter_add_(1, top_idx, weights)  # add weights back to original slots\n",
    "\n",
    "print(\"scores:\\n\", scores)\n",
    "print(\"top_vals:\\n\", top_vals)\n",
    "print(\"top_idx:\\n\", top_idx)\n",
    "print(\"picked_features.shape:\", picked_features.shape)\n",
    "print(\"max_vals:\", max_vals)\n",
    "print(\"max_idx:\", max_idx)\n",
    "print(\"max_vals_kd (keepdim):\\n\", max_vals_kd)\n",
    "print(\"one_hot:\\n\", one_hot)\n",
    "print(\"routing (after scatter_add):\\n\", routing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3249f",
   "metadata": {},
   "source": [
    "## 总结： 三种\"按 index 取值\"的方式：高级索引 vs gather vs take_along_dim\n",
    "\n",
    "### 场景回顾\n",
    "给定 `x: [B, N, C]`，想按某个维度（如 dim=1）用 index `idx: [B, K]` 选出 K 个行，得到 `y: [B, K, C]`。\n",
    "\n",
    "---\n",
    "\n",
    "### A) 高级索引（Advanced Indexing）—— 最直观\n",
    "\n",
    "**核心思想**：就像 NumPy 一样直接\"选行\"。\n",
    "\n",
    "```python\n",
    "b = torch.arange(B)[:, None]  # [B, 1]\n",
    "y = x[b, idx]                 # idx:[B,K] -> y:[B,K,C]\n",
    "```\n",
    "\n",
    "**公式逻辑**：\n",
    "$$y[b,k,:] = x[b, \\text{idx}[b,k], :]$$\n",
    "\n",
    "**优点**\n",
    "- 最像\"选行\"，最容易读懂\n",
    "- 不需要 `unsqueeze + expand`，代码最简洁\n",
    "\n",
    "**缺点**\n",
    "- 高级索引的 shape 规则比较复杂（混用 slice + 整型索引张量时很绕）\n",
    "- 在某些编译/导出场景（`torch.compile` / ONNX）可控性/可预测性不如 `gather`\n",
    "- 维度重排规则隐晦，容易踩坑\n",
    "\n",
    "---\n",
    "\n",
    "### B) gather(dim=...) —— 最\"通用\"、最规则\n",
    "\n",
    "**核心思想**：严格按 index 张量的形状作为输出形状，逐元素对应取值。\n",
    "\n",
    "```python\n",
    "idx_exp = idx.unsqueeze(-1).expand(B, K, C)  # [B,K] -> [B,K,1] -> [B,K,C]\n",
    "y = x.gather(1, idx_exp)                      # [B,K,C]\n",
    "```\n",
    "\n",
    "**公式逻辑**：\n",
    "$$y[b,k,c] = x[b, \\text{index}[b,k,c], c]$$\n",
    "\n",
    "因此必须构造 `index[b,k,c] \\equiv \\text{idx}[b,k]`（对所有 c 重复）。\n",
    "\n",
    "**优点**\n",
    "- 规则非常硬：**输出 shape = index shape**\n",
    "- 在多维、批量、图编译里更稳定（形状推理清晰）\n",
    "- 不会触发高级索引那套\"维度重排\"规则\n",
    "- 和 `scatter` / `scatter_add` 天然配对（MoE routing、top-k 后操作）\n",
    "\n",
    "**缺点**\n",
    "- 反直觉：需要把 index 做到跟输出同 rank（必须 `unsqueeze + expand`）\n",
    "- 容易在 expand 步骤上迷糊\n",
    "\n",
    "**为什么要 expand？**\n",
    "> `gather` 是逐元素操作，每个输出元素 `y[b,k,c]` 都需要一个对应的 index 位置。\n",
    "> 虽然逻辑上我们只想在 `dim=1` 上索引（用 `idx[b,k]`），但 `gather` 要求 index 和输出同 rank。\n",
    "> 所以必须\"复制\" `idx[b,k]` 到 C 维：`idx.unsqueeze(-1).expand(-1, -1, C)`。\n",
    "\n",
    "---\n",
    "\n",
    "### C) torch.take_along_dim —— \"更像 numpy.take_along_axis\"\n",
    "\n",
    "**核心思想**：API 和 `gather` 相近，但语义更像 NumPy。\n",
    "\n",
    "```python\n",
    "idx_exp = idx.unsqueeze(-1).expand(B, K, C)  # 同样需要 expand\n",
    "y = torch.take_along_dim(x, idx_exp, dim=1)   # [B,K,C]\n",
    "```\n",
    "\n",
    "**优点**\n",
    "- 语义好读：沿某个 dim \"take along\"\n",
    "- 规则接近 `gather`（输出 shape ~ index shape）\n",
    "- 从 NumPy 迁移的人会更熟悉\n",
    "\n",
    "**缺点**\n",
    "- 同样需要把 index 变到和输出同 rank（`[B,K]` → `[B,K,C]`）\n",
    "- 本质和 `gather` 差不多，选型意义不大\n",
    "\n",
    "---\n",
    "\n",
    "### 一句话选型指南（最实用）\n",
    "\n",
    "| 场景 | 推荐 | 理由 |\n",
    "|------|------|------|\n",
    "| 快速验证、想最直观 | **高级索引** | 代码最简洁，最像 NumPy |\n",
    "| 通用模块、形状要稳定 | **gather** | 规则硬、可预测、容易编译导出 |\n",
    "| 和 `scatter` / `scatter_add` 配对 | **gather** | 天然搭配，形状对齐 |\n",
    "| 从 NumPy 迁移、用过 `take_along_axis` | **take_along_dim** | 心智模型一致 |\n",
    "| MoE routing、top-k 后写回 | **gather + scatter** | 这是行业标准 pipeline |\n",
    "\n",
    "---\n",
    "\n",
    "### 什么时候 gather 比高级索引更\"安全\"\n",
    "\n",
    "1. **不想触发复杂 shape 行为**  \n",
    "   混合多个索引张量时，高级索引会自动广播和重排维度，容易出现意想不到的输出 shape。\n",
    "\n",
    "2. **需要\"index 张量决定输出形状\"的强规则**  \n",
    "   例如 MoE 的 routing、softmax 后的 topk 选择。高级索引在这里容易出错。\n",
    "\n",
    "3. **后面要配套 scatter / scatter_add 写回**  \n",
    "   ```python\n",
    "   # gather 取出来\n",
    "   idx = top_indices  # [B, K]\n",
    "   idx_exp = idx.unsqueeze(-1).expand(B, K, C)\n",
    "   picked = x.gather(1, idx_exp)  # [B, K, C]\n",
    "   \n",
    "   # scatter 写回（形状自动对齐）\n",
    "   out = torch.zeros_like(x)\n",
    "   out.scatter_(1, idx_exp, picked)  # [B, N, C]\n",
    "   ```\n",
    "   这套组合在库代码（PyTorch 自己的 MoE、注意力机制、路由模块）里是标配。\n",
    "\n",
    "4. **涉及 torch.compile / ONNX 导出**  \n",
    "   `gather` 的形状推理清晰、在各平台支持度高；高级索引在某些编译器上可能有意想不到的行为。\n",
    "\n",
    "---\n",
    "\n",
    "### 完整对比表\n",
    "\n",
    "| 特性 | 高级索引 | gather | take_along_dim |\n",
    "|------|---------|--------|----------------|\n",
    "| 代码简洁度 | ★★★ | ★ | ★★ |\n",
    "| 易读性 | ★★★ | ★★ | ★★★ |\n",
    "| 形状规则清晰 | ★ | ★★★ | ★★★ |\n",
    "| 编译导出友好 | ★ | ★★★ | ★★★ |\n",
    "| 和 scatter 配对 | ▲ | ★★★ | ★★★ |\n",
    "| 库代码常见度 | ★★ | ★★★ | ★ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a5177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
