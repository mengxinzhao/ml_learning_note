{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375b5de0",
   "metadata": {
    "id": "375b5de0"
   },
   "source": [
    "# Runtime Inference Acceleration 数学基础\n",
    "\n",
    "作者：ChatGPT + Vesper\n",
    "版本：v0.1\n",
    "用途： Runtime Inference Acceleration 的数学速成参考书\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e72a2",
   "metadata": {
    "id": "b15e72a2"
   },
   "source": [
    "## 使用方式 / How to Use\n",
    "\n",
    "- 这是一个 **纯 Markdown + 数学公式** 的 notebook，方便你：\n",
    "  - 在浏览器/JupyterLab 里阅读\n",
    "  - 自己插入代码单元，做数值实验\n",
    "  - 逐章扩写，变成你自己的参考书\n",
    "- 内容结构：\n",
    "  - 第 0 章：全局结构和 9 大数学模块总览\n",
    "  - 第 1–9 章：分别展开每个模块，强调与 **Runtime Inference** 的直接关系\n",
    "  - 附录：符号表、练习建议\n",
    "\n",
    "---\n",
    "\n",
    "> 标注约定：  \n",
    "> - **英文字母为主语义，中文解释为直观补充**  \n",
    "> - 公式使用 LaTeX，可以在 Jupyter 中正常渲染  \n",
    "> - 工程相关处会用 `【工程视角】` 特别标记\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd04212",
   "metadata": {
    "id": "9fd04212"
   },
   "source": [
    "# 第 0 章：整体视图 —— Runtime Inference 的 9 大数学支柱\n",
    "\n",
    "在现代推理系统中（LLM、Transformer、CNN、语音模型等），涉及到的数学大致可以归纳为 9 大类：\n",
    "\n",
    "1. **线性代数（Linear Algebra）**  \n",
    "   - 矩阵乘法（GEMM）、SVD、低秩分解、卷积表达\n",
    "2. **数值优化（Numerical Optimization）**  \n",
    "   - 剪枝、量化、蒸馏背后的优化目标与约束\n",
    "3. **近似理论（Approximation Theory）**  \n",
    "   - 量化噪声建模、激活函数近似、cheap ops 替代\n",
    "4. **概率与统计（Probability & Statistics）**  \n",
    "   - KL、温度缩放、分布拟合、分位数剪裁\n",
    "5. **信息论（Information Theory）**  \n",
    "   - 熵、编码、压缩极限、rate–distortion 视角\n",
    "6. **信号处理与卷积数学（Signal Processing）**  \n",
    "   - FFT、Winograd、Toeplitz/Circulant 结构\n",
    "7. **计算图与图论（Computational Graph & Graph Theory）**  \n",
    "   - DAG、图重写、kernel fusion、memory planning\n",
    "8. **数值稳定性与复杂度（Numerical Stability & Complexity）**  \n",
    "   - 浮点误差、softmax 稳定写法、复杂度分析\n",
    "9. **硬件相关数学（Hardware-Aware Math）**  \n",
    "   - Roofline 模型、tiling、SIMD/FMA、GEMM 内核\n",
    "        |\n",
    "\n",
    "这些模块之间的关系可以大致用一张“心智地图”（文字版）表示：\n",
    "\n",
    "```text\n",
    "              ┌─────────┐\n",
    "              │ 线性代数 │  ← 关键：GEMM / SVD / Conv=MatMul\n",
    "              └────┬────┘\n",
    "                   │\n",
    "          ┌────────┴─────────┐\n",
    "          ▼                  ▼\n",
    "   数值优化(剪枝/量化)   近似理论(激活/量化误差)\n",
    "          │                  │\n",
    "          ├────────┬─────────┤\n",
    "          ▼        ▼         ▼\n",
    "       概率统计  信息论   信号处理(Conv/FFT)\n",
    "          │                  │\n",
    "          └────────┬─────────┘\n",
    "                   ▼\n",
    "          计算图 & 图论 (fusion, scheduling)\n",
    "                   │\n",
    "                   ▼\n",
    "     数值稳定性 & 硬件数学 (FP16/FP8, Roofline, tiling)\n",
    "```\n",
    "\n",
    "> 本 notebook 后续章节将按照这个结构展开。  \n",
    "> 第 1 章从 **线性代数** 开始，因为这是推理加速中最核心、出现频率最高的数学语言。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72dc0f",
   "metadata": {
    "id": "da72dc0f"
   },
   "source": [
    "# 第 1 章：线性代数（Linear Algebra for Runtime Inference）\n",
    "\n",
    "## 1.1 线性代数在推理加速中的角色\n",
    "\n",
    "在深度学习推理中，绝大部分“重计算”都可以抽象为：\n",
    "\n",
    "- 向量内积（dot product）\n",
    "- 矩阵–向量乘法（GEMV）\n",
    "- 矩阵–矩阵乘法（GEMM）\n",
    "- 卷积经变换后变成的 GEMM（im2col / Winograd）\n",
    "\n",
    "例如：\n",
    "\n",
    "- 全连接层（Fully Connected）：  \n",
    "  $y = W x + b$\n",
    "- Transformer MLP：  \n",
    "  $Y = \\sigma(X W_1 + b_1) W_2 + b_2$\n",
    "- 自注意力（Self-Attention）：  \n",
    "  $Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V$  \n",
    "  $A = \\text{softmax}(QK^\\top / \\sqrt{d_k})$  \n",
    "  $O = A V$\n",
    "\n",
    "**【工程视角】**  \n",
    "推理加速库（cuBLAS、CUTLASS、MKL、QNNPACK、TensorRT 等）的核心任务，就是：\n",
    "\n",
    "> 让这些矩阵/向量运算在特定硬件上尽可能高效地执行。\n",
    "\n",
    "理解线性代数 → 理解：\n",
    "\n",
    "- 为什么所有人都在说 GEMM\n",
    "- 为什么 low-rank / SVD / LoRA 能够“白嫖”加速\n",
    "- 为什么 Conv2d 经常被变成 GEMM\n",
    "- 为什么 Attention 的优化集中在 QKᵀ / AV 上\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 从内积到 GEMM：推理的基本算子\n",
    "\n",
    "### 1.2.1 内积（dot product）\n",
    "\n",
    "给定两个向量 $a,b \\in \\mathbb{R}^d$，它们的内积为：\n",
    "\n",
    "$$\n",
    "\\langle a, b \\rangle = \\sum_{i=1}^d a_i b_i\n",
    "$$\n",
    "\n",
    "\n",
    "这是最小的“乘–加”单元。\n",
    "\n",
    "**【硬件视角】**  \n",
    "现代 CPU/GPU/NPU 都有 **FMA (Fused Multiply-Add)** 指令：\n",
    "\n",
    "$$\n",
    "\\text{FMA}(a,b,c) = a \\times b + c\n",
    "$$\n",
    "\n",
    "\n",
    "dot product 可以写成：\n",
    "\n",
    "```text\n",
    "acc = 0\n",
    "for i in range(d):\n",
    "    acc = fma(a[i], b[i], acc)\n",
    "```\n",
    "\n",
    "- FMA 是矩阵乘法的底层原语（primitive）\n",
    "- 越能让硬件长时间“刷 FMA”，推理吞吐越高\n",
    "- 数学上的意义：\n",
    "$\\text{round}\\big(\\text{round}(a\\times b) + c \\big)\n",
    "\\neq\n",
    "\\text{round}(a\\times b + c)$\n",
    "FMA 使用右边的（更精确）。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2.2 矩阵乘法（GEMM）\n",
    "\n",
    "GEMM 的一般形式：\n",
    "\n",
    "$$\n",
    "C = \\alpha A B + \\beta C\n",
    "$$\n",
    "\n",
    "\n",
    "其中：\n",
    "\n",
    "- $A \\in \\mathbb{R}^{M \\times K}$\n",
    "- $B \\in \\mathbb{R}^{K \\times N}$\n",
    "- $C \\in \\mathbb{R}^{M \\times N}$\n",
    "- $\\alpha, \\beta$ 为标量（常见情况：$\\alpha=1, \\beta=0$）\n",
    "\n",
    "展开单个元素：\n",
    "\n",
    "$$\n",
    "c_{ij} = \\alpha \\sum_{k=1}^K a_{ik} b_{kj} + \\beta c_{ij}^{\\text{(old)}}\n",
    "$$\n",
    "\n",
    "\n",
    "这就是：**多次 FMA 的二维版本**。\n",
    "\n",
    "**【工程视角】**  \n",
    "- 全连接层 = 多个 GEMM\n",
    "- Attention = 2 个大 GEMM（QKᵀ 和 AV）+ 若干小算子\n",
    "- Conv2d 经过变换后 = GEMM\n",
    "- 推理加速的大头，就是把不同算子统一映射到高效 GEMM 内核上\n",
    "小结\n",
    "| 网络组件            | 数学形式     | 最终变成          |\n",
    "| --------------- | -------- | ------------- |\n",
    "| 全连接层 FC         | Wx + b   | GEMM          |\n",
    "| 卷积 conv2d       | K * X    | im2col → GEMM |\n",
    "| Attention QKᵀ   | QKᵀ      | GEMM          |\n",
    "| Attention AV    | AV       | GEMM          |\n",
    "| Transformer MLP | XW₁, XW₂ | GEMM          |\n",
    "| Embedding       | 查表       | 选行（类似 GEMM）   \n",
    "\n",
    "附： 为什么说“Embedding 类似 GEMM”？\n",
    "\n",
    "虽然 数学上 Embedding 是选行，\n",
    "但它可以看成一个非常稀疏的矩阵乘法。\n",
    "给一个 embedding matrix（词向量矩阵）\n",
    "$E \\in \\mathbb{R}^{V \\times D}$\n",
    "- V = vocabulary size（如 50k）\n",
    "\n",
    "- D = hidden dimension（如 1024）\n",
    "\n",
    "每一行 $𝐸_i$ 就是一个 token 的向量。当一个 token 的 ID = i 时，embedding 做的事情是：$\\text{Embedding}(i) = E_{i}$\n",
    "没有乘法，没有矩阵运算，所以 Embedding 的 FLOPs ~ 0（几乎 0）。\n",
    "真正的成本是：\n",
    "\n",
    "- 内存访问（memory bandwidth）\n",
    "\n",
    "- 随机访问（random access）\n",
    "\n",
    "- cache miss（embedding layer 很容易 miss cache）\n",
    "\n",
    "假设 token id = 3， 这时可以构造一个 size V 的 one-hot 向量：$x = [0,0,0,1,0,0,\\dots]^T \\in \\mathbb{R}^V$\n",
    "\n",
    "Embedding 的输出就是：$y = x^T E \\in \\mathbb{R}^D$\n",
    "\n",
    "展开来看：$y = \\sum_{i=1}^V x_i E_i$\n",
    "\n",
    "但因为只有一个 $X_i = 1$，其余全是 0： $y = E_3$\n",
    "\n",
    "这本质上是一种特殊的 GEMM：$Y = X E$\n",
    "其中 X 是 batch 个 one-hot 向量。\n",
    "\n",
    "只是：\n",
    "\n",
    "X 中绝大部分值是 0， X 只有一个 1， 所以矩阵乘法退化成“选行”.\n",
    "\n",
    "这么理解的好处：\n",
    "Embedding 从数学上可以视为 GEMM 的特例（稀疏 GEMM）\n",
    "\n",
    "这让我们可以：\n",
    "\n",
    "- 使用矩阵观点分析它\n",
    "\n",
    "- 使用 GEMM 的 layout、tiling 理解其优化\n",
    "\n",
    "- 把 embedding 也当成一个“矩阵算子”，可以融合、量化、cache友好化\n",
    "\n",
    "总结一下，Embedding 的瓶颈不是算力（不是 FMA），而是：\n",
    "\n",
    "- 内存带宽（bandwidth）\n",
    "\n",
    "- 随机访问（random access patterns）\n",
    "\n",
    "- cache line miss\n",
    "\n",
    "因此：\n",
    "\n",
    "常见看到优化策略是：\n",
    "\n",
    "- 把 embedding matrix rearrange（行压缩）\n",
    "\n",
    "- 把常用 token 提前放在 cache-friendly 区域\n",
    "\n",
    "- 更小 embedding（量化、减少维度）\n",
    "\n",
    "- batching lookups\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2.3 张量运算：线性代数的多维版本\n",
    "\n",
    "张量（tensor）本质就是多维数组，常见是 rank-3 / rank-4：\n",
    "\n",
    "- rank-3：$B \\times T \\times D$（batch × seq_len × hidden_dim）\n",
    "- rank-4：$B \\times C \\times H \\times W$（用于卷积）\n",
    "\n",
    "张量运算（einsum、batched matmul）最终都会被分解成：\n",
    "\n",
    "- 一堆 reshape / transpose\n",
    "- 若干矩阵乘法（batched GEMM）\n",
    "- 再 reshape 回想要的形状\n",
    "\n",
    "> 所以“学线性代数”不仅是学矩阵，还要学会**如何把高维张量 reshape 成矩阵**以便使用高效 GEMM。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050fee9",
   "metadata": {
    "id": "1050fee9"
   },
   "source": [
    "## 1.3 低秩近似（Low-Rank Approximation）与 SVD\n",
    "\n",
    "### 1.3.1 问题背景\n",
    "\n",
    "给定一个大的权重矩阵 $W \\in \\mathbb{R}^{m \\times n}$，例如：\n",
    "\n",
    "- Transformer MLP 的权重（如 4096 × 11008）\n",
    "- Attention 中的投影矩阵（如 4096 × 4096）\n",
    "\n",
    "在很多实际模型中，$W$ 的“有效秩（effective rank）”远小于 $\\min(m,n)$：\n",
    "- 也就是说，“信息”集中在少数几个方向上。\n",
    "\n",
    "这时我们希望用一个秩为 $k \\ll \\min(m,n)$ 的矩阵 $W_k$ 来近似 $W$：\n",
    "\n",
    "$$\n",
    "\\min_{\\text{rank}(X) \\le k} \\ \\|W - X\\|_F\n",
    "$$\n",
    "\n",
    "\n",
    "这个问题的解析解由 **奇异值分解（SVD）** 给出。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.2 奇异值分解（SVD）的形式\n",
    "\n",
    "$$\n",
    "W = U \\Sigma V^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$：列向量为左奇异向量\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$：列向量为右奇异向量\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$：对角线为奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$，其余为 0\n",
    "\n",
    "截断到前 $k$ 个奇异值：\n",
    "\n",
    "$$\n",
    "W_k = U_k \\Sigma_k V_k^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "- $U_k \\in \\mathbb{R}^{m \\times k}$\n",
    "- $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$\n",
    "- $V_k \\in \\mathbb{R}^{n \\times k}$\n",
    "\n",
    "这是一个秩为 $k$ 的矩阵。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.3 最优低秩近似定理（Eckart–Young–Mirsky）\n",
    "\n",
    "**定理：**  \n",
    "在 Frobenius 范数下，秩不超过 $k$ 的矩阵中离 $W$ 最近的是 $W_k$：\n",
    "\n",
    "$$\n",
    "W_k = \\arg\\min_{\\text{rank}(X) \\le k} \\|W - X\\|_F\n",
    "$$\n",
    "\n",
    "\n",
    "而误差大小刚好等于被截掉的奇异值的平方和：\n",
    "\n",
    "$$\n",
    "\\|W - W_k\\|_F^2 = \\sum_{i=k+1}^r \\sigma_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "**【直观解释】**  \n",
    "奇异值越大，对矩阵“能量”的贡献越大；截断后，丢掉的能量恰好是对应奇异值平方的总和。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.4 用两个小矩阵实现低秩近似\n",
    "\n",
    "在工程中，为了减少 FLOPs，通常把 $W_k$ 拆成两个更小的矩阵相乘：\n",
    "\n",
    "$$\n",
    "W_k = A B, \\quad A \\in \\mathbb{R}^{m \\times k}, \\ B \\in \\mathbb{R}^{k \\times n}\n",
    "$$\n",
    "\n",
    "\n",
    "一种常见构造是：\n",
    "\n",
    "$$\n",
    "A = U_k \\Sigma_k^{1/2}, \\quad B = \\Sigma_k^{1/2} V_k^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "于是：\n",
    "\n",
    "$$\n",
    "W x \\approx W_k x = A (B x)\n",
    "$$\n",
    "\n",
    "\n",
    "- 原始计算：$W x$ 是一个 $m \\times n$ 的矩阵–向量乘法，复杂度 $O(mn)$\n",
    "- 分解后：\n",
    "  - 先算 $z = B x$（维度 $k$）\n",
    "  - 再算 $A z$\n",
    "  - 总复杂度 $O(kn + mk)$\n",
    "\n",
    "如果 $k \\ll \\min(m,n)$，总体 FLOPs 大幅减少。\n",
    "\n",
    "**【工程视角】**  \n",
    "- 这就是很多 **SVD-based compression / low-rank factorization / LoRA** 内核的数学基础。\n",
    "- 在模型部署时，你会看到某些层被替换成两层小的 Linear：`Linear(d_in → k)` + `Linear(k → d_out)`。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.5 文字图示（矩阵结构的可视化）\n",
    "\n",
    "原始矩阵 $W$：\n",
    "\n",
    "```text\n",
    "W (m×n)\n",
    "+------------------------+\n",
    "| █ █ █ █ █ █ █ █ █ █ █  |\n",
    "| █ █ █ █ █ █ █ █ █ █ █  |\n",
    "| █ █ █ █ █ █ █ █ █ █ █  |\n",
    "| █ █ █ █ █ █ █ █ █ █ █  |\n",
    "|         ···            |\n",
    "+------------------------+\n",
    "```\n",
    "\n",
    "低秩分解后：\n",
    "\n",
    "```text\n",
    "A (m×k)          B (k×n)\n",
    "+------+       +------------------+\n",
    "| █ █ |       | █ █ █ █ █ █ █ █   |\n",
    "| █ █ |   x   | █ █ █ █ █ █ █ █   |\n",
    "| █ █ |       |        ···        |\n",
    "| █ █ |       +------------------+\n",
    "+------+\n",
    "```\n",
    "\n",
    "推理时：\n",
    "\n",
    "1. 先计算 $z = B x$（维度 k）  \n",
    "2. 再计算 $y = A z$\n",
    "\n",
    "当 k 比 m,n 小很多时，这是显著的加速。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.6 在推理加速中的典型用法\n",
    "\n",
    "1. **MLP 权重低秩分解**  \n",
    "   - 原始：4096×11008 的巨矩阵  \n",
    "   - 分解：4096×k 和 k×11008，k=1024 或更小  \n",
    "2. **LoRA（Low-Rank Adaptation）**  \n",
    "   - 把更新量 $\\Delta W$ 参数化为 $A B$，rank 通常很小（如 8、16）  \n",
    "   - 对部署来说，推理时多了两层小矩阵乘法\n",
    "3. **KV Cache 压缩 / 投影**  \n",
    "   - 用低秩映射降低 KV 维度，从而减少内存和带宽\n",
    "4. **结构化剪枝的替代方案**  \n",
    "   - 相比直接硬剪通道，低秩分解是更平滑的维度削减方式\n",
    "\n",
    "> 在这类做边缘推理的场景中：  \n",
    "> - 你会希望在 **不大幅降精度** 的前提下，**显著减少 MLP 和投影层的 FLOPs 和参数量**，低秩分解是最主流的数学工具之一。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fdd01",
   "metadata": {
    "id": "787fdd01"
   },
   "source": [
    "## 1.4 矩阵范数与误差度量\n",
    "\n",
    "在做任何压缩（低秩、剪枝、量化）时，你都需要一个“度量标准”来衡量：\n",
    "\n",
    "- 压缩前后的权重差异有多大？\n",
    "- 这会带来多大的输出误差？\n",
    "\n",
    "矩阵范数提供了这些度量工具。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.1 Frobenius 范数（整体能量）\n",
    "\n",
    "$$\n",
    "\\|W\\|_F = \\sqrt{\\sum_{i,j} w_{ij}^2}\n",
    "$$\n",
    "\n",
    "\n",
    "对应于把矩阵当成一个长向量后的 $L_2$ 范数。\n",
    "\n",
    "- 它衡量整体“能量”（energy）\n",
    "- 在 SVD 中有非常漂亮的性质：\n",
    "\n",
    "$$\n",
    "\\|W\\|_F^2 = \\sum_{i=1}^r \\sigma_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "而对低秩近似：\n",
    "\n",
    "$$\n",
    "\\|W - W_k\\|_F^2 = \\sum_{i=k+1}^r \\sigma_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "**【工程应用】**  \n",
    "- 可以通过奇异值快速估计压缩误差的上界\n",
    "- 可以比较不同 $k$ 值下的误差，做 bit–accuracy tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.2 谱范数（operator norm / 2-范数）\n",
    "\n",
    "$$\n",
    "\\|W\\|_2 = \\sigma_{\\max}(W)\n",
    "$$\n",
    "\n",
    "\n",
    "- 等于最大的奇异值\n",
    "- 反映：$W$ 作为线性算子能把向量放大的最大比例\n",
    "\n",
    "具体来说：\n",
    "\n",
    "$$\n",
    "\\|W\\|_2 = \\max_{\\|x\\|_2 = 1} \\|W x\\|_2\n",
    "$$\n",
    "\n",
    "\n",
    "**【工程意义】**  \n",
    "- 如果 $\\|W\\|_2$ 非常大，则对输入的小扰动非常敏感  \n",
    "**解释**：当矩阵的谱范数（spectral norm），即 $|W|_2$ 非常大时，它意味着这个矩阵作为一个线性变换，能够将输入向量“放大”的最大比例非常大。\n",
    "\n",
    "谱范数的定义是：\n",
    "\n",
    "$$\n",
    "\\|W\\|_2 = \\max_{\\|x\\|_2 = 1} \\|W x\\|_2\n",
    "$$\n",
    "\n",
    "这意味着，对于任何非零向量 $x$，\n",
    "\n",
    "$$|W x|_2 \\le |W|_2 |x|_2$$\n",
    "\n",
    "现在，考虑一个输入向量 $x_0$ 受到一个小扰动 $\\Delta x$。那么，输出的改变将是 $W(x_0 + \\Delta x) - W x_0 = W \\Delta x$。\n",
    "\n",
    "输出扰动的范数是 $|W \\Delta x|_2$。根据上面的不等式，我们可以得到：\n",
    "\n",
    "$$|W \\Delta x|_2 \\le |W|_2 |\\Delta x|_2$$\n",
    "\n",
    "如果 $|W|_2$ 非常大，即使输入的扰动 $|\\Delta x|_2$ 非常小，经过矩阵 $W$ 变换后，输出的扰动 $|W \\Delta x|_2$ 也会被放大很多倍。这意味着原始输入中微小的误差或噪声，在经过这个矩阵层之后，可能会被显著放大，导致输出结果的剧烈变化。\n",
    "\n",
    "总结来说：\n",
    "\n",
    "大 $|W|_2$ 意味着强放大效应：矩阵 $W$ 有能力将某些方向上的输入向量放大很多倍。\n",
    "误差传播：当输入存在小扰动时（比如量化误差、浮点数误差或传感器噪声），这个扰动会被 $|W|_2$ 的大小所放大，从而导致最终输出的误差也很大。\n",
    "在推理系统中，尤其是在量化或低精度计算时，如果模型的某个权重矩阵的谱范数过大，那么该层将对量化噪声、数值舍入误差非常敏感，容易导致精度大幅下降或输出不稳定。因此，在模型设计或优化过程中，有时会通过正则化等方式来限制权重的谱范数，以提高模型的鲁棒性和数值稳定性。\n",
    "- 在量化/剪枝时，这种层更容易出现数值不稳定 / 输出来回抖动  \n",
    "- 有些鲁棒性分析会用谱范数做 Lipschitz 常数的上界\n",
    "\n",
    "**解释**：\n",
    "\n",
    "*什么是 Lipschitz 常数*： 在数学中，一个函数 $f: X \\to Y$ 是 Lipschitz 连续的，如果存在一个常数 $K \\ge 0$，使得对于 $X$ 域中的任意两个点 $x_1$ 和 $x_2$，都有：\n",
    "\n",
    "$$\\|f(x_1) - f(x_2)\\| \\le K \\|x_1 - x_2\\|$$\n",
    "这个常数 $K$ 就被称为该函数的 Lipschitz 常数。这里的 $\\|\\cdot\\|$ 表示某种范数（例如向量的欧几里得范数或矩阵的谱范数）。\n",
    "\n",
    "*直观理解*： Lipschitz 常数 $K$ 给出了函数 “最陡峭” 的程度。它限制了函数值变化的速率：\n",
    "\n",
    "如果 $K$很小，函数是“平缓”的，输出的变化不会比输入的微小变化大太多。它像一个斜率有限的函数。\n",
    "如果 $K$很大，函数可能是“陡峭”的，输出的变化可能比输入的微小变化大很多。这意味着函数对输入的小扰动非常敏感。\n",
    "\n",
    "*数值稳定性与误差传播*：如果一个矩阵（可以看作一个线性函数）的谱范数很大，那么它会将输入的微小误差放大很多倍。一个线性函数 $f(x) = Wx$ 的 Lipschitz 常数就是它的谱范数 $|W|_2$。\n",
    "对于非线性函数（如激活函数、整个神经网络），其 Lipschitz 常数限制了输入噪声或量化误差在网络中传播时的最大放大倍数。\n",
    "低 Lipschitz 常数 的模型通常具有更好的 数值稳定性和对抗样本的鲁棒性。\n",
    "\n",
    "*模型压缩与量化*：在量化或剪枝过程中，我们会对模型参数引入微小的扰动（量化误差）。如果某一层或整个模型的 Lipschitz 常数过大，这些微小扰动就可能被显著放大，导致模型精度急剧下降。\n",
    "因此，在进行模型压缩时，有时会关注保持或限制模型的 Lipschitz 常数，以确保压缩后的模型仍然稳定且性能良好。在实践中，直接计算复杂深度学习模型的精确 Lipschitz 常数通常非常困难（NP-hard）。\n",
    "但可以通过一些方法来 估计其上界 或 正则化 来间接限制它，例如：谱范数正则化 (Spectral Normalization)：限制神经网络中权重矩阵的谱范数，从而限制线性层的 Lipschitz 常数。 某些激活函数（如 ReLU）是 1-Lipschitz 的，有助于控制传播的放大。Batch Normalization 或 Layer Normalization 也有助于稳定激活的分布，间接控制函数的行为。总而言之，Lipschitz 常数是衡量函数平滑度和对输入扰动敏感程度的一个重要指标。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.3 $L_1$ 和 $L_0$ 范数（稀疏性）\n",
    "\n",
    "- 元素级 $L_1$：\n",
    "\n",
    "$$\n",
    "\\|W\\|_1 = \\sum_{i,j} |w_{ij}|\n",
    "$$\n",
    "\n",
    "\n",
    "- “$L_0$”（非零个数）：\n",
    "\n",
    "$$\n",
    "\\|W\\|_0 = \\#\\{(i,j) : w_{ij} \\ne 0\\}\n",
    "$$\n",
    "\n",
    "\n",
    "**【工程视角】**  \n",
    "\n",
    "- $L_0$ 直接度量“有多少元素被保留/剪掉”，但对应的优化问题是 NP-hard\n",
    "- 通常用 $L_1$ 作为凸近似：\n",
    "\n",
    "$$\n",
    "L(W) + \\lambda \\|W\\|_1\n",
    "$$\n",
    "\n",
    "\n",
    "  训练后很多元素自然趋近于 0，方便做后处理剪枝。\n",
    "\n",
    "**【具体解释】**\n",
    "1. $L_0$ 范数：理想但难优化\n",
    "定义：$\\|W\\|_0$ 严格来说不是一个范数，它表示矩阵 $W$ 中 非零元素的个数。\n",
    "目标：在剪枝中，我们希望最小化非零元素的数量，从而让模型变得稀疏，这样可以减少存储和计算。所以优化目标可能是这样的：\n",
    "$$\n",
    "\\min_{\\widehat{W}} \\; L(\\widehat{W}) \\;+\\; \\lambda \\,\\|\\widehat{W}\\|_{0}\n",
    "$$\n",
    "\n",
    "问题：$L_0$ 范数是非凸、不连续的，这意味着它没有良好的数学性质，导致优化问题是 NP-hard 的（即计算上非常困难，没有高效的全局最优解算法）。你无法直接使用梯度下降等基于梯度的优化方法来最小化它，因为它的梯度几乎处处为零或不存在。\n",
    "\n",
    "2. $L_1$ 范数：凸且促进稀疏性\n",
    "定义：$\\|W\\|1$ 表示矩阵 $W$ 中所有元素的 绝对值之和：\n",
    "$$\n",
    "\\|W\\|_1 = \\sum_{i,j} |w_{ij}|\n",
    "$$\n",
    "\n",
    "性质：\n",
    "\n",
    "凸性：$L_1$ 范数是一个 凸函数。这意味着当我们将它作为正则项加入损失函数时，整个优化问题（如果是凸损失函数）将仍然是凸的，或者至少是更容易优化的。我们可以使用梯度下降及其变种（例如次梯度下降，因为 $|x|$ 在 $x=0$ 处不可导）来找到全局最优解或近似最优解。\n",
    "\n",
    "促进稀疏性：虽然 $L_1$ 范数不直接统计非零元素的个数，但它有一个非常重要的性质：在优化过程中，为了最小化 $\\sum |w_{ij}|$，优化器倾向于将许多权重推向 精确的零，而不是一个非常小但非零的值。这是因为 $|x|$ 在 $x=0$ 附近有一个“尖点”，这使得优化器在接近零时受到更大的惩罚，从而促使权重跨过零点。\n",
    "\n",
    "3. 为什么是“凸近似”？\n",
    "近似 $L_0$ 行为：虽然 $L_1$ 不等于 $L_0$，但它能有效地诱导稀疏性，从而在实践中近似达到我们用 $L_0$ 追求的效果（即得到一个有很多零的模型）。\n",
    "可优化性： $L_1$ 是凸的，这使得优化问题变得可解。它提供了一个数学上更容易处理的替代方案，可以在合理的时间内找到一个好的解。所以，当看到剪枝的优化目标中包含 $L_1$ 正则项时，比如：\n",
    "$$\n",
    "\\min_{\\widehat{W}} \\; L(\\widehat{W}) + \\lambda \\|\\widehat{W}\\|_1\n",
    "$$\n",
    "它的意思是：我们希望在保持模型精度的同时（最小化 $L(\\widehat{W})$），也能让模型的权重尽可能小，并且倾向于零（最小化 $\\lambda |\\widehat{W}|_1$），从而达到稀疏化的目的。这种方法是实践中实现模型稀疏化（尤其是非结构化剪枝）非常常见且有效的方式。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.4 在工程中如何实际使用这些范数？\n",
    "\n",
    "1. **判断某层是否适合低秩分解**  \n",
    "   - 计算奇异值 $\\{\\sigma_i\\}$，看前 k 个是否已经占到整体能量的绝大部分：\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^k \\sigma_i^2}{\\sum_{i=1}^r \\sigma_i^2} \\approx 0.95 \\text{ 或更高}\n",
    "$$\n",
    "\n",
    "\n",
    "2. **设计剪枝策略**  \n",
    "   - 若某层权重的 $L_1$ 分布极不均匀（大量接近 0 的元素），说明存在自然稀疏性 → 可以安全剪枝\n",
    "   - 可以对每一行/每一列计算 $L_1$ 或 $L_2$ norm，作为“通道重要性”的指标\n",
    "\n",
    "3. **量化敏感性分析**  \n",
    "   - 对于谱范数较大的层，可以考虑：\n",
    "     - 使用更高 bit 的量化（例如 8-bit 而不是 4-bit）\n",
    "     - 或做更细致的 per-channel scaling 来减小量化误差\n",
    "\n",
    "> 总结：范数帮助你从“拍脑袋选层/选 rank/选 bit”  \n",
    "> 变成“有指标、有依据的工程决策”。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa23b4",
   "metadata": {
    "id": "9faa23b4"
   },
   "source": [
    "## 1.5 卷积的线性代数表达：Conv = MatMul\n",
    "\n",
    "卷积层（Conv2d）是 CNN 和很多视觉模型的核心。  \n",
    "从数学上，它是一个带有结构的线性算子：\n",
    "\n",
    "$$\n",
    "y = K * x\n",
    "$$\n",
    "\n",
    "\n",
    "- $x$：输入特征图（如 $\\mathbb{R}^{C_{\\text{in}} \\times H \\times W}$）  \n",
    "- $K$：卷积核（如 $\\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times k_h \\times k_w}$）\n",
    "\n",
    "通过适当展开和重排，可以把它改写为**矩阵乘法**。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5.1 im2col：把局部 patch 展成列向量\n",
    "\n",
    "以单通道、kernel=2×2、stride=1 为例：\n",
    "\n",
    "输入：\n",
    "\n",
    "```text\n",
    "1 2 3\n",
    "4 5 6\n",
    "7 8 9\n",
    "```\n",
    "\n",
    "卷积核：\n",
    "\n",
    "```text\n",
    "k11 k12\n",
    "k21 k22\n",
    "```\n",
    "\n",
    "所有 2×2 patch：\n",
    "\n",
    "1. 上左：$\\{1,2,4,5\\}$\n",
    "2. 上右：$\\{2,3,5,6\\}$\n",
    "3. 下左：$\\{4,5,7,8\\}$\n",
    "4. 下右：$\\{5,6,8,9\\}$\n",
    "\n",
    "我们把每个 patch 按固定顺序展开成列向量：\n",
    "\n",
    "$$\n",
    "x_1 = [1,2,4,5]^\\top, \\quad\n",
    "x_2 = [2,3,5,6]^\\top, \\quad\n",
    "x_3 = [4,5,7,8]^\\top, \\quad\n",
    "x_4 = [5,6,8,9]^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "然后组成一个矩阵 $X_{\\text{col}}$：\n",
    "\n",
    "$$\n",
    "X_{\\text{col}} =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 4 & 5 \\\\\n",
    "2 & 3 & 5 & 6 \\\\\n",
    "4 & 5 & 7 & 8 \\\\\n",
    "5 & 6 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{4 \\times 4}\n",
    "$$\n",
    "\n",
    "\n",
    "卷积核也展开成向量：\n",
    "\n",
    "$$\n",
    "w = [k_{11}, k_{12}, k_{21}, k_{22}]\n",
    "$$\n",
    "\n",
    "\n",
    "则每个输出位置：\n",
    "\n",
    "$$\n",
    "y_i = w x_i\n",
    "$$\n",
    "\n",
    "\n",
    "所有位置一起写成矩阵形式：\n",
    "\n",
    "$$\n",
    "y^\\top = w X_{\\text{col}}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5.2 多通道 / 多卷积核的情况\n",
    "\n",
    "对于一般情况：\n",
    "\n",
    "- 输入：$x \\in \\mathbb{R}^{C_{\\text{in}} \\times H \\times W}$\n",
    "- 卷积核：$K \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times k_h \\times k_w}$\n",
    "\n",
    "通过 im2col：\n",
    "\n",
    "- 生成矩阵 $X_{\\text{col}} \\in \\mathbb{R}^{(C_{\\text{in}} k_h k_w) \\times L}$  \n",
    "  其中 L 是滑窗数量（输出空间尺寸的乘积）\n",
    "- 卷积核压平成矩阵 $W \\in \\mathbb{R}^{C_{\\text{out}} \\times (C_{\\text{in}} k_h k_w)}$\n",
    "\n",
    "则卷积可以写成：\n",
    "\n",
    "$$\n",
    "Y = W X_{\\text{col}}\n",
    "$$\n",
    "\n",
    "\n",
    "- $Y \\in \\mathbb{R}^{C_{\\text{out}} \\times L}$，之后 reshape 回输出尺寸\n",
    "\n",
    "**【核心结论】**  \n",
    "> Conv2d 经过 im2col 变换后，本质就是一个 GEMM。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5.3 在推理加速中的意义\n",
    "\n",
    "1. **重用高性能 GEMM 内核**  \n",
    "   - 不需要为每种卷积形状写全新的 kernel  \n",
    "   - 只需实现 im2col（或更高级的变换）+ 调用 GEMM\n",
    "2. **统一优化目标**  \n",
    "   - GEMM 的 tiling、向量化、cache 利用一旦写好，Conv 也能享受\n",
    "3. **更容易做量化/剪枝**  \n",
    "   - 权重在 im2col 形式下就是一个矩阵 W，方便 per-channel / per-row 量化和剪枝\n",
    "\n",
    "> 在高性能推理库中，Conv2d 的实现通常要么：  \n",
    "> - 走 im2col + GEMM 路线，  \n",
    "> - 要么走专门的 convolution kernel，但其内部仍然围绕“块矩阵乘法”的思想设计。\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6 Attention 的线性代数结构\n",
    "\n",
    "以单头 self-attention 为例（不考虑 bias & mask）：\n",
    "\n",
    "$$\n",
    "Q = X W_Q,\\quad\n",
    "K = X W_K,\\quad\n",
    "V = X W_V \\\\\n",
    "A = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) \\\\\n",
    "O = A V\n",
    "$$\n",
    "\n",
    "\n",
    "- $X \\in \\mathbb{R}^{T \\times d_{\\text{model}}}$：输入序列\n",
    "- $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "- $Q,K,V \\in \\mathbb{R}^{T \\times d_k}$\n",
    "- $A \\in \\mathbb{R}^{T \\times T}$\n",
    "- $O \\in \\mathbb{R}^{T \\times d_v}$\n",
    "\n",
    "其中：\n",
    "\n",
    "1. $Q = X W_Q$ 是一个 GEMM（$T \\times d_{\\text{model}}$ 乘 $d_{\\text{model}} \\times d_k$）\n",
    "2. $K = X W_K$、$V = X W_V$ 同理\n",
    "3. $QK^\\top$ 是一个 **非常大的 GEMM**（$T \\times d_k$ 乘 $d_k \\times T$）\n",
    "4. $A V$ 又是一个 GEMM（$T \\times T$ 乘 $T \\times d_v$）\n",
    "\n",
    "**【工程视角】**  \n",
    "- Attention 中真正“贵”的地方就是两个矩阵乘法：$QK^\\top$ 和 $A V$\n",
    "- softmax 本身 FLOPs 不多，但存在数值稳定性的挑战（第 8 章展开）\n",
    "- 所有对 Attention 的加速（如 FlashAttention、各种 kernel fusion）本质都是：\n",
    "  - 减少显存读写\n",
    "  - 改善访问模式\n",
    "  - 在不显式构造 $T \\times T$ 矩阵的前提下，实现等价的线性代数运算\n",
    "\n",
    "> 一旦你站在“线性代数”视角看 Attention，就更容易理解各种加速论文的思路。\n",
    "\n",
    "---\n",
    "\n",
    "## 1.7 Embedding：查表 = 选行 = 稀疏 GEMM\n",
    "\n",
    "Embedding 矩阵：$E \\in \\mathbb{R}^{V \\times D}$\n",
    "\n",
    "- V：词表大小\n",
    "- D：隐层维度 / embedding 维度\n",
    "\n",
    "给定 token id = i，Embedding 的作用就是：\n",
    "\n",
    "$$\n",
    "\\text{Embedding}(i) = E_i\n",
    "$$\n",
    "\n",
    "\n",
    "即：**从矩阵 E 中选取第 i 行**。\n",
    "\n",
    "如果构造一个 one-hot 向量 $x \\in \\mathbb{R}^V$：\n",
    "\n",
    "$$\n",
    "x_j = \\begin{cases}\n",
    "1, & j = i \\\\\n",
    "0, & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "则：\n",
    "\n",
    "$$\n",
    "y = x^\\top E = E_i\n",
    "$$\n",
    "\n",
    "\n",
    "也就是说：\n",
    "\n",
    "> Embedding 在数学上等价于：**one-hot 向量乘 embedding 矩阵的矩阵乘法（一个极其稀疏的 GEMM）**。\n",
    "\n",
    "**【工程视角】**  \n",
    "\n",
    "- 对 Embedding 来说，算力（FMA）几乎不构成瓶颈，因为没有大规模乘加\n",
    "- 真正的瓶颈是：\n",
    "  - 内存访问（随机访问 E 的不同行）\n",
    "  - cache 命中率\n",
    "  - 带宽\n",
    "\n",
    "这解释了为什么：  \n",
    "> 巨大词表的 LLM，在部署时经常是 “embedding 和 LM head 很吃带宽”；  \n",
    "> 而中间层（GEMM）则更吃计算。\n",
    "\n",
    "---\n",
    "\n",
    "## 1.8 数据布局（Memory Layout）与线性代数\n",
    "\n",
    "矩阵在内存中可以有两种常见布局：\n",
    "\n",
    "- row-major（行优先，C-style）\n",
    "- column-major（列优先，Fortran/BLAS-style）\n",
    "\n",
    "对于 $A \\in \\mathbb{R}^{M \\times K}$：\n",
    "\n",
    "- row-major：同一行的元素连续存放\n",
    "- column-major：同一列的元素连续存放\n",
    "\n",
    "这会影响：\n",
    "\n",
    "- 连续访问的方向（stride=1）\n",
    "- 缓存局部性（cache locality）\n",
    "- 是否容易被 SIMD 向量化\n",
    "\n",
    "**【工程视角】**  \n",
    "\n",
    "- 很多 GEMM 内核会要求 A/B/C 矩阵采用特定布局（如 A row-major，B col-major），以便：\n",
    "  - 在 inner-k loop 中连续访问数据\n",
    "  - 利用硬件的向量加载指令（如 `ld1q`、`vmovaps` 等）\n",
    "- 这就是为什么你在 kernel 代码中会看到大量 transpose / layout transform，它们本质是为线性代数运算创建“更友好的 memory layout”。\n",
    "\n",
    "---\n",
    "\n",
    "## 1.9 小结：线性代数是 Runtime Inference 的共同语言\n",
    "\n",
    "本章核心结论：\n",
    "\n",
    "1. **推理的大部分 FLOPs 都来自 GEMM（矩阵乘法）**  \n",
    "   - 全连接 / MLP / Attention projection / Conv2d (im2col) 等\n",
    "2. **低秩近似（SVD）是最重要的压缩数学之一**  \n",
    "   - 通过 rank-k 近似，把大矩阵拆成两个小矩阵乘法\n",
    "3. **矩阵范数给出度量压缩误差的工具**  \n",
    "   - Frobenius、谱范数、L1/L0\n",
    "4. **Conv、Attention、Embedding 都可以在“矩阵视角”下统一看待**\n",
    "5. **数据布局与线性代数紧密相关**  \n",
    "   - Row-major / col-major / tiling 决定是否能高效利用硬件\n",
    "\n",
    "> 接下来几章会在这个基础上，继续引入：  \n",
    "> - 为何这些矩阵要被“优化”（数值优化章节）  \n",
    "> - 为何我们敢于用粗糙的近似（近似理论章节）  \n",
    "> - 如何在概率/信息论框架下理解压缩与量化  \n",
    "> - 如何结合硬件特性真正做到“跑满”推理芯片\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ba67f",
   "metadata": {
    "id": "143ba67f"
   },
   "source": [
    "# 第 2 章：数值优化（Optimization for Pruning, Quantization & Distillation）\n",
    "\n",
    "本章目标：理解剪枝、量化、蒸馏等操作背后的“优化问题”视角。  \n",
    "你不需要成为优化理论专家，但需要：\n",
    "\n",
    "- 看懂常见损失函数与正则项的形式\n",
    "- 理解 L0/L1 稀疏化、量化参数优化的大致思路\n",
    "- 知道多目标优化（精度 vs 延迟）的典型写法\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 剪枝（Pruning）的优化视角\n",
    "\n",
    "### 2.1.1 基本目标\n",
    "\n",
    "给定训练好的权重 $W$、损失函数 $L(W)$，剪枝希望：\n",
    "\n",
    "- 大量元素变为 0（稀疏）\n",
    "- 损失/精度变化尽量小\n",
    "\n",
    "可以形式化为：\n",
    "\n",
    "$$\n",
    "\\min_{\\hat{W}} L(\\hat{W}) + \\lambda \\|\\hat{W}\\|_0\n",
    "$$\n",
    "\n",
    "\n",
    "其中：\n",
    "\n",
    "- $\\|\\hat{W}\\|_0$：非零元素个数（稀疏性度量）\n",
    "- $\\lambda$：权衡“精度 vs 稀疏度”的超参数\n",
    "\n",
    "这个问题一般是 NP-hard，常见近似方式有：\n",
    "\n",
    "1. 用 $L_1$ 范数替代 $L_0$：\n",
    "\n",
    "$$\n",
    "\\min_{\\hat{W}} L(\\hat{W}) + \\lambda \\|\\hat{W}\\|_1\n",
    "$$\n",
    "\n",
    "\n",
    "2. 先正常训练，再做基于某种 saliency 的后处理剪枝：\n",
    "   - 按权重绝对值大小剪掉小值\n",
    "   - 按 Hessian 近似（如 OBS/OBD）计算敏感度\n",
    "\n",
    "**【工程视角】**  \n",
    "- 推理加速更喜欢 **结构化剪枝**（按通道/块/头剪）：\n",
    "  - 便于利用矩阵库和硬件\n",
    "  - 而不是“完全稀疏”的 irregular sparse（不易加速）\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2 结构化剪枝（channel/head pruning）\n",
    "\n",
    "可以把通道剪枝写成：\n",
    "\n",
    "$$\n",
    "\\min_{\\theta, m} \\ L(\\theta \\odot m) + \\lambda \\|m\\|_0\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\theta$：原始参数\n",
    "- $m$：mask，按通道/块为单位取 0 或 1\n",
    "- $\\odot$：逐元素或逐通道乘法\n",
    "\n",
    "通常会：\n",
    "\n",
    "1. 训练一个带有可微近似 mask 的模型（如用 sigmoid/Concrete distribution）\n",
    "2. 在训练过程中逐渐压缩某些通道的权重\n",
    "3. 最后将接近 0 的通道硬剪掉\n",
    "\n",
    "> 本质仍然是“带稀疏约束的优化问题”，只是作用对象从单个元素提升到了“结构化块”。\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 量化（Quantization）的优化视角\n",
    "\n",
    "### 2.2.1 简单的 uniform quantization\n",
    "\n",
    "考虑对权重集合 $\\{w_i\\}$ 做对称均匀量化，bit 宽为 b：\n",
    "\n",
    "- 量化级别：$q_i \\in \\{-Q,\\dots,Q\\}$，其中 $Q = 2^{b-1}-1$\n",
    "- 缩放因子（scale）：$s > 0$\n",
    "- 量化/反量化过程：\n",
    "\n",
    "$$\n",
    "q_i = \\text{round}(w_i / s), \\quad\n",
    "  \\hat{w}_i = s \\cdot q_i\n",
    "$$\n",
    "\n",
    "\n",
    "典型目标是最小化量化误差：\n",
    "\n",
    "$$\n",
    "\\min_s \\sum_i (w_i - \\hat{w}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "这实际上是一个一维凸优化问题。\n",
    "\n",
    "工程上常见 heuristics：\n",
    "\n",
    "- 直接取：\n",
    "\n",
    "$$\n",
    "s = \\frac{\\max_i |w_i|}{Q}\n",
    "$$\n",
    "\n",
    "\n",
    "- 或取某个分位数（如 99.9%）替代 max，防止 outlier 过大：\n",
    "\n",
    "$$\n",
    "s = \\frac{\\text{quantile}_{p}(|w_i|)}{Q}\n",
    "$$\n",
    "\n",
    "\n",
    "**【要点】**  \n",
    "你需要知道：\n",
    "\n",
    "- 量化参数（scale/zero-point）可以通过最小二乘意义下的优化求得\n",
    "- 工程实现中用 histogram + 搜索 / heuristics 做近似求解\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2.2 LSQ（Learned Step Size Quantization）思想\n",
    "\n",
    "更进一步，可以把 scale $s$ 当成可学习参数，让反向传播直接优化：\n",
    "\n",
    "- 定义一个“伪量化”算子（用 STE 逼近梯度）\n",
    "- 在训练/微调时 joint optimize $W$ 和 $s$\n",
    "\n",
    "形式上：\n",
    "\n",
    "$$\n",
    "\\min_{W,s} L(\\text{Quantize}(W; s))\n",
    "$$\n",
    "\n",
    "\n",
    "这种方法使得量化参数针对当前任务/数据集自适应地收敛到较好的值，从而提升低 bit 量化的精度。\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 知识蒸馏（Distillation）的优化视角\n",
    "\n",
    "给定 teacher 模型 $T$ 和 student 模型 $S$：\n",
    "\n",
    "- Teacher 输出：$p^T$\n",
    "- Student 输出：$p^S$\n",
    "\n",
    "常见的 distillation loss：\n",
    "\n",
    "$$\n",
    "L_{\\text{distill}} = \\alpha \\cdot \\text{CE}(y, p^S)\n",
    "+ (1 - \\alpha) \\cdot T^2 \\cdot \\text{KL}\\big(\\sigma(z^T/T) \\,\\|\\, \\sigma(z^S/T)\\big)\n",
    "$$\n",
    "\n",
    "\n",
    "其中：\n",
    "\n",
    "- CE：交叉熵\n",
    "- KL：KL 散度\n",
    "- $T$：温度（temperature）\n",
    "- $\\sigma$：softmax\n",
    "\n",
    "从优化角度看：\n",
    "\n",
    "- 这是一个带有两部分损失的目标函数\n",
    "- 你在“精确拟合 ground-truth 标签”和“模仿 teacher 的软分布”之间做权衡\n",
    "\n",
    "**【与推理加速的关系】**  \n",
    "- Student 模型通常更小、更浅、维度更低\n",
    "- 在边缘设备上部署时会大幅降低延迟与内存占用\n",
    "- 你要理解：蒸馏可以看作是一种“在 teacher 的函数空间附近寻找浅网络逼近”的优化过程\n",
    "\n",
    "---\n",
    "\n",
    "## 2.4 多目标优化：精度 vs 延迟 vs 内存\n",
    "\n",
    "在做部署时，你不会只关心 loss，还会关心：\n",
    "\n",
    "- latency（推理延迟）\n",
    "- memory（显存 / DRAM 占用）\n",
    "- throughput（QPS）\n",
    "\n",
    "可以将这些纳入优化目标：\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\quad \\alpha \\cdot \\text{Error}(\\theta)\n",
    "+ \\beta \\cdot \\text{Latency}(\\theta)\n",
    "+ \\gamma \\cdot \\text{Memory}(\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "其中：\n",
    "\n",
    "- $\\theta$：模型结构 + 量化配置 + 剪枝策略 等\n",
    "- $\\text{Latency}(\\theta)$：通过 profile 或 analytical model 估计\n",
    "- $\\text{Memory}(\\theta)$：由参数量、activation、KV cache 决定\n",
    "\n",
    "工程实现中常见简化：\n",
    "\n",
    "- 固定某个 latency/memory 上限作为约束：\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\text{Error}(\\theta)\n",
    "  \\quad \\text{s.t.} \\quad \\text{Latency}(\\theta) \\le L_{\\max}, \\\n",
    "  \\text{Memory}(\\theta) \\le M_{\\max}\n",
    "$$\n",
    "\n",
    "\n",
    "- 或把 latency/memory 转换为正则项加入 loss\n",
    "\n",
    "> 重要的是：你要能把“部署需求”翻译成数学上的“目标 + 约束”，这样才能用优化工具系统地设计算法。\n",
    "\n",
    "---\n",
    "\n",
    "## 2.5 本章小结\n",
    "\n",
    "本章重点：\n",
    "\n",
    "1. 剪枝可以看作带 $L_0/L_1$ 稀疏约束的优化问题\n",
    "2. 量化可以通过最小化量化误差的优化问题确定 scale/zero-point，进一步可通过训练 joint optimize\n",
    "3. 蒸馏是一个联合最小化 CE 和 KL 的多目标优化问题\n",
    "4. 部署时的精度、延迟、内存可以统一本为多目标/带约束的优化问题\n",
    "\n",
    "> 在后续章节中，这些优化目标会与近似理论、概率统计、硬件模型结合在一起，形成一套完整的推理加速思维。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13525d76",
   "metadata": {
    "id": "13525d76"
   },
   "source": [
    "# 第 3 章：近似理论（Approximation Theory for Quantization & Cheap Ops）\n",
    "\n",
    "近似理论回答的问题是：\n",
    "\n",
    "> “在允许一定误差的前提下，如何用更简单的函数 / 更粗的精度来逼近原函数？”\n",
    "\n",
    "在推理加速中主要应用于：\n",
    "\n",
    "- 量化误差建模\n",
    "- 激活函数近似（GELU/SiLU/tanh 等）\n",
    "- 用多项式 / 分段线性函数替代复杂算子\n",
    "- approximate computing（例如用 cheap ops 替换 expensive ops）\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 均匀量化的误差模型\n",
    "\n",
    "考虑步长为 $\\Delta$ 的均匀量化：\n",
    "\n",
    "$$\n",
    "\\hat{w} = Q(w) = \\Delta \\cdot \\text{round}\\Big(\\frac{w}{\\Delta}\\Big)\n",
    "$$\n",
    "\n",
    "\n",
    "定义量化误差：\n",
    "\n",
    "$$\n",
    "e = w - \\hat{w}\n",
    "$$\n",
    "\n",
    "\n",
    "在很多假设下（信号在每个量化区间内分布比较“均匀”），可以近似认为：\n",
    "\n",
    "- $e$ 在 $[-\\Delta/2, \\Delta/2]$ 上均匀分布\n",
    "- 即：$e \\sim \\mathcal{U}(-\\Delta/2, \\Delta/2)$\n",
    "\n",
    "于是：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[e] \\approx 0, \\quad\n",
    "\\mathbb{E}[e^2] = \\text{Var}(e) \\approx \\frac{\\Delta^2}{12}\n",
    "$$\n",
    "\n",
    "\n",
    "**【工程意义】**  \n",
    "- 量化步长 $\\Delta$ 越大 → 误差方差越大\n",
    "- 对某层使用更低 bit（更少级别）时，可以估算噪声能量：$\\sigma_e^2 \\approx \\Delta^2/12$\n",
    "\n",
    "这为“按层动态选择 bit 宽”提供了理论依据：\n",
    "\n",
    "- 对更敏感的层使用小 $\\Delta$（更多 bit）\n",
    "- 对不敏感的层使用大 $\\Delta$（更少 bit）\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 激活函数的近似：以 GELU 为例\n",
    "\n",
    "GELU 定义：\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$$\n",
    "\n",
    "\n",
    "其中 $\\Phi(x)$ 是标准正态分布的 CDF：\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-t^2/2} \\, dt\n",
    "$$\n",
    "\n",
    "\n",
    "直接计算 CDF 非常昂贵，因此常用近似：\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5 x \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}}(x + 0.044715 x^3)\\right]\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "进一步，可以将 tanh 又近似为一个多项式或分段线性函数，使整体计算只包含：\n",
    "\n",
    "- 加法\n",
    "- 乘法\n",
    "- 少量表查\n",
    "\n",
    "**【近似理论视角】**  \n",
    "\n",
    "- 在一个有限区间 $[a,b]$ 上，用多项式 $P_n(x)$ 逼近一个平滑函数 $f(x)$ 是可行的：\n",
    "\n",
    "$$\n",
    "\\|f - P_n\\|_{\\infty, [a,b]} \\to 0 \\quad \\text{(当 n → ∞ 时)}\n",
    "$$\n",
    "\n",
    "\n",
    "- 对应到工程中，就是选取一个多项式阶数 n，使得：\n",
    "  - 逼近误差足够小（精度要求）\n",
    "  - 计算代价足够低（乘法/加法次数）\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 多项式近似的构造（最小二乘 + Chebyshev）\n",
    "\n",
    "给定函数 $f(x)$，希望在区间 $[a,b]$ 上用多项式：\n",
    "\n",
    "$$\n",
    "P_n(x) = \\sum_{k=0}^n c_k x^k\n",
    "$$\n",
    "\n",
    "\n",
    "来逼近它。\n",
    "\n",
    "常见方法：\n",
    "\n",
    "1. **最小二乘拟合**（离散点）：\n",
    "   - 在区间内采样若干点 $x_i$，最小化：\n",
    "\n",
    "$$\n",
    "\\sum_i (f(x_i) - P_n(x_i))^2\n",
    "$$\n",
    "\n",
    "\n",
    "   - 这会导致一个线性最小二乘问题求 $\\{c_k\\}$\n",
    "\n",
    "2. **Chebyshev 逼近**（均匀最大误差最小）：\n",
    "   - 用 Chebyshev 多项式作为基函数\n",
    "   - 在理论上能提供更好的最大误差界\n",
    "\n",
    "**【对推理工程师的要求】**  \n",
    "- 不需要会手推 Chebyshev 多项式\n",
    "- 需要知道：\n",
    "  - 激活函数/特殊函数的近似来自“多项式拟合”或“有理函数拟合”\n",
    "  - 阶数越高、基函数越复杂 → 精度越好，但计算也越贵\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Approximate Computing：用“更便宜”的算子替代\n",
    "\n",
    "典型思路：\n",
    "\n",
    "- 用 shift 替代乘法（当系数接近 $2^k$ 时）\n",
    "- 用 LUT（查表）替代复杂函数（如 exp / log / erf）\n",
    "- 用 piecewise linear 替代 smooth 非线性（如 ReLU6, HardSwish 等）\n",
    "\n",
    "这都可以看作是：\n",
    "\n",
    "> 在允许一定误差的前提下，用“更便宜的算子组合”近似原函数。\n",
    "\n",
    "**【与你的工作关系】**  \n",
    "\n",
    "- 在设计自定义 kernel 时，你可以：\n",
    "  - 通过数学近似把复杂算子分解成“mul+add+max+shift+table lookup”\n",
    "  - 利用硬件对这些基本算子的高速支持\n",
    "- 在评估近似时，你需要有：\n",
    "  - 定性直觉：误差对下游的影响（例如激活输出范围变窄/变宽）\n",
    "  - 简单定量工具：最大误差、L2 误差等\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 本章小结\n",
    "\n",
    "1. 近似理论为量化误差提供了噪声模型（例如 $\\Delta^2/12$）\n",
    "2. 激活函数（如 GELU）的工程实现广泛依赖多项式/有理函数近似\n",
    "3. Approximate computing 本质是“用更便宜的函数族逼近原函数族”\n",
    "4. 作为推理加速工程师，你需要：\n",
    "   - 看懂“误差 vs 计算代价”的 tradeoff\n",
    "   - 在部署时合理选择近似等级，而不是“盲目追求低误差”或“盲目省算力”\n",
    "\n",
    "后续章节，会用概率/信息论的视角进一步理解这些误差是如何在系统中传播的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9220a4dc",
   "metadata": {
    "id": "9220a4dc"
   },
   "source": [
    "# 第 4 章：概率与统计（Probability & Statistics for Runtime Inference）\n",
    "\n",
    "本章重点：\n",
    "\n",
    "- 量化和剪枝中的“分布视角”\n",
    "- 蒸馏和温度缩放中的 KL/交叉熵\n",
    "- 通过分位数、直方图选择 clipping 阈值\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 输出分布与 KL 散度\n",
    "\n",
    "### 4.1.1 交叉熵与 KL\n",
    "\n",
    "给定真实分布 $p$ 与模型分布 $q$：\n",
    "\n",
    "- 交叉熵：\n",
    "\n",
    "$$\n",
    "H(p, q) = - \\sum_i p_i \\log q_i\n",
    "$$\n",
    "\n",
    "\n",
    "- KL 散度：\n",
    "\n",
    "$$\n",
    "\\text{KL}(p\\|q) = \\sum_i p_i \\log \\frac{p_i}{q_i}\n",
    "$$\n",
    "\n",
    "\n",
    "二者关系：\n",
    "\n",
    "$$\n",
    "H(p, q) = H(p) + \\text{KL}(p\\|q)\n",
    "$$\n",
    "\n",
    "\n",
    "其中 $H(p)$ 与参数无关，优化时常直接最小化交叉熵。\n",
    "\n",
    "在蒸馏中，teacher 分布 $p^T$，student 分布 $p^S$：\n",
    "\n",
    "$$\n",
    "L_{\\text{KD}} = \\text{KL}(p^T \\| p^S)\n",
    "$$\n",
    "\n",
    "\n",
    "反映 student 在多大程度上“模仿”了 teacher 的输出分布。\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1.2 温度（Temperature）\n",
    "\n",
    "logits $z$ 经过 softmax：\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}}\n",
    "$$\n",
    "\n",
    "\n",
    "- $T=1$：常规 softmax\n",
    "- $T>1$：分布变平滑（“软标签”）\n",
    "- $T<1$：分布变更尖锐\n",
    "\n",
    "在蒸馏中通常用 $T>1$：\n",
    "\n",
    "- Teacher 输出变得更“软”，包含更多类别间相对关系信息\n",
    "- Student 更容易学习到 teacher 的“暗知识”（dark knowledge）\n",
    "\n",
    "**【与你的工作关系】**  \n",
    "- 在边缘部署 Distilled 模型时，你应当理解：\n",
    "  - student 的行为不仅拟合 hard label，也拟合了 teacher 提供的 soft label\n",
    "  - 在某些分布 shift 场景中，蒸馏过的模型可能更鲁棒\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 量化中的统计视角：直方图与分位数\n",
    "\n",
    "在选择量化阈值（clipping range）时，常见的做法是：\n",
    "\n",
    "1. 收集一段时间的 activations / weights\n",
    "2. 画直方图（histogram）\n",
    "3. 决定：\n",
    "   - 是否使用对称量化（symmetric）\n",
    "   - 是否 clip 例如 top 0.1% 的 outliers\n",
    "\n",
    "### 4.2.1 分位数剪裁（percentile clipping）\n",
    "\n",
    "例如设置：\n",
    "\n",
    "$$\n",
    "\\alpha = \\text{quantile}_{p}(|x|)\n",
    "$$\n",
    "\n",
    "\n",
    "然后将范围裁剪到 $[- \\alpha, \\alpha]$，再在该范围内做均匀量化。\n",
    "\n",
    "这样做的理由是：\n",
    "\n",
    "- 极少数非常大的 outlier 会极大放大 scale，从而让大部分值被挤压到极细的 quantization bin\n",
    "- 将 outlier 裁剪掉通常对最终性能影响不大，有时甚至有正则化效果\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2.2 KL-based threshold 选择\n",
    "\n",
    "部分框架（例如早期的 TensorRT / TensorFlow 量化方案）会：\n",
    "\n",
    "- 尝试多个不同的 clipping threshold\n",
    "- 对每个阈值，将原始直方图与量化后重建的直方图比较\n",
    "- 选择使 KL(p\\|q) 最小的阈值\n",
    "\n",
    "**【本质】**  \n",
    "通过 KL 来衡量分布形状的变化，避免量化导致分布严重“畸变”。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 统计稳健性（Robust Statistics）\n",
    "\n",
    "在推理部署中，你需要考虑：\n",
    "\n",
    "- 上游数据分布是否会发生漂移（distribution shift）\n",
    "- 是否存在噪声、异常值（outliers）\n",
    "- 对不同层的激活分布要有宏观认知：\n",
    "  - 是否近似高斯\n",
    "  - 是否有长尾\n",
    "\n",
    "有些技术（例如 robust norm、Huber 损失）可以缓解 outlier 对参数/统计量的影响。  \n",
    "在这里你不需要深入推导，只需要知道：\n",
    "\n",
    "- 许多可以“抑制 outlier”的方法本质上来自稳健统计理论\n",
    "- 在量化/clip/归一化中，使用分位数而不是 max/min 也是一种稳健统计实践\n",
    "\n",
    "---\n",
    "\n",
    "## 4.4 本章小结\n",
    "\n",
    "1. KL、交叉熵是蒸馏与分布对齐的数学工具\n",
    "2. 温度缩放（T）控制输出分布的“平滑度”，影响学习信号\n",
    "3. 量化中的阈值选择通常基于直方图与分位数剪裁\n",
    "4. 稍有统计直觉，有助于理解：\n",
    "   - 为什么 outlier 会毁掉量化\n",
    "   - 为什么剪裁 outlier 反而可能更好\n",
    "   - 为什么蒸馏后的模型在部署中可能更鲁棒\n",
    "\n",
    "后续章节的信息论会从更基础的角度解释熵与压缩的关系。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b56d6",
   "metadata": {
    "id": "6e9b56d6"
   },
   "source": [
    "# 第 5 章：信息论（Information Theory for Compression）\n",
    "\n",
    "信息论回答的问题是：\n",
    "\n",
    "> “要表示一个随机变量/信号最少需要多少 bit？”\n",
    "\n",
    "在推理压缩中，信息论提供：\n",
    "\n",
    "- 熵（entropy）：理论最小 bit 宽\n",
    "- rate–distortion：压缩率 vs 失真度\n",
    "- 作为模型剪枝 / 量化的一种“上帝视角”\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 熵（Entropy）\n",
    "\n",
    "离散随机变量 $X$ 取值 $\\{x_i\\}$，概率 $p_i$，其熵定义为：\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_i p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "\n",
    "- 单位是 bits\n",
    "- 反映“平均需要多少 bit 来编码 X 的取值”\n",
    "\n",
    "**例子**：硬币正反面等概率：\n",
    "\n",
    "$$\n",
    "H(X) = - (0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1 \\text{ bit}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 与量化 / 编码的关系\n",
    "\n",
    "如果你对权重/激活做了量化和统计建模：\n",
    "\n",
    "- 量化后值域有限，例如 $\\{-127,\\dots,127\\}$\n",
    "- 但每个值出现频率不同\n",
    "- 你可以用 entropy coding（Huffman / arithmetic coding）进一步压缩比特流\n",
    "\n",
    "理论上，平均码长下界接近熵：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\text{code length}] \\ge H(X)\n",
    "$$\n",
    "\n",
    "\n",
    "**【工程视角】**  \n",
    "- 这解释了：\n",
    "  - 为什么稀疏矩阵 / 集中分布的值更好压缩\n",
    "  - 为什么有些 post-training compression 工具可以在不影响精度的前提下，大幅减少模型文件大小（对部署包很关键）\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3 Rate–Distortion（率失真）\n",
    "\n",
    "Rate–distortion 理论研究：\n",
    "\n",
    "> 在平均失真不超过 D 的前提下，最小的比特率 R 是多少？\n",
    "\n",
    "通俗版本：\n",
    "\n",
    "- “我要压小一点，愿意损失多少精度？”\n",
    "- 或反过来：“给我这么多 bit，我最多能守住多少质量？”\n",
    "\n",
    "虽然在部署端不会直接用上正式的 rate–distortion 函数，  \n",
    "但这个思路在很多论文/方法论中都反复出现：\n",
    "\n",
    "- bit allocation（给不同层/通道分配不同 bit）\n",
    "- 剪枝比例分配（给不同层分配不同稀疏度）\n",
    "\n",
    "可以理解为：\n",
    "\n",
    "> 一个受限资源（bit budget / FLOPs budget）下的“优化分配问题”，信息论提供对“最优情况”的一个理论参考。\n",
    "\n",
    "---\n",
    "\n",
    "## 5.4 本章小结\n",
    "\n",
    "1. 熵度量了平均所需的编码 bit 数\n",
    "2. 熵越低 → 越容易压缩（更高压缩比）\n",
    "3. 量化 + 熵编码可以进一步减少模型大小\n",
    "4. Rate–distortion 提供了一种“资源–损失”的系统性思考方式\n",
    "\n",
    "在具体部署中，你不用推导定理，但可以用这些概念来：\n",
    "\n",
    "- 解释“为什么这层可以 aggressively 压缩，而那层不行？”\n",
    "- 理解“为什么压缩工具能做到看似‘白嫖’空间”。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5810c",
   "metadata": {
    "id": "36d5810c"
   },
   "source": [
    "# 第 6 章：信号处理与卷积数学（Signal Processing & Convolution）\n",
    "\n",
    "本章连接：\n",
    "\n",
    "- 信号处理视角下的卷积\n",
    "- FFT、Winograd 等快速卷积算法\n",
    "- Toeplitz / Circulant 等结构化矩阵\n",
    "\n",
    "这些数学为卷积加速提供理论基础。\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 卷积定理：时域卷积 = 频域乘法\n",
    "\n",
    "一维离散卷积：\n",
    "\n",
    "$$\n",
    "y[n] = \\sum_k x[k] h[n-k]\n",
    "$$\n",
    "\n",
    "\n",
    "其离散时间傅里叶变换（DTFT）满足：\n",
    "\n",
    "$$\n",
    "Y(\\omega) = X(\\omega) H(\\omega)\n",
    "$$\n",
    "\n",
    "\n",
    "这就是 **卷积定理**：\n",
    "\n",
    "> 卷积在频域里变成点乘。\n",
    "\n",
    "计算复杂度：\n",
    "\n",
    "- 直接卷积：$O(NK)$\n",
    "- FFT-based：\n",
    "  - $O(N \\log N)$ 计算 FFT/逆 FFT\n",
    "  - 中间是逐点乘法（$O(N)$）\n",
    "\n",
    "在某些大核/长序列场景，FFT 卷积更划算。\n",
    "\n",
    "---\n",
    "\n",
    "## 6.2 Toeplitz 与 Circulant 矩阵\n",
    "\n",
    "一维卷积可以写成 Toeplitz 矩阵与向量乘法：\n",
    "\n",
    "$$\n",
    "y = T x\n",
    "$$\n",
    "\n",
    "\n",
    "其中 Toeplitz 矩阵 T 的每条对角线元素相同。  \n",
    "在周期边界条件下可变成 Circulant 矩阵，其特征：\n",
    "\n",
    "- 可以被傅里叶基对角化\n",
    "- 矩阵–向量乘法可以通过 FFT 高效实现\n",
    "\n",
    "$$\n",
    "C = F^\\ast \\Lambda F\n",
    "$$\n",
    "\n",
    "\n",
    "- F：离散傅里叶变换矩阵\n",
    "- $\\Lambda$：对角矩阵（频域增益）\n",
    "\n",
    "**【工程视角】**  \n",
    "\n",
    "- 理解 Toeplitz/Circulant 结构有助于：\n",
    "  - 理解 FFT 卷积的本质\n",
    "  - 理解某些“频域参数化”的卷积层\n",
    "- 不需要记住复杂推导，但要知道：\n",
    "  - 卷积可以被看成带特殊结构的矩阵\n",
    "  - 特殊结构可以带来更快的乘法算法\n",
    "\n",
    "---\n",
    "\n",
    "## 6.3 Winograd 卷积（最小乘法算法）\n",
    "\n",
    "Winograd 思想：\n",
    "\n",
    "> 用更多的加法/减法和少量变换，换取更少的乘法次数。\n",
    "\n",
    "例如 F(2×2, 3×3) 算法：  \n",
    "- 输入 tile 大小：4×4\n",
    "- 输出 tile 大小：2×2\n",
    "- 通过对输入和卷积核做线性变换，将卷积转化为：\n",
    "  - element-wise 乘法\n",
    "  - 再经过逆变换\n",
    "\n",
    "其优点：\n",
    "\n",
    "- 对小核卷积（如 3×3）乘法数大幅减少\n",
    "- 在乘法远比加法“贵”的硬件上更有优势\n",
    "\n",
    "缺点：\n",
    "\n",
    "- 数值稳定性可能变差（放大误差）\n",
    "- 对大核/stride 不友好\n",
    "- 实现复杂\n",
    "\n",
    "**【推理加速视角】**  \n",
    "\n",
    "- 你需要知道：\n",
    "  - 许多高性能 Conv kernel（尤其在移动端）使用 Winograd 或其变体\n",
    "  - 它们都是在线性代数/多项式插值基础上得到的最小乘法算法\n",
    "\n",
    "---\n",
    "\n",
    "## 6.4 本章小结\n",
    "\n",
    "1. 卷积定理：时域卷积 ↔ 频域乘法，为 FFT 卷积提供数学基础\n",
    "2. Toeplitz/Circulant 矩阵提供了“卷积是结构化矩阵乘法”的视角\n",
    "3. Winograd 算法通过巧妙变换减少乘法次数，是很多 Conv 加速方法的核心\n",
    "\n",
    "在实际部署中，你不一定亲自实现 FFT/Winograd kernel，  \n",
    "但理解其数学，有助于：\n",
    "\n",
    "- 解读不同硬件/库对卷积的性能差异\n",
    "- 做出“某些卷积形状适配某些算法”的合理判断。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913aa00",
   "metadata": {
    "id": "b913aa00"
   },
   "source": [
    "# 第 7 章：计算图与图论（Computational Graph & Graph Theory）\n",
    "\n",
    "本章关注：\n",
    "\n",
    "- 如何用 DAG（有向无环图）表示模型\n",
    "- graph rewrite / fusion 的数学抽象\n",
    "- 如何基于图结构做 scheduling / partitioning\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 模型即计算图（DAG）\n",
    "\n",
    "在框架中，一个模型通常被表示为：\n",
    "\n",
    "- 节点（node）：算子（op），例如 MatMul、Add、GELU、LayerNorm 等\n",
    "- 边（edge）：张量数据流\n",
    "\n",
    "由于不存在环（没有算子依赖未来的结果），该图是一个 **DAG**。\n",
    "\n",
    "数学上，DAG 性质保证：\n",
    "\n",
    "- 可以进行 **拓扑排序（topological sort）**\n",
    "- 存在至少一种执行顺序使得所有依赖都满足\n",
    "\n",
    "---\n",
    "\n",
    "## 7.2 图重写（Graph Rewrite）\n",
    "\n",
    "图重写的目标：\n",
    "\n",
    "> 在不改变整体输入–输出语义的前提下，改写图结构以获得更高效率。\n",
    "\n",
    "常见操作：\n",
    "\n",
    "1. **算子融合（Operator Fusion）**\n",
    "\n",
    "   例如：\n",
    "\n",
    "$$\n",
    "Y = \\text{GELU}(X W + b)\n",
    "$$\n",
    "\n",
    "\n",
    "   在图中通常表现为：`X → MatMul → Add → Gelu` 四个节点。\n",
    "\n",
    "   通过图重写，可以识别这是一种常见 pattern，将其替换为：\n",
    "\n",
    "   - 单个 `FusedMatMulBiasGelu` 节点\n",
    "   - 内部实现为一个 kernel，减少中间内存读写\n",
    "\n",
    "2. **常量折叠（Constant Folding）**\n",
    "\n",
    "   若某些节点仅依赖常量张量，可以在编译时提前执行，简化推理时的计算量。\n",
    "\n",
    "3. **消除冗余算子（Elimination）**\n",
    "\n",
    "   - 连续的 reshape/transpose 合并\n",
    "   - 反向操作互相抵消（如 transpose 两次）\n",
    "\n",
    "**数学抽象**：\n",
    "\n",
    "- 一组 rewrite rule：\n",
    "\n",
    "$$\n",
    "P \\Rightarrow R\n",
    "$$\n",
    "\n",
    "\n",
    "  其中 P 是 pattern 子图，R 是替换后的子图。\n",
    "\n",
    "- 在 DAG 上遍历，匹配 pattern，进行等价替换。\n",
    "\n",
    "---\n",
    "\n",
    "## 7.3 图划分与并行（Partitioning & Parallelism）\n",
    "\n",
    "对大模型，尤其在多设备/多核场景下，需要将计算图划分为子图：\n",
    "\n",
    "- pipeline parallelism\n",
    "- tensor parallelism\n",
    "- expert parallelism（MoE）\n",
    "\n",
    "数学上涉及：\n",
    "\n",
    "- 图划分（graph partitioning）：\n",
    "  - 希望 cut 边尽量少（减少通信）\n",
    "  - 各子图负载尽量平衡（避免一块成为瓶颈）\n",
    "- 负载均衡问题本质上是一个 NP-hard 的组合优化问题\n",
    "- 实际使用启发式算法（greedy / spectral partitioning / METIS 等）\n",
    "\n",
    "**【与你的工作关系】**  \n",
    "\n",
    "- 在 edge device 上，多核/多引擎并行也需要合理切图\n",
    "- 推理框架常会自动做 operator placement，你需要能看懂“为什么某些 op 被放在某个引擎上”\n",
    "\n",
    "---\n",
    "\n",
    "## 7.4 Liveness 分析与内存规划（Memory Planning）\n",
    "\n",
    "计算图还承载：\n",
    "\n",
    "- 每个张量的 **生命周期（liveness interval）**\n",
    "- 哪些张量在同一时间段内“同时活跃”\n",
    "\n",
    "基于此可做：\n",
    "\n",
    "- 内存重用（buffer reuse）\n",
    "- in-place 计算\n",
    "\n",
    "从数学上，这类似于：\n",
    "\n",
    "- 图着色 / 区间图着色（interval graph coloring）\n",
    "- 目标是最少的“颜色”（内存块）覆盖所有区间\n",
    "\n",
    "> 虽然你不会直接实现图着色算法，但要知道内存规划问题在图论中是一个经典主题。\n",
    "\n",
    "---\n",
    "\n",
    "## 7.5 本章小结\n",
    "\n",
    "1. 模型可以抽象为一个 DAG；拓扑排序提供执行顺序\n",
    "2. 图重写（fusion/folding/elimination）是推理加速的关键步骤\n",
    "3. 图划分与并行涉及图划分问题，需要平衡通信与负载\n",
    "4. 内存规划可以用 liveness 分析 + 图着色的视角理解\n",
    "\n",
    "从这一章开始，数学不再是“公式推导”，而是更多地提供了一种 **抽象建模工具**，帮助你从更高层理解编译器与推理框架在干什么。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59e686",
   "metadata": {
    "id": "9f59e686"
   },
   "source": [
    "# 第 8 章：数值稳定性与复杂度（Numerical Stability & Complexity）\n",
    "\n",
    "本章关注：\n",
    "\n",
    "- 浮点数的误差模型\n",
    "- 稳定 softmax / LayerNorm\n",
    "- 算法复杂度（时间/空间）与推理性能\n",
    "\n",
    "---\n",
    "\n",
    "## 8.1 浮点数与舍入误差\n",
    "\n",
    "一个典型的浮点运算可以抽象为：\n",
    "\n",
    "$$\n",
    "\\text{fl}(x \\circ y) = (x \\circ y)(1 + \\delta), \\quad |\\delta| \\le \\epsilon\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\circ$：基本运算（+,-,×,÷）\n",
    "- $\\epsilon$：机器精度（machine epsilon）\n",
    "- $\\delta$：舍入误差\n",
    "\n",
    "连续多次运算后，误差会积累，有时还会被放大。\n",
    "\n",
    "**【工程意义】**  \n",
    "\n",
    "- 在 FP32 → FP16/FP8 量化时，$\\epsilon$ 变大  \n",
    "- 在长序列、深网络中，累积误差不容忽视\n",
    "- 需要设计数值稳定的实现方式（见 softmax 示例）\n",
    "\n",
    "---\n",
    "\n",
    "## 8.2 稳定 softmax\n",
    "\n",
    "朴素写法：\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "\n",
    "可能遇到：\n",
    "\n",
    "- 当某个 $x_k$ 很大时，$e^{x_k}$ 溢出\n",
    "- 其它项相对变成 0，导致数值问题\n",
    "\n",
    "稳定写法：\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i - m}}{\\sum_j e^{x_j - m}}, \\quad m = \\max_j x_j\n",
    "$$\n",
    "\n",
    "\n",
    "推导：\n",
    "\n",
    "$$\n",
    "\\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "= \\frac{e^{x_i - m} e^m}{\\sum_j e^{x_j - m} e^m}\n",
    "= \\frac{e^{x_i - m}}{\\sum_j e^{x_j - m}}\n",
    "$$\n",
    "\n",
    "\n",
    "- 分子分母同时乘以 $e^{-m}$，数值上避免了溢出/下溢\n",
    "\n",
    "**【工程视角】**  \n",
    "\n",
    "- 在 FP16/FP8 或混合精度中，这种稳定化处理尤其重要\n",
    "- FlashAttention 等算法在实现时非常强调：\n",
    "  - 按 block 计算 max\n",
    "  - 增量式维护稳定 softmax\n",
    "\n",
    "---\n",
    "\n",
    "## 8.3 LayerNorm / RMSNorm 的数值考虑\n",
    "\n",
    "LayerNorm：\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{d}\\sum_{i=1}^d x_i, \\quad\n",
    "\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^d (x_i - \\mu)^2 \\\\\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "\n",
    "数值稳定性挑战：\n",
    "\n",
    "- $\\sigma^2$ 很小或很大时，都可能出现精度损失\n",
    "- FP16 中的累加（sum）不精确\n",
    "\n",
    "常见对策：\n",
    "\n",
    "- 在高精度（FP32）accumulator 中累加，再 cast 回低精度\n",
    "- 使用 Kahan summation 改善求和精度（较少见于实际 ML kernel，但思想值得知道）\n",
    "\n",
    "---\n",
    "\n",
    "## 8.4 算法复杂度（Complexity）\n",
    "\n",
    "在推理场景中，关注的复杂度包括：\n",
    "\n",
    "- 时间复杂度：FLOPs 数 / 算法阶\n",
    "- 空间复杂度：参数量 / activation / KV cache\n",
    "\n",
    "例：自注意力 $O(T^2 d)$：\n",
    "\n",
    "- QKᵀ：$O(T^2 d)$\n",
    "- A V：$O(T^2 d)$\n",
    "\n",
    "当 T 很大时（例如长上下文 LLM），这是主要瓶颈。  \n",
    "因此：\n",
    "\n",
    "- 许多“线性注意力”、“局部注意力”、“稀疏注意力”方法试图将复杂度降到 $O(T d)$ 或 $O(T \\log T)$\n",
    "- 你需要能大致估算不同模块的 FLOPs，判断瓶颈在哪一层\n",
    "\n",
    "---\n",
    "\n",
    "## 8.5 本章小结\n",
    "\n",
    "1. 浮点误差模型帮助你理解：为什么低精度需要数值稳定技巧\n",
    "2. 稳定 softmax / LayerNorm 是经典例子，推理 kernel 必须实现稳定版本\n",
    "3. 算法复杂度决定了模型在大规模输入/长序列下的 scalability\n",
    "4. 在做 runtime acceleration 时：\n",
    "   - 一部分工作是 **减少常数项**（kernel 级优化）\n",
    "   - 另一部分是 **降低复杂度阶数**（模型/算法级优化）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c237fb5",
   "metadata": {
    "id": "7c237fb5"
   },
   "source": [
    "# 第 9 章：硬件相关数学（Hardware-Aware Math: Roofline, Tiling, SIMD, FMA）\n",
    "\n",
    "本章聚焦：\n",
    "\n",
    "- Roofline 模型：算力 vs 带宽\n",
    "- 算术强度（arithmetic intensity）\n",
    "- 分块（tiling）与数据重用\n",
    "- SIMD / FMA / GEMM 内核的数学视角\n",
    "\n",
    "---\n",
    "\n",
    "## 9.1 Roofline 模型\n",
    "\n",
    "Roofline 模型给出在给定硬件上，某个 kernel 的理论性能上界：\n",
    "\n",
    "$$\n",
    "P_{\\text{attainable}} = \\min\\left(P_{\\text{peak}},\\ I \\cdot B\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "其中：\n",
    "\n",
    "- $P_{\\text{peak}}$：峰值计算性能（如 TFLOPs）\n",
    "- $B$：内存带宽（如 GB/s）\n",
    "- $I$：算术强度（arithmetic intensity）\n",
    "\n",
    "算术强度定义为：\n",
    "\n",
    "$$\n",
    "I = \\frac{\\text{FLOPs}}{\\text{Bytes moved}}\n",
    "$$\n",
    "\n",
    "\n",
    "**两种 regime：**\n",
    "\n",
    "1. **memory-bound**（带宽受限）：\n",
    "   - 若 $I \\cdot B < P_{\\text{peak}}$\n",
    "   - 性能主要受内存带宽限制，FLOPs 很充裕\n",
    "2. **compute-bound**（算力受限）：\n",
    "   - 若 $I \\cdot B \\ge P_{\\text{peak}}$\n",
    "   - 性能由算力上限决定\n",
    "\n",
    "**【工程视角】**  \n",
    "\n",
    "- 对算子而言：\n",
    "  - Conv/GEMM 通常具有较高 $I$，更容易成为 compute-bound\n",
    "  - 简单 element-wise op 的 $I$ 很低，通常是 memory-bound\n",
    "- 对 Embedding 而言：几乎没有 FLOPs，但读写大量数据 → 典型 memory-bound\n",
    "\n",
    "---\n",
    "\n",
    "## 9.2 分块（Tiling）与数据重用\n",
    "\n",
    "以 GEMM 为例：$C = A B$，其中：\n",
    "\n",
    "- $A \\in \\mathbb{R}^{M \\times K}$\n",
    "- $B \\in \\mathbb{R}^{K \\times N}$\n",
    "- $C \\in \\mathbb{R}^{M \\times N}$\n",
    "\n",
    "如果直接三重循环，会频繁从内存加载 A/B/C，导致带宽浪费。\n",
    "\n",
    "分块思想：\n",
    "\n",
    "- 将 C 分成小块（tile）：$M_b \\times N_b$\n",
    "- 对每个 tile：\n",
    "  - 加载对应的 A 子块（$M_b \\times K_b$）\n",
    "  - 加载对应的 B 子块（$K_b \\times N_b$）\n",
    "  - 在寄存器或小缓存中完成所有 FMA\n",
    "\n",
    "约束条件：\n",
    "\n",
    "$$\n",
    "(M_b \\cdot K_b + K_b \\cdot N_b + M_b \\cdot N_b) \\cdot \\text{bytes} \\le \\text{cache size}\n",
    "$$\n",
    "\n",
    "\n",
    "**【直观理解】**  \n",
    "\n",
    "- 通过分块，让同一块数据被多次重用（reuse），有效提高算术强度 $I$\n",
    "- 减少对 DRAM 的访问次数\n",
    "\n",
    "---\n",
    "\n",
    "## 9.3 SIMD / FMA / Tensor Core\n",
    "\n",
    "### 9.3.1 FMA 再回顾\n",
    "\n",
    "$$\n",
    "\\text{FMA}(a,b,c) = a \\times b + c\n",
    "$$\n",
    "\n",
    "\n",
    "- 在硬件中可以作为一个基本指令\n",
    "- dot product 与 GEMM 内部都是大量 FMA\n",
    "\n",
    "### 9.3.2 SIMD（Single Instruction, Multiple Data）\n",
    "\n",
    "SIMD 指令可以：\n",
    "\n",
    "- 一条指令对多个数据元素执行相同运算\n",
    "- 例如 AVX2 中 256-bit 寄存器可同时处理 8 个 float\n",
    "\n",
    "矩阵乘法内部：\n",
    "\n",
    "- 每次从内存加载一个向量 block\n",
    "- 使用 SIMD/FMA 对多个元素并行乘加\n",
    "\n",
    "### 9.3.3 Tensor Core / Matrix Unit\n",
    "\n",
    "在现代 GPU/NPU 中：\n",
    "\n",
    "- 提供专门的矩阵乘单元（如 NVIDIA Tensor Core、Intel AMX）\n",
    "- 每条指令执行一个小矩阵乘运算（如 16×16 × 16×16）\n",
    "\n",
    "**【数学视角】**  \n",
    "\n",
    "- 这些专用单元本质上实现了一个：\n",
    "\n",
    "$$\n",
    "C_{\\text{tile}} = A_{\\text{tile}} B_{\\text{tile}} + C_{\\text{tile}}\n",
    "$$\n",
    "\n",
    "\n",
    "- 你的任务是：\n",
    "  - 把大矩阵拆成符合硬件 tile 大小的小块\n",
    "  - 做好数据布局，使得这些 tile 在内存中连续、对齐\n",
    "\n",
    "---\n",
    "\n",
    "## 9.4 硬件意识下的量化与打包（Packing）\n",
    "\n",
    "对 INT8 / INT4 GEMM：\n",
    "\n",
    "- 权重/激活不仅被量化，还会按特定 pattern 打包（pack）到寄存器友好的格式\n",
    "- 例如把多个 int4 压在一个 16-bit 或 32-bit 容器中\n",
    "\n",
    "这涉及：\n",
    "\n",
    "- 位运算（bitwise ops）\n",
    "- 对齐（alignment）约束\n",
    "- 多个元素的“并行 unpack”数学\n",
    "\n",
    "虽然这里不展开细节，但要知道：\n",
    "\n",
    "> 高性能量化推理内核的数学本质是：  \n",
    "> **用整数算术和位运算实现“向量化 FMA”的等价行为。**\n",
    "\n",
    "---\n",
    "\n",
    "## 9.5 本章小结\n",
    "\n",
    "1. Roofline 模型帮助你判断 kernel 是 compute-bound 还是 memory-bound\n",
    "2. 算术强度 $I$ 是理解“为什么要做 tiling / fusion / 缓存重用”的关键指标\n",
    "3. FMA、SIMD、Tensor Core 是高性能线性代数的硬件支柱\n",
    "4. 量化推理中还要考虑整数打包与位运算的数学结构\n",
    "\n",
    "> 到这里为止，你已经拥有了一套：  \n",
    "> **从线性代数 → 优化 → 近似 → 统计 → 信息论 → 信号处理 → 图论 → 数值分析 → 硬件数学** 的完整思维链条。\n",
    "\n",
    "在实际工程中，每一次做部署决策，你都可以在脑中快速走一遍这条链：\n",
    "\n",
    "- 这个算子本质是什么线性代数？\n",
    "- 能不能用压缩/近似/剪枝优化？\n",
    "- 对分布/信息量有何影响？\n",
    "- 会不会引发数值问题？\n",
    "- 在硬件上是 compute-bound 还是 memory-bound？\n",
    "- 该如何做 tiling / fusion / placement？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43e51a",
   "metadata": {
    "id": "0b43e51a"
   },
   "source": [
    "# 附录 A：符号表与术语速查\n",
    "\n",
    "- $W, A, B, C$：矩阵\n",
    "- $x, y, z$：向量\n",
    "- $\\|\\cdot\\|_F$：Frobenius 范数\n",
    "- $\\|\\cdot\\|_2$：谱范数 / operator norm\n",
    "- $\\|\\cdot\\|_1, \\|\\cdot\\|_0$：L1 范数、L0 “范数”（非零个数）\n",
    "- SVD：Singular Value Decomposition\n",
    "- GEMM：General Matrix-Matrix Multiply\n",
    "- FMA：Fused Multiply-Add\n",
    "- AI：Arithmetic Intensity（算术强度）\n",
    "- KL：Kullback-Leibler 散度\n",
    "- CE：Cross-Entropy 交叉熵\n",
    "- FFT：Fast Fourier Transform\n",
    "- DAG：Directed Acyclic Graph（有向无环图）\n",
    "\n",
    "---\n",
    "\n",
    "# 附录 B：练习建议（自我检查）\n",
    "\n",
    "你可以按照下面的 checklist 检查自己是否掌握了本书核心内容：\n",
    "\n",
    "1. **线性代数**\n",
    "   - 能写出 GEMM 的元素级公式，并解释其 FLOPs 规模\n",
    "   - 能解释 SVD 截断是最优低秩近似，并画出示意图\n",
    "   - 能从 Conv2d 推导出 im2col + GEMM 形式\n",
    "\n",
    "2. **数值优化**\n",
    "   - 能写出带 L0/L1 正则的剪枝目标函数\n",
    "   - 能解释如何通过最小化量化误差来求 scale\n",
    "   - 能解释蒸馏中 CE + KL 的含义\n",
    "\n",
    "3. **近似理论**\n",
    "   - 能写出均匀量化误差的方差 $\\Delta^2/12$\n",
    "   - 能解释为什么激活函数可以用多项式近似\n",
    "\n",
    "4. **概率与统计 / 信息论**\n",
    "   - 理解 KL、交叉熵的定义及其关系\n",
    "   - 能解释熵与平均码长的关系\n",
    "\n",
    "5. **信号处理 / 图论 / 数值稳定 / 硬件数学**\n",
    "   - 理解卷积定理与 FFT 卷积大致思路\n",
    "   - 知道 DAG、graph rewrite 的意义\n",
    "   - 能解释稳定 softmax 的推导\n",
    "   - 能用 Roofline 模型说清楚一个 kernel 是 compute-bound 还是 memory-bound\n",
    "\n",
    "---\n",
    "\n",
    "> 建议：  \n",
    "> 你可以在本 notebook 的基础上：\n",
    "> - 增加代码 cell，做一些数值实验（例如 SVD 压缩一个随机矩阵，观察误差 vs rank）  \n",
    "> - 写一些小脚本 profiling 不同形状的 GEMM，感受 tiling 的影响  \n",
    "> - 整合你的 GM 实际部署案例，逐渐把这里变成你自己的“Runtime Inference 数学工作手册”。\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
