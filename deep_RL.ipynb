{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5336b4b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 阅读笔记\n",
    "## 基础\n",
    "### 标准强化学和深度强化学习\n",
    "- 标准强化学习：比如 TD-Gammon 玩 Backgammon 游戏的过程，其实就是设计特征，然后训练价值函数的过程。标准强化学习先设计很多特征，这些特征可以描述现在整个状态。 得到这些特征后，我们就可以通过训练一个分类网络或者分别训练一个价值估计函数来采取动作。\n",
    "\n",
    "- 深度强化学习：自从我们有了深度学习，有了神经网络，就可以把智能体玩游戏的过程改进成一个端到端训练（end-to-end training）的过程，我们不需要设计特征，直接输入状态就可以输出动作。我们可以用一个神经网络来拟合价值函数或策略网络，省去特征工程（feature engineering）的过程\n",
    "\n",
    "![pic1](./deep_RL/standard_and_deep_rl.png)\n",
    "\n",
    "### Reward\n",
    "智能体在环境里面存在的目 的就是最大化它的期望的累积奖励（expected cumulative reward）\n",
    "\n",
    "### Trajectory\n",
    "每一个观测可看成一个轨迹（trajectory）。 轨迹就是当前帧以及它采取的策略，即状态和动作的序列： $$ \\tau=\\left(s_{0}, a_{0}, s_{1}, a_{1}, \\ldots\\right) $$\n",
    "\n",
    "### State and Observation\n",
    "状态是对世界的完整描述，不会隐藏世界的信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用实值的向量、矩阵或者更高阶的张量来表示状态和观测.\n",
    "\n",
    "环境有自己的函数$s_{t}^{e}=f^{e}\\left(H_{t}\\right)$ 来更新状态，在智能体的内部也有一个函数$s_{t}^{a}=f^{a}\\left(H_{t}\\right)$来更新状 态。当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的（fully observed）。在这种情况下面，强化学习通常被建模成一个马尔可夫决策过程 （Markov decision process，MDP）的问题。在马尔可夫决策过程中，$o_{t}=s_{t}^{e}=s_{t}^{a}$。\n",
    "\n",
    "强化学习通常被建模成**部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）**的问题。部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。 部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道 部分观测值。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。部分可观测马尔可夫决策过程可以用一个七元组描述：$(S,A,T,R,\\Omega,O,\\gamma)$。其中 \n",
    "S表示状态空间，为隐变量，$A$ 为动作空间，$T(s'|s,a)$ 为状态转移概率，$R$ 为奖励函数，$\\Omega(o|s,a)$ 为观测概率，$O$ 为观测空间，$\\gamma$ 为折扣系数。\n",
    "\n",
    "### Action space\n",
    "不同的环境允许不同种类的动作。在给定的环境中，有效动作的集合经常被称为动作空间（action space）。有两种action space：（discrete action space），在这个动作空间里，智能体的动作数量是有限的。在其他环境，比如在物理世界中控制一个智能体，在这个环境中就有连续动作空间（continuous action space）。在连续动作空间中，动作是实值的向量。\n",
    "\n",
    "### 强化学习组成\n",
    "- Policy\n",
    "  - 随机性策略（stochastic policy）**就是 π 函数，即$\\pi(a | s)=p\\left(a_{t}=a | s_{t}=s\\right)$。输入一个状态s，输出一个概率。 这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。比如可能是有 0.7 的概率往左，0.3 的概率往右，那么通过采样就可以得到智能体将采取的动作。\n",
    "\n",
    "  - 确定性策略（deterministic policy）**就是智能体直接采取最有可能的动作，即 \n",
    "$$\n",
    "\\pi^* = \\arg\\max_a \\pi(a \\mid s)\n",
    "$$\n",
    "\n",
    "- Value function\n",
    "价值函数的值是对未来奖励的预测，我们用它来评估状态的好坏。 价值函数里面有一个折扣因子（discount factor），我们希望在尽可能短的时间里面得到尽可能多的奖励. 价值函数的定义为\n",
    "\n",
    "$$ V_{\\pi}(s) \\doteq \\mathbb{E}{\\pi}\\left[G{t} \\mid s_{t}=s\\right]=\\mathbb{E}{\\pi}\\left[\\sum{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} \\mid s_{t}=s\\right], \\text{对于所有的} s \\in S $$\n",
    "期望 $E_\\pi$的下标是 π 函数，$\\pi$ 函数的值可反映在我们使用策略 π的时候，到底可以得到多少奖励。  $G_t$ 是回报函数 。$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $\n",
    "\n",
    "另一种价值函数：Q 函数。Q 函数里面包含两个变量：状态和动作。其定义为 $$ Q_{\\pi}(s, a) \\doteq \\mathbb{E}{\\pi}\\left[G{t} \\mid s_{t}=s, a_{t}=a\\right]=\\mathbb{E}{\\pi}\\left[\\sum{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} \\mid s_{t}=s, a_{t}=a\\right] $$ 所以我们未来可以获得奖励的期望取决于当前的状态和当前的动作。Q 函数是强化学习算法里面要学习的一个函数。因为当我们得到 Q 函数后，进入某个状态要采取的最优动作可以通过 Q 函数得到。\n",
    "\n",
    "\n",
    "- Model\n",
    "模型决定了下一步的状态。下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率即 $$ p_{s s^{\\prime}}^{a}=p\\left(s_{t+1}=s^{\\prime} \\mid s_{t}=s, a_{t}=a\\right) $$\n",
    "\n",
    "奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即 $$ R(s,a)=\\mathbb{E}\\left[r_{t+1} \\mid s_{t}=s, a_{t}=a\\right] $$ 当我们有了策略、价值函数和模型3个组成部分后，就形成了一个马尔可夫决策过程（Markov decision process）。\n",
    "\n",
    "### 强化学习类型\n",
    "- 基于价值学习。\n",
    " 显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的\n",
    "- 基于策略学习\n",
    " 基于策略的智能体。（policy-based agent）直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。把基于价值的智能体和基于策略的智能体结合起来就有了actor-评论员智能体（actor-critic agent）。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作\n",
    "- 二者区别:\n",
    "对于一个状态转移概率已知的马尔可夫决策过程，我们可以使用动态规划算法来求解。从决策方式来看，强化学习又可以划分为基于策略的方法和基于价值的方法。决策方式是智能体在给定状态下从动作集合中选择一个动作的依据，它是静态的，不随状态变化而变化。 在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。 而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于动作集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。 基于价值的强化学习算法有Q学习（Q-learning）、 Sarsa 等，而基于策略的强化学习算法有策略梯度（Policy Gradient，PG）算法等。此外，actor-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果\n",
    "\n",
    "\n",
    "- 有模型强化学习\n",
    "- 无模型强化学习\n",
    "- 二者区别:\n",
    "我们可以用马尔可夫决策过程来定义强化学习任务，并将其表示为四元组 $<S,A,P,R>$，即状态集合、动作集合、状态转移函数和奖励函数。如果这个四元组中所有元素均已知，且状态集合和动作集合在有限步数内是有限集，则智能体可以对真实环境进行建模，构建一个虚拟世界来模拟真实环境中的状态和交互反应。 具体来说，当智能体知道状态转移函数 $P(s_{t+1}|s_t,a_t)$ 和奖励函数 $R(s_t,a_t)$后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。这种学习方法称为有模型强化学习. 然而在实际应用中，智能体并不是那么容易就能知晓马尔可夫决策过程中的所有元素的。通常情况下，状态转移函数和奖励函数很难估计，甚至连环境中的状态都可能是未知的，这时就需要采用无模型强化学习。无模型强化学习没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新动作策略，这样反复迭代直到学习到最优策略。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb889fc",
   "metadata": {},
   "source": [
    "## Police Gradient\n",
    "### 算法数学推导\n",
    "基本整理[蘑菇书RL](https://datawhalechina.github.io/easy-rl/#/chapter4/chapter4)\n",
    "#### expected reward\n",
    "![trajectory](./deep_RL/trajectory.png)\n",
    "在一场游戏里面，我们把环境输出的 $s$ 与演员输出的动作 $a$ 全部组合起来，就是一个轨迹，即\n",
    "$$\n",
    "    \\tau=\\left\\{s_{1}, a_{1}, s_{2}, a_{2}, \\cdots, s_{t}, a_{t}\\right\\}\n",
    "$$\n",
    "\n",
    "给定actor的参数 θ，我们可以计算某个轨迹$\\tau$发生的概率为 $$ \\begin{aligned} p_{\\theta}(\\tau) &=p\\left(s_{1}\\right) p_{\\theta}\\left(a_{1} | s_{1}\\right) p\\left(s_{2} | s_{1}, a_{1}\\right) p_{\\theta}\\left(a_{2} | s_{2}\\right) p\\left(s_{3} | s_{2}, a_{2}\\right) \\cdots \\ &=p\\left(s_{1}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(a_{t} | s_{t}\\right) p\\left(s_{t+1} | s_{t}, a_{t}\\right) \\end{aligned} $$\n",
    "\n",
    "我们先计算环境输出 $s_1$的概率 $p(s_1)$，再计算根据 $s1执行 $a1 的概率$p_{\\theta}\\left(a_{1} | s_{1}\\right)$，$p_{\\theta}\\left(a_{1} | s_{1}\\right)$是由策略里面的网络参数 θ所决定的。策略网络的输出是一个分布，actor根据这个分布进行采样，决定实际要采取的动作。接下来环境根据 $a1$与 $s1$ 产生 $s2$，因为 $s2$ 与 $s1$是有关系的（游戏画面是连续的，下一个游戏画面与上一个游戏画面通常是有关系的），所以给定上一个游戏画面 $s1$和actor采取的动作 $a1$，就会产生 $s2$。主机在决定输出游戏画面的时候，可能有概率，也可能没有概率，这取决于环境（主机内部设定）。\n",
    "\n",
    "某个轨迹出现的概率取决于环境的动作和智能体的动作。环境的动作是指环境根据其函数内部的参数或内部的规则采取的动作。$p(s_{t+1}|s_t,a_t)$代表的是环境，通常我们无法控制环境，因为环境是设定好的。我们能控制的是 $p_\\theta(a_t|s_t)$。给定一个 $s_t$，actor要采取的 $a_t$ 取决于actor的参数 $\\theta$， 所以智能体的动作是actor可以控制的。actor的动作不同，每个同样的轨迹就有不同的出现的概率。\n",
    "\n",
    "reward function: 奖励函数根据在某一个状态采取的某一个动作决定这个动作可以得到的分数。对奖励函数输入 $s_1$、$a_1$，它会输出$r_1$；输入 $s_2$、$a_2$，奖励函数会输出 $r_2$。 我们把轨迹所有的奖励 $r$ 都加起来，就得到了 $R(\\tau)$ ，其代表某一个轨迹 $\\tau$ 的奖励。 在某一场游戏的某一个回合里面，我们会得到 $R(\\tau)$。我们要做的就是调整actor内部的参数 $\\theta$， 使得$R(\\tau)$的值越大越好。 但实际上$R(\\tau)$并不只是一个标量（scalar），它是一个随机变量，因为actor在给定同样的状态下会采取什么样的动作，这是有随机性的。环境在给定同样的观测时要采取什么样的动作，要产生什么样的观测，本身也是有随机性的，所以$R(\\tau)$是一个随机变量。我们能够计算的是$R(\\tau)$的期望值。给定某一组参数 $\\theta$，我们可计算 $r_{\\theta}$ 的期望值为\n",
    "$$\n",
    "    \\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)\n",
    "$$\n",
    "我们要穷举所有可能的轨迹 $\\tau$， 每一个轨迹 $\\tau$ 都有一个概率。\n",
    "\n",
    "给定一个参数，我们可以计算期望值为\n",
    "$$\n",
    "    \\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[R(\\tau)]\n",
    "$$\n",
    "从分布 $p_{\\theta}(\\tau)$ 采样一个轨迹 $\\tau$，计算 $R(\\tau)$ 的期望值，就是期望奖励（expected reward）。我们要最大化期望奖励。\n",
    "\n",
    "#### 引入梯度\n",
    "**我们要让奖励越大越好**，所以可以使用梯度上升（gradient ascent）来最大化期望奖励。 要进行梯度上升，我们先要计算期望奖励 $\\bar{R}_{\\theta}$ 的梯度。**我们对 $\\bar{R}_{\\theta}$ 做梯度运算**。 梯度是对expected reward！\n",
    "$$\n",
    "    \\nabla \\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau)\n",
    "$$\n",
    "其中，只有 $p_{\\theta}(\\tau)$ 与 $\\theta$ 有关。\n",
    "\n",
    "奖励函数$R(\\tau)$ 不需要是可微分的（differentiable），这不影响我们解决接下来的问题。\n",
    "我们可以对 $\\nabla p_{\\theta}(\\tau)$ 使用式(4.1)，得到 $\\nabla p_{\\theta}(\\tau)=p_{\\theta}(\\tau)  \\nabla \\log p_{\\theta}(\\tau)$。\n",
    "$$\n",
    "    \\nabla f(x)=f(x)\\nabla \\log f(x) \\tag{4.1}\n",
    "$$\n",
    "\n",
    "接下来，代数运算移项\n",
    "$$\n",
    "    \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)}= \\nabla \\log p_{\\theta}(\\tau)\n",
    "$$\n",
    "\n",
    "注：上式用了对数函数的求导公式，对数函数 $f(x)=\\log x$ 的导数为 $\\frac{1}{x}$。\n",
    "\n",
    "我们对 $\\tau$ 进行求和，把 $R(\\tau)$  和  $\\log p_{\\theta}(\\tau)$ 这两项使用 $p_{\\theta}(\\tau)$ 进行加权， 既然使用 $p_{\\theta}(\\tau)$ 进行加权 ，它们就可以被写成期望的形式。也就是我们从 $p_{\\theta}(\\tau)$ 这个分布里面采样 $\\tau$ ， 去计算 $R(\\tau)$ 乘 $\\nabla\\log p_{\\theta}(\\tau)$，对所有可能的 $\\tau$ 进行求和，就是期望的值（expected value）。\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\nabla \\bar{R}_{\\theta}&=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau)\\\\&=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)} \\\\&=\n",
    "        \\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla \\log p_{\\theta}(\\tau) \\\\\n",
    "        &=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]\n",
    "        \\end{aligned} \\tag{4.2}\n",
    "$$\n",
    "\n",
    "note： 我觉得 $\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}$ 只是为了写起来方便。\n",
    "\n",
    "实际上期望值 $\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]$ 无法计算，所以我们用采样的方式采样 $N$ 个 $\\tau$并计算每一个的值，把每一个的值加起来，就可以得到梯度，即\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right] &\\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(\\tau^{n}\\right) \\\\\n",
    "        &=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n",
    "        \\end{aligned}\n",
    "$$\n",
    "$\\nabla \\log p_{\\theta}(\\tau)$ 的具体计算过程可写为\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\nabla \\log p_{\\theta}(\\tau) &= \\nabla \\left(\\log p(s_1)+\\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)+ \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t) \\right) \\\\\n",
    "        &= \\nabla \\log p(s_1)+ \\nabla \\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)+  \\nabla \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t) \\\\\n",
    "        &=\\nabla \\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)\\\\\n",
    "        &=\\sum_{t=1}^{T} \\nabla\\log p_{\\theta}(a_t|s_t)\n",
    "        \\end{aligned}\n",
    "$$\n",
    "注意， $p(s_1)$ 和 $p(s_{t+1}|s_t,a_t)$ 来自环境，$p_\\theta(a_t|s_t)$ 来自智能体。$p(s_1)$ 和 $p(s_{t+1}|s_t,a_t)$ 由环境决定，与 $\\theta$ 无关，因此 $\\nabla \\log p(s_1)=0$ ，$\\nabla \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t)=0$。\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\nabla \\bar{R}_{\\theta}&=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau)\\\\&=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)} \\\\&=\n",
    "        \\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla \\log p_{\\theta}(\\tau) \\\\\n",
    "        &=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]\\\\\n",
    "        &\\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(\\tau^{n}\\right) \\\\\n",
    "        &=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n",
    "        \\end{aligned} \\tag{4.3}\n",
    "$$\n",
    "我们可以直观地理解式(4.3)，也就是在我们采样到的数据里面，采样到在某一个状态 $s_t$ 要执行某一个动作 $a_t$，$(s_t,a_t)$ 是在整个轨迹 $\\tau$ 的里面的某一个状态和动作的对。假设我们在 $s_t$ 执行 $a_t$，最后发现 $\\tau$ 的奖励是正的，我们就要增加在 $s_t$ 执行 $a_t$ 的概率。反之，如果在 $s_t$ 执行 $a_t$ 会导致 $\\tau$ 的奖励变成负的， 我们就要减少在 $s_t$ 执行 $a_t$ 的概率。这怎么实现呢？我们用梯度上升来更新参数，原来有一个参数 $\\theta$ ，把 $\\theta$  加上梯度$\\nabla \\bar{R}_{\\theta}$，当然我们要有一个学习率 $\\eta$，学习率也是要调整的，可用 Adam、RMSProp 等方法来调整学习率，即\n",
    "$$\n",
    "    \\theta \\leftarrow \\theta+\\eta \\nabla \\bar{R}_{\\theta}\n",
    "$$\n",
    "\n",
    "**QQ**： 为什么要有两个加合 $\\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} $? \n",
    "- 外层求和 $\\frac{1}{N} \\sum_{n=1}^{N}$：\n",
    "\n",
    "这是因为使用了蒙特卡洛方法，通过采样 $N$ 个轨迹 $\\tau^n$（每个轨迹 $n$ 对应一个完整的状态-动作序列）来估计期望值。\n",
    "每个轨迹 $\\tau^n$ 都有自己的回报 $R(\\tau^n)$ 和对数概率的梯度 $\\nabla \\log p_{\\theta}(\\tau^n)$。通过对这 $N$ 个样本取平均，近似整个期望值。\n",
    "\n",
    "\n",
    "- 内层求和 $\\sum_{t=1}^{T_n}$：\n",
    "\n",
    "这是因为每个轨迹 $\\tau^n$ 是一个时间序列，由多个时间步 $t$ 组成（长度为 $T_n$）。\n",
    "在策略梯度中，$\\nabla \\log p_{\\theta}(\\tau)$ 被分解为对每个时间步 $t$ 的贡献之和，即 $\\sum_{t=1}^{T} \\nabla \\log p_{\\theta}(a_t | s_t)$。这反映了轨迹概率 $p_{\\theta}(\\tau)$ 是由每个时间步的动作概率 $p_{\\theta}(a_t | s_t)$ 连乘得到的。\n",
    "具体推导中，$\\nabla \\log p_{\\theta}(\\tau) = \\sum_{t=1}^{T} \\nabla \\log p_{\\theta}(a_t | s_t)$，因为初始状态 $s_1$ 和状态转移 $p(s_{t+1} | s_t, a_t)$ 的概率通常不依赖于参数 $\\theta$（在标准设置中），它们的梯度为零。\n",
    "\n",
    "#### 计算梯度\n",
    "实际上要计算梯度，如图 所示，首先我们要收集很多$s$与$a$的对（pair），还要知道这些$s$与$a$在与环境交互的时候，会得到多少奖励。 这些数据怎么收集呢？我们要用参数为 $\\theta$的智能体与环境交互， 也就是拿已经训练好的智能体先与环境交互，交互完以后，就可以得到大量游戏的数据，我们会记录在第一场游戏里面，我们在状态 $s_1$ 采取动作 $a_1$，在状态$s_2$ 采取动作 $a_2$。\n",
    "智能体本身是有随机性的，在同样的状态$s_1$下，不是每次都会采取动作 $a_1$的，所以我们要记录，在状态 $s_1^1$ 采取 $a_1^1$、在状态 $s_2^1$ 采取 $a_2^1$等，整场游戏结束以后，得到的奖励是 $R(\\tau^1)$。我们会采样到另外一些数据，也就是另外一场游戏。在另外一场游戏里面，在状态 $s_1^2$ 采取 $a_1^2$，在状态 $s_2^2$ 采取 $a_2^2$，我们采样到的就是 $\\tau^2$，得到的奖励是 $R(\\tau^2)$。\n",
    "\n",
    "这时我们就可以把采样到的数据代入式(4.4)里面，把梯度算出来。也就是把每一个$s$与$a$的对拿进来，计算在某一个状态下采取某一个动作的对数概率（log probability）$\\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)$。对这个概率取梯度，在梯度前面乘一个权重，权重就是这场游戏的奖励。我们计算出梯度后，就可以更新模型。\n",
    "\n",
    "$$\n",
    "    \\nabla \\bar{R}_{\\theta}=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\\tag{4.4}\n",
    "$$\n",
    "\n",
    "下一轮$\\theta$ 更新仍然是deep learning 的老套路\n",
    "$$\n",
    "    \\theta \\leftarrow \\theta+\\eta \\nabla \\bar{R}_{\\theta}\n",
    "$$\n",
    "\n",
    "![howto](./deep_RL/police_gradient_howto.png)\n",
    "\n",
    "*更新完模型以后，我们要重新采样数据再更新模型。注意，一般策略梯度（policy gradient，PG）采样的数据只会用一次。我们采样这些数据，然后用这些数据更新参数，再丢掉这些数据。接着重新采样数据，才能去更新参数。*\n",
    "\n",
    "#### 优势函数 Advantage\n",
    "为了让reward有正有负（不然有的游戏/场景下reward永远为正；还有一种情况：由于采样量限制采不到所有的情况，这样导致某些action的概率被人工拉高，没有采到的action概率被人工拉低），可以用baseline = $b \\approx E[R(\\tau)]$ 的方法使得reward有正有负\n",
    "$$\n",
    "    \\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}}\\left(R\\left(\\tau^{n}\\right)-b\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n",
    "$$\n",
    "\n",
    "但用baseline之后仍然有缺陷：因为在同一场游戏里面，也许有些动作是好的，有些动作是不好的。 假设整场游戏的结果是好的， 但并不代表这场游戏里面每一个动作都是好的。若是整场游戏结果不好， 但并不代表游戏里面的每一个动作都是不好的。形象的理解是下棋开盘开的好，得了很多分；但是中盘下了一手糟子，之后连连输，最后得正分数可能主要贡献都来自开盘；也可能开盘不好，但是中盘逐渐扳回劣势，尾盘赢，正分数主要贡献都是中盘。用一刀切的baseline 不能突出贡献者对全局影响的因素。好的做法是计算某个状态-动作对的奖励的时候，不把整场游戏得到的奖励全部加起来，只计算从这个动作执行以后得到的奖励。因为这场游戏在执行这个动作之前发生的事情是与执行这个动作是没有关系的，所以在执行这个动作之前得到的奖励都不能算是这个动作的贡献。我们把执行这个动作以后发生的所有奖励加起来，才是这个动作真正的贡献。\n",
    "\n",
    "分配合适的分数这一技巧可以表达为\n",
    "$$\n",
    "    \\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}}\\left(\\sum_{t^{\\prime}=t}^{T_{n}} r_{t^{\\prime}}^{n}-b\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n",
    "$$\n",
    "原来的权重是整场游戏的奖励的总和，现在改成从某个时刻 $t$ 开始，假设这个动作是在 $t$ 开始执行的，从 $t$ 一直到游戏结束所有奖励的总和才能代表这个动作的好坏。 \n",
    "\n",
    "接下来更进一步，我们把未来的奖励做一个折扣，即\n",
    "$$\n",
    "    \\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}}\\left(\\sum_{t^{\\prime}=t}^{T_{n}} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}^{n}-b\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n",
    "$$\n",
    "为什么要把未来的奖励做一个折扣呢？因为虽然在某一时刻，执行某一个动作，会影响接下来所有的结果（有可能在某一时刻执行的动作，接下来得到的奖励都是这个动作的功劳），但在一般的情况下，时间拖得越长，该动作的影响力就越小。 比如在第2个时刻执行某一个动作， 那在第3个时刻得到的奖励可能是在第2个时刻执行某个动作的功劳，但是在第 100 个时刻之后又得到奖励，那可能就不是在第2个时刻执行某一个动作的功劳。实际上，我们会在$R$前面乘一个折扣因子 $\\gamma$（$\\gamma \\in [0,1] $ ，一般会设为 0.9 或 0.99），如果 $\\gamma = 0$，这表示我们只关心即时奖励；如果$\\gamma = 1$，这表示未来奖励等同于即时奖励。时刻 $t'$ 越大，它前面就多次乘 $\\gamma$，就代表现在在某一个状态 $s_t$， 执行某一个动作 $a_t$ 的时候，它真正的分数是执行这个动作之后所有奖励的总和，而且还要乘 $\\gamma$。例如，假设游戏有两个回合，我们在游戏的第二回合的某一个 $s_t$ 执行 $a_t$ 得到 +1 分，在 $s_{t+1}$ 执行 $a_{t+1}$ 得到 +3 分，在 $s_{t+2}$ 执行 $a_{t+2}$ 得到 $-$5 分，第二回合结束。$a_t$ 的分数应该是\n",
    "$$\n",
    "1+ \\gamma \\times 3+\\gamma^2 \\times(-5)\n",
    "$$\n",
    "\n",
    "实际上就是这么实现的。$b$ 可以是依赖状态（state-dependent）的，事实上 $b$ 通常是一个网络估计出来的，它是一个网络的输出。我们把 $R-b$ 这一项称为**优势函数（advantage function）**， **用 $A^{\\theta}(s_t,a_t)$ 来代表优势函数**。优势函数取决于 $s$ 和 $a$，我们就是要计算在某个状态 $s$ 采取某个动作 $a$ 的时候，优势函数的值。在计算优势函数值时，我们要计算 $\\sum_{t^{\\prime}=t}^{T_{n}} r_{t^{\\prime}}^{n}$，需要有一个模型与环境交互，才能知道接下来得到的奖励。优势函数 $A^{\\theta}\\left(s_{t}, a_{t}\\right)$ 的上标是 $\\theta$，$\\theta$ 代表用模型 $\\theta$ 与环境交互。从时刻 $t$ 开始到游戏结束为止，所有 $r$ 的加和减去 $b$，优势函数的意义是，假设我们在某一个状态$s_t$ 执行某一个动作 $a_t$，相较于其他可能的动作，$a_t$有多好。*优势函数在意的不是绝对的好，而是相对的好*，即**相对优势（relative advantage）**。因为在优势函数中，我们会减去一个基线 $b$，所以这个动作是相对的好，不是绝对的好。 $A^{\\theta}\\left(s_{t}, a_{t}\\right)$ 通常可以由一个网络估计出来，这个网络称为评论员（critic）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff94d2",
   "metadata": {},
   "source": [
    "## PPO（proximal policy optimization）\n",
    "### Importance Sampling\n",
    "#### Motivating Problem\n",
    "在强化学习里面，要学习的是一个智能体。如果要学习的智能体和与环境交互的智能体是相同的，我们称之为同策略。如果要学习的智能体和与环境交互的智能体不是相同的，我们称之为异策略。\n",
    "为什么要考虑异策略？policy gradient是一个同策略算法，也就是\n",
    "$$\n",
    "    \\nabla \\bar{R}_{\\theta}=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right] \\tag{5.1}\n",
    "$$\n",
    "$\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}$ 是对策略 $\\pi_{\\theta}$ 采样的轨迹 $\\tau$ 求期望。一旦更新了参数，从 $\\theta$ 变成 $\\theta'$ ，概率 $p_\\theta(\\tau)$ 就不对了，之前采样的数据也不能用了。所以策略梯度是一个会花很多时间来采样数据的算法，其大多数时间都在采样数据。智能体与环境交互以后，接下来就要更新参数。我们只能更新参数一次，然后就要重新采样数据， 才能再次更新参数。这显然是非常花时间的，所以我们想要从同策略变成异策略，这样就可以用另外一个策略$\\pi_{\\theta'}$、另外一个演员$\\theta'$ 与环境交互（$\\theta'$ 被固定了），用 $\\theta'$ 采样到的数据去训练 $\\theta$。假设我们可以用 $\\theta'$ 采样到的数据去训练 $\\theta$，我们可以多次使用 $\\theta'$ 采样到的数据，可以多次执行梯度上升（gradient ascent），可以多次更新参数， 都只需要用同一批数据。因为假设 $\\theta$ 有能力学习另外一个演员 $\\theta'$ 所采样的数据，所以$\\theta'$ 只需采样一次，并采样多一点的数据，让 $\\theta$ 去更新很多次，这样就会比较有效率。\n",
    "这就引入了重要性采样\n",
    "\n",
    "#### 数学推导\n",
    "随机变量x服从p分布的$f(x)$的数学期望， 当无法做积分时，可以用采样均值近似\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p}[f(x)] \\approx \\frac{1}{N } \\sum_{i=1}^N f(x^i) \\tag{5.2}\n",
    "$$\n",
    "如果无法从中$p(x)$采样，只能从$q(x)$中采样 则可以考虑\n",
    "$$\n",
    "    \\int f(x) p(x) \\mathrm{d}x=\\int f(x) \\frac{p(x)}{q(x)} q(x) \\mathrm{d}x=\\mathbb{E}_{x \\sim q}[f(x){\\frac{p(x)}{q(x)}}]\n",
    "==> \\mathbb{E}_{x \\sim p}[f(x)]=\\mathbb{E}_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right] \\tag{5.3}\n",
    "$$\n",
    "因为是从 $q$ 采样数据，所以从 $q$ 采样出来的每一笔数据，都需要乘一个**重要性权重（importance weight）** $\\frac{p(x)}{q(x)}$ 来修正这两个分布的差异。$q(x)$ 可以是任何分布，唯一的限制就是 $q(x)$ 的概率是 0 的时候，$p(x)$ 的概率不为 0，不然会没有定义。假设  $q(x)$ 的概率是 0 的时候，$p(x)$ 的概率也都是 0，$p(x)$ 除以 $q(x)$是有定义的。所以这个时候我们就可以使用重要性采样，把从 $p$ 采样换成从 $q$ 采样。\n",
    "\n",
    "重要性采样有一些问题。虽然我们可以把 $p$ 换成任何的 $q$。但是在实现上， $p$ 和 $q$ 的差距不能太大。差距太大，会有一些问题。比如，虽然式(5.3)成立（式(5.3)左边是 $f(x)$ 的期望值，它的分布是 $p$，式(5.3)右边是 $f(x) \\frac{p(x)}{q(x)}$ 的期望值，它的分布是 $q$），但如果不是计算期望值，而是计算方差，$\\operatorname{Var}_{x \\sim p}[f(x)]$ 和 $\\operatorname{Var}_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]$ 可能差距很大。\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\operatorname{Var}_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right] &=\\mathbb{E}_{x \\sim q}\\left[\\left(f(x) \\frac{p(x)}{q(x)}\\right)^{2}\\right]-\\left(\\mathbb{E}_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]\\right)^{2} \\\\\n",
    "        第一项套用（5.3） 把f(x)^{2} \\frac{p(x)}{q(x)} 当成f(x),  \\frac{p(x)}{q(x)} 当成权重；第二项直接推出是x～p f(x)的期望\\\\\n",
    "        => &=\\mathbb{E}_{x \\sim p}\\left[f(x)^{2} \\frac{p(x)}{q(x)}\\right]-\\left(\\mathbb{E}_{x \\sim p}[f(x)]\\right)^{2}\n",
    "    \\end{aligned}\n",
    "$$\n",
    "如果 $\\frac{p(x)}{q(x)}$ 差距很大，$f(x)\\frac{p(x)}{q(x)}$ 的方差就会很大。所以理论上它们的期望值一样，也就是，我们只要对分布$p$采样足够多次，对分布$q$采样足够多次，得到的结果会是一样的。但是如果我们采样的次数不够多，因为它们的方差差距是很大的，所以我们就有可能得到差别非常大的结果。\n",
    "\n",
    "#### 应用\n",
    "用重要性采用，把同策略训练的算法改成异策略训练。之前我们用策略 $\\pi_{\\theta}$ 与环境交互，采样出轨迹 $\\tau$，计算 $R(\\tau) \\nabla \\log p_{\\theta}(\\tau)$。现在我们不用 $\\theta$ 与环境交互，假设有另外一个策略 $\\pi_{\\theta}'$，它就是另外一个演员，它的工作是做示范（demonstration）。\n",
    "$$\n",
    "    \\nabla \\bar{R}_{\\theta}=\\mathbb{E}_{\\tau \\sim p_{\\theta^{\\prime}(\\tau)}}\\left[\\frac{p_{\\theta}(\\tau)}{p_{\\theta^{\\prime}}(\\tau)} R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right] \\tag{5.4}\n",
    "$$\n",
    "\n",
    "$\\theta'$ 的工作是为$\\theta$ 做示范。它与环境交互，告诉 $\\theta$ 它与环境交互会发生什么事，借此来训练 $\\theta$。我们要训练的是 $\\theta$ ，$\\theta'$  只负责做示范，负责与环境交互。我们现在的 $\\tau$ 是从 $\\theta'$ 采样出来的，是用 $\\theta'$ 与环境交互。所以采样出来的 $\\tau$ 是从 $\\theta'$ 采样出来的，这两个分布不一样。但没有关系，假设我们本来是从 $p$ 采样，但发现不能从 $p$ 采样，所以我们不用 $\\theta$ 与环境交互，可以把 $p$ 换成 $q$，在后面补上一个重要性权重。同理，我们把 $\\theta$ 换成 $\\theta'$ 后，要补上一个重要性权重 $\\frac{p_{\\theta}(\\tau)}{p_{\\theta^{\\prime}}(\\tau)}$。这个重要性权重就是某一个轨迹 $\\tau$ 用 $\\theta$ 算出来的概率除以这个轨迹 $\\tau$ 用 $\\theta'$ 算出来的概率。这一项是很重要的，因为我们要学习的是演员 $\\theta$，$\\theta'$ 见到的情形与 $\\theta$ 见到的情形可能不是一样的，所以中间要有一个修正的项。\n",
    "\n",
    "现在的数据是从 $\\theta'$ 采样出来的，与环境交互的是 $\\theta'$ 而不是 $\\theta$，所以采样的数据与 $\\theta$ 本身是没有关系的。因此我们就可以让 $\\theta'$ 与环境交互采样大量的数据，$\\theta$ 可以多次更新参数，一直到 $\\theta$ 训练到一定的程度。更新多次以后，$\\theta'$ 再重新做采样，这就是同策略换成异策略的妙处。\n",
    "\n",
    "实际在做策略梯度的时候，我们并不是给整个轨迹 $\\tau$ 一样的分数，而是将每一个状态-动作对分开计算。实际更新梯度的过程可写为\n",
    "$$\n",
    "    \\mathbb{E}_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta}}\\left[A^{\\theta}\\left(s_{t}, a_{t}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\\right]\n",
    "$$\n",
    "状态-动作对的优势（advantage）$A^{\\theta}\\left(s_{t}, a_{t}\\right)$ 是actor 在$\\theta$, （$s_t$, $a_t$） 优势函数，即用累积奖励减去基线，这一项就是估测出来的。它要估测的是，在状态 $s_t$ 采取动作 $a_t$ 是好的还是不好的。接下来在后面乘 $\\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)$，也就是如果 $A^{\\theta}\\left(s_{t}, a_{t}\\right)$ 是正的，就要增大概率；如果是负的，就要减小概率。\n",
    "\n",
    "现在通过重要性采样把同策略变成异策略，从 $\\theta$ 变成 $\\theta'$。所以现在 $s_t$、$a_t$ 是 $\\theta'$ 与环境交互以后所采样到的数据。 但是训练时，要调整的参数是模型 $\\theta$。因为 $\\theta'$  与 $\\theta$ 是不同的模型，所以我们要有一个修正的项。这个修正的项，就是用重要性采样，把 $s_t$、$a_t$ 用 $\\theta$ 采样出来的概率除以 $s_t$、$a_t$  用 $\\theta'$  采样出来的概率。\n",
    "\n",
    "$$\n",
    "    \\mathbb{E}_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(s_{t}, a_{t}\\right)}{p_{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)} A^{\\theta}\\left(s_{t}, a_{t}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\\right] \n",
    "$$\n",
    "其中，$A^{\\theta}(s_t,a_t)$ 有一个上标 $\\theta$，$\\theta$  代表 $A^{\\theta}(s_t,a_t)$ 是演员 $\\theta$ 与环境交互的时候计算出来的。但是实际上从 $\\theta$ 换到 $\\theta'$  的时候，$A^{\\theta}(s_t,a_t)$ 应该改成 $A^{\\theta'}(s_t,a_t)$，为什么呢？$A(s_t,a_t)$ 这一项是想要估测在某一个状态采取某一个动作，接下来会得到累积奖励的值减去基线的值。我们怎么估计 $A(s_t,a_t)$?我们在状态 $s_t$ 采取动作 $a_t$，接下来会得到的奖励的总和，再减去基线就是 $A(s_t,a_t)$。之前是 $\\theta$ 与环境交互，所以我们观察到的是 $\\theta$ 可以得到的奖励。但现在是 $\\theta'$  与环境交互，所以我们得到的这个优势是根据 $\\theta'$  所估计出来的优势。但我们现在先不要管那么多，就假设 $A^{\\theta}(s_t,a_t)$ 和 $A^{\\theta'}(s_t,a_t)$ 相等。（note 为什么这个假设合理？我感觉是PPO的clip算法保障了$\\theta$ $\\theta'$ 不会相差很多， 所有优势大致相同？）\n",
    "\n",
    "接下来，我们可以拆解 $p_{\\theta}\\left(s_{t}, a_{t}\\right)$ 和 $p_{\\theta'}\\left(s_{t}, a_{t}\\right)$，即\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        p_{\\theta}\\left(s_{t}, a_{t}\\right)&=p_{\\theta}\\left(a_{t}|s_{t}\\right) p_{\\theta}(s_t) \\\\\n",
    "        p_{\\theta'}\\left(s_{t}, a_{t}\\right)&=p_{\\theta'}\\left(a_{t}|s_{t}\\right) p_{\\theta'}(s_t) \n",
    "        \\end{aligned}\n",
    "$$\n",
    "于是我们可得\n",
    "$$\n",
    "    \\mathbb{E}_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)} \\frac{p_{\\theta}\\left(s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(s_{t}\\right)} A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\\right]\n",
    "$$\n",
    "此时又假设$p_{\\theta}(s_t)$ = $p_{\\theta'}(s_t)$。 原因是看到状态往往与采取的动作是没有太大的关系的，不同的 $\\theta$  对 $s_t$ 是没有影响的。但更直接的理由（李宏毅讲课）就是 $p_{\\theta}(s_t)$ 很难算，$p_{\\theta}(s_t)$有一个参数 $\\theta$，它表示的是我们用 $\\theta$ 去与环境交互，计算 $s_t$ 出现的概率，而这个概率很难算。尤其是如果输入的是图片，同样的 $s_t$ 可能根本就不会出现第二次。我们根本没有办法估计$p_{\\theta}(s_t)$，所以干脆就无视这个问题。\n",
    "\n",
    "但是 $p_{\\theta}(a_t|s_t)$很好算，我们有参数 $\\theta$ ，它就是一个策略网络。我们输入状态 $s_t$ 到策略网络中，它会输出每一个 $a_t$ 的概率。所以我们只要知道$\\theta$ 和 $\\theta'$ 的参数就可以计算 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)}$。最后reward 梯度变成\n",
    "\n",
    "$$\n",
    "     \\nabla \\bar{R}_{\\theta}= \\mathbb{E}_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)} A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\\right] \\tag{5.5}  \n",
    "$$\n",
    "又有导数关系\n",
    "$$\n",
    "    \\nabla f(x)=f(x) \\nabla \\log f(x)\n",
    "$$\n",
    "可以从梯度反推出原来的目标函数， 对$\\theta$ 求梯度时，$p_{\\theta^{\\prime}}(a_{t} | s_{t})$ 和 $A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)$ 都是常数。可以看成是\n",
    "$$\n",
    "\\nabla \\bar{R}_{\\theta} =  \\mathbb{E}_{\\theta'}[\\frac{A^{\\theta'}}{p^{\\theta'}} * f(x) * \\nabla f(x)],  f(x) = p_{\\theta}\\left(a_{t} | s_{t}\\right)\n",
    "$$\n",
    "这样消去log项 得到目标函数\n",
    "$$\n",
    "    J^{\\theta^{\\prime}}(\\theta)=\\mathbb{E}_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)} A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)\\right]\n",
    "$$\n",
    "总结是$J^{\\theta^{\\prime}}(\\theta)$ 括号里面的 $\\theta$ 代表我们要去优化的参数。$\\theta'$  是指我们用 $\\theta'$  在真正在与环境交互。$\\theta$ 不与环境交互。然后我们在 $\\theta'$  与环境交互中采样出 $s_t$、$a_t$ 以后，要去计算 $s_t$ 与 $a_t$ 的优势 $A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)$，再用它乘 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)}$。$\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)}$ 是容易计算的，我们可以从采样的结果来估测$A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)$ ，所以 $J^{\\theta^{\\prime}}(\\theta)$ 是可以计算的。实际上在更新参数的时候，我们就是按照式(5.5)来更新参数的。\n",
    "\n",
    "### PPO优化\n",
    "我们可以通过重要性采样把同策略换成异策略， 但如果 $p_{\\theta}\\left(a_{t} | s_{t}\\right)$ 与 $p_{\\theta'}\\left(a_{t} | s_{t}\\right)$ 相差太多，即这两个分布相差太多，重要性采样的结果就会不好。\n",
    "\n",
    "PPO优化解决了这个问题。方法是：在训练的时候，应多加一个约束（constrain）。这个约束是 $\\theta$  与 $\\theta'$ 输出的动作的 KL 散度（KL divergence），这一项用于衡量 $\\theta$ 与 $\\theta'$ 的相似程度。我们希望在训练的过程中，学习出的 $\\theta$ 与 $\\theta'$  越相似越好。因为如果 $\\theta$ 与 $\\theta'$ 不相似，最后的结果就会不好。所以在 PPO 里面有两项：一项是优化本来要优化的$J^{\\theta^{\\prime}}(\\theta)$，另一项是一个约束。这个约束就好像正则化（regularization）的项（term） 一样，它所做的就是希望最后学习出的 $\\theta$ 与 $\\theta'$ 相差不大。**注意，虽然 PPO 的优化目标涉及到了重要性采样，但其只用到了上一轮策略 $\\theta^{\\prime}$ 的数据。PPO 目标函数中加入了 KL 散度的约束，行为策略 $\\theta^{\\prime}$  和目标策略 $\\theta$ 非常接近，PPO 的行为策略和目标策略可认为是同一个策略，因此 PPO 是同策略算法。**\n",
    "\n",
    "PPO 有一个前身：**信任区域策略优化（trust region policy optimization，TRPO）**。TRPO 可表示为\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        J_{\\mathrm{TRPO}}^{\\theta^{\\prime}}(\\theta)=\\mathbb{E}_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)} A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)\\right],\\mathrm{KL}\\left(\\theta, \\theta^{\\prime}\\right)<\\delta\n",
    "        \\end{aligned}\n",
    "$$\n",
    "\n",
    "TRPO 与 PPO 不一样的地方是约束所在的位置不一样，PPO 直接把约束放到要优化的式子里面，我们就可以用梯度上升的方法去最大化式(5.6)。但 TRPO 是把 KL 散度当作约束，它希望 $\\theta$ 与 $\\theta'$ 的 KL 散度小于 $\\delta$。如果我们使用的是基于梯度的优化，有约束是很难处理的。TRPO 是很难处理的，因为它把 KL 散度约束当作一个额外的约束，没有放在目标（objective）里面，所以它很难计算。因此我们一般就使用 PPO，而不使用 TRPO 。PPO 与 TRPO 的性能差不多，但 PPO 在实现上比 TRPO 容易得多。\n",
    "#### PPO-penalty\n",
    "它先初始化一个策略的参数 $\\theta^0$。在每一个迭代里面，我们用前一个训练的迭代得到的演员的参数 $\\theta^k$ 与环境交互，采样到大量状态-动作对。根据 $\\theta^k$ 交互的结果，我们估测$A^{\\theta^{k}}\\left(s_{t}, a_{t}\\right)$。我们使用 PPO 的优化公式。但与原来的策略梯度不一样，原来的策略梯度只能更新一次参数，更新完以后，我们就要重新采样数据。但是现在不同，我们用 $\\theta^k$ 与环境交互，采样到这组数据以后，我们可以让 $\\theta$ 更新很多次，想办法最大化目标函数，如式(5.7)所示。这里面的 $\\theta$ 更新很多次也没有关系，因为我们已经有重要性采样，所以这些经验，这些状态-动作对是从 $\\theta^k$ 采样出来的也没有关系。$\\theta$ 可以更新很多次，它与 $\\theta^k$ 变得不太一样也没有关系，我们可以照样训练 $\\theta$。\n",
    "\n",
    "$$\n",
    "    J_{\\mathrm{PPO}}^{\\theta^{k}}(\\theta)=J^{\\theta^{k}}(\\theta)-\\beta \\mathrm{KL}\\left(\\theta, \\theta^{k}\\right) \\tag{5.7}\n",
    "$$\n",
    "\n",
    "在 PPO 的论文里面还有一个**自适应KL散度（adaptive KL divergence）**。这里会遇到一个问题就，即$\\beta$  要设置为多少。这个问题与正则化一样，正则化前面也要乘一个权重，所以 KL 散度前面也要乘一个权重，但 $\\beta$  要设置为多少呢？我们有一个动态调整 $\\beta$ 的方法。在这个方法里面，我们先设一个可以接受的 KL 散度的最大值。假设优化完式(5.7)以后，KL 散度的值太大，这就代表后面惩罚的项$\\beta \\mathrm{KL}\\left(\\theta, \\theta^{k}\\right)$ 没有发挥作用，我们就把 $\\beta$ 增大。另外，我们设一个 KL 散度的最小值。如果优化完式(5.7)以后，KL 散度比最小值还要小，就代表后面这一项的效果太强了，我们怕他只优化后一项，使$\\theta$ 与 $\\theta^k$ 一样，这不是我们想要的，所以我们要减小 $\\beta$。$\\beta$ 是可以动态调整的，因此我们称之为**自适应KL惩罚（adaptive KL penalty）**。我们可以总结一下自适应KL惩罚：\n",
    "* 如果 $\\mathrm{KL}(\\theta,\\theta^k)>\\mathrm{KL}_{\\max}$，增大 $\\beta$；\n",
    "* 如果 $\\mathrm{KL}(\\theta,\\theta^k)<\\mathrm{KL}_{\\min}$，减小 $\\beta$。\n",
    "\n",
    "近端策略优化惩罚可表示为\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        &J_{\\text{PPO}}^{\\theta^{k}}(\\theta)=J^{\\theta^{k}}(\\theta)-\\beta \\text{KL}\\left(\\theta, \\theta^{k}\\right) \\\\\n",
    "        &J^{\\theta^{k}}(\\theta) \\approx \\sum_{\\left(s_{t}, a_{t}\\right)} \\frac{p_{\\theta}\\left(a_{t} \\mid s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} \\mid s_{t}\\right)} A^{\\theta^{k}}\\left(s_{t}, a_{t}\\right)\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "#### PPO-clip\n",
    "PPO2 即**近端策略优化裁剪**。近端策略优化裁剪的目标函数里面没有 KL 散度，其要最大化的目标函数为\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        J_{\\mathrm{PPO2}}^{\\theta^{k}}(\\theta) \\approx \\sum_{\\left(s_{t}, a_{t}\\right)} \\min &\\left(\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)} A^{\\theta^{k}}\\left(s_{t}, a_{t}\\right),\\right.\\\\\n",
    "        &\\left.\\operatorname{clip}\\left(\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A^{\\theta^{k}}\\left(s_{t}, a_{t}\\right)\\right)\n",
    "        \\end{aligned} \\tag{5.8}\n",
    "$$\n",
    "其中，\n",
    "* 操作符（operator）min 是在第一项与第二项里面选择比较小的项。\n",
    "* 第二项前面有一个裁剪（clip）函数，裁剪函数是指，在括号里面有3项，如果第一项小于第二项，那就输出 $1-\\varepsilon$；第一项如果大于第三项，那就输出 $1+\\varepsilon$。 \n",
    "* $\\varepsilon$ 是一个超参数，是我们要调整的，可以设置成 0.1 或 0.2 。\n",
    "\n",
    "剪裁函数意义：希望 $p_{\\theta}(a_{t} | s_{t})$ 与 $p_{\\theta^k}(a_{t} | s_{t})$比较接近，也就是做示范的模型与实际上学习的模型在优化以后不要差距太大。\n",
    "如下图，红线是输出。\n",
    "![clip](./deep_RL/clip.png)\n",
    "* 如果 $A > 0$，也就是某一个状态-动作对是好的，我们希望增大这个状态-动作对的概率。也就是，我们想让  $p_{\\theta}(a_{t} | s_{t})$ 越大越好，但它与 $p_{\\theta^k}(a_{t} | s_{t})$ 的比值不可以超过 $1+\\varepsilon$。如果超过 $1+\\varepsilon$  ，就没有好处了。红色的线就是目标函数。在训练的时候，当 $p_{\\theta}(a_{t} | s_{t})$ 被训练到 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}>1+\\varepsilon$ 时，它就会停止。假设 $p_{\\theta}(a_{t} | s_{t})$  比 $p_{\\theta^k}(a_{t} | s_{t})$ 还要小，并且这个优势是正的。因为这个动作是好的，我们希望这个动作被采取的概率越大越好，希望 $p_{\\theta}(a_{t} | s_{t})$ 越大越好。所以假设 $p_{\\theta}(a_{t} | s_{t})$ 还比 $p_{\\theta^k}(a_{t} | s_{t})$  小，那就尽量把它变大，但只要大到 $1+\\varepsilon$ 就好。\n",
    "\n",
    "* 如果 $A < 0$，也就是某一个状态-动作对是不好的，那么我们希望把 $p_{\\theta}(a_{t} | s_{t})$ 减小。如果 $p_{\\theta}(a_{t} | s_{t})$ 比 $p_{\\theta^k}(a_{t} | s_{t})$  还大，那我们就尽量把它减小，减到 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}$ 是 $1-\\varepsilon$ 的时候停止，此时不用再减得更小。\n",
    "\n",
    "这样的好处就是，我们不会让 $p_{\\theta}(a_{t} | s_{t})$ 与 $p_{\\theta^k}(a_{t} | s_{t})$ 差距太大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603af90c-a5d1-412a-bc40-bad8e02811a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
